[
["index.html", "Aprendizaje de máquina Temario y referencias", " Aprendizaje de máquina Felipe González 2017-12-12 Temario y referencias Todas las notas y material del curso estarán en este repositorio. Introducción al aprendizaje máquina Regresión lineal múltiple y descenso en gradiente Problemas de clasificación y regresión logística Validación cruzada y métodos de remuestreo Regularización y selección de modelos Redes neuronales Diagnóstico y mejora en problemas de aprendizaje supervisado. Árboles y bosques aleatorios Máquinas de soporte vectorial Componentes principales y análisis de conglomerados Evaluación Tareas semanales (20%) Examen parcial (30% práctico, 20% teórico) Un examen final (30% práctico) Software: R y Rstudio R Sitio de R (CRAN) Rstudio Interfaz gráfica para trabajar en R. Recursos para aprender R Referencias principales An Introduction to Statistical Learning, James et al. (2014) Curso de Machine Learning de Andrew Ng, Ng (2017) Deep Learning, Goodfellow, Bengio, and Courville (2016) Otras referencias Pattern Recognition and Machine Learning, Bishop (2006) The Elements of Statistical Learning, Hastie, Tibshirani y Friedman, Hastie, Tibshirani, and Friedman (2017) References "],
["introduccion.html", "Clase 1 Introducción 1.1 ¿Qué es aprendizaje de máquina (machine learning)? 1.2 Aprendizaje Supervisado 1.3 Predicciones 1.4 Cuantificación de error o precisión 1.5 Tarea de aprendizaje supervisado 1.6 ¿Por qué tenemos errores? 1.7 ¿Cómo estimar f? 1.8 Resumen 1.9 Tarea", " Clase 1 Introducción 1.1 ¿Qué es aprendizaje de máquina (machine learning)? Métodos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempeño en alguna tarea o toma de decisión. En este curso nos enfocamos en las tareas de aprendizaje supervisado (predecir o estimar una variable respuesta a partir de datos de entrada) y aprendizaje no supervisado (describir estructuras interesantes en datos, donde no necesariamente hay una respuesta que predecir). Ejemplos de tareas de aprendizaje: Predecir si un cliente de tarjeta de crédito va a caer en impago en los próximos tres meses. Reconocer palabras escritas a mano (OCR). Detectar llamados de ballenas en grabaciones de boyas. Estimar el ingreso mensual de un hogar a partir de las características de la vivienda, posesiones y equipamiento y localización geográfica. Dividir a los clientes de Netflix según sus gustos. Recomendar artículos a clientes de un programa de lealtad o servicio online. Las razones usuales para intentar resolver estos problemas computacionalmente son diversas: Quisiéramos obtener una respuesta barata, rápida, automatizada, y con suficiente precisión. Por ejemplo, reconocer caracteres en una placa de coche de una fotografía se puede hacer por personas, pero eso es lento y costoso. Igual oír cada segundo de grabación de las boyas para saber si hay ballenas o no. Hacer mediciones directas del ingreso de un hogar requiere mucho tiempo y esfuerzo. Quisiéramos superar el desempeño actual de los expertos o de reglas simples utilizando datos: por ejemplo, en la decisión de dar o no un préstamo a un solicitante, puede ser posible tomar mejores decisiones con algoritmos que con evaluaciones personales o con reglas simples que toman en cuenta el ingreso mensual, por ejemplo. Queremos entender de manera más completa y sistemática el comportamiento de un fenómeno, identificando variables o patrones importantes. Es posible aproximarse a todos estos problemas usando reglas (por ejemplo, si los pixeles del centro de la imagen están vacíos, entonces es un cero, si el crédito total es mayor al 50% del ingreso anual, declinar el préstamo, etc) Las razones para intentar usar aprendizaje para producir reglas en lugar de intentar construir estas reglas directamente son, por ejemplo: Cuando conjuntos de reglas creadas a mano se desempeñan mal (por ejemplo, para otorgar créditos, reconocer caracteres, etc.) Reglas creadas a mano pueden ser difíciles de mantener (por ejemplo, un corrector ortográfico.) Ejemplo: reconocimiento de dígitos escritos a mano ¿Cómo reconocer los siguientes dígitos de manera automática? En los datos tenemos los valores de cada pixel (los caracteres son imagenes de 16x16 pixeles), y una etiqueta asociada, que es el número que la imagen representa. Podemos ver las imágenes y las etiquetas: library(dplyr) library(tidyr) library(purrr) library(readr) zip_train &lt;- read_csv(file = &#39;datos/zip-train.csv&#39;) muestra_1 &lt;- sample_n(zip_train, 10) graficar_digitos(muestra_1) muestra_2 &lt;- sample_n(zip_train, 10) graficar_digitos(muestra_2) Los 16x16=256 están escritos acomodando las filas de la imagen en vector de 256 valores (cada renglón de zip_train). Un dígito entonces se representa como sigue: dim(zip_train) ## [1] 7291 257 as.numeric(zip_train[1,]) ## [1] 6.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.631 0.862 ## [11] -0.167 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 ## [21] -1.000 -1.000 -1.000 -0.992 0.297 1.000 0.307 -1.000 -1.000 -1.000 ## [31] -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.410 ## [41] 1.000 0.986 -0.565 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 ## [51] -1.000 -1.000 -1.000 -1.000 -0.683 0.825 1.000 0.562 -1.000 -1.000 ## [61] -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.938 ## [71] 0.540 1.000 0.778 -0.715 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 ## [81] -1.000 -1.000 -1.000 -1.000 -1.000 0.100 1.000 0.922 -0.439 -1.000 ## [91] -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 ## [101] -0.257 0.950 1.000 -0.162 -1.000 -1.000 -1.000 -0.987 -0.714 -0.832 ## [111] -1.000 -1.000 -1.000 -1.000 -1.000 -0.797 0.909 1.000 0.300 -0.961 ## [121] -1.000 -1.000 -0.550 0.485 0.996 0.867 0.092 -1.000 -1.000 -1.000 ## [131] -1.000 0.278 1.000 0.877 -0.824 -1.000 -0.905 0.145 0.977 1.000 ## [141] 1.000 1.000 0.990 -0.745 -1.000 -1.000 -0.950 0.847 1.000 0.327 ## [151] -1.000 -1.000 0.355 1.000 0.655 -0.109 -0.185 1.000 0.988 -0.723 ## [161] -1.000 -1.000 -0.630 1.000 1.000 0.068 -0.925 0.113 0.960 0.308 ## [171] -0.884 -1.000 -0.075 1.000 0.641 -0.995 -1.000 -1.000 -0.677 1.000 ## [181] 1.000 0.753 0.341 1.000 0.707 -0.942 -1.000 -1.000 0.545 1.000 ## [191] 0.027 -1.000 -1.000 -1.000 -0.903 0.792 1.000 1.000 1.000 1.000 ## [201] 0.536 0.184 0.812 0.837 0.978 0.864 -0.630 -1.000 -1.000 -1.000 ## [211] -1.000 -0.452 0.828 1.000 1.000 1.000 1.000 1.000 1.000 1.000 ## [221] 1.000 0.135 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.483 0.813 ## [231] 1.000 1.000 1.000 1.000 1.000 1.000 0.219 -0.943 -1.000 -1.000 ## [241] -1.000 -1.000 -1.000 -1.000 -1.000 -0.974 -0.429 0.304 0.823 1.000 ## [251] 0.482 -0.474 -0.991 -1.000 -1.000 -1.000 -1.000 Un enfoque más utilizado anteriormente para resolver este tipo de problemas consistía en procesar estas imágenes con filtros hechos a mano (por ejemplo, calcular cuántos pixeles están prendidos, si existen ciertas curvas o trazos) para después construir reglas para determinar cada dígito. Actualmente, el enfoque más exitoso es utilizar métodos de aprendizaje que aprendan automáticamente esos filtros y esas reglas basadas en filtros (redes convolucionales). Ejemplo: predecir ingreso trimestral Consideramos la medición de ingreso total trimestral para una muestra de hogares de la encuesta de ENIGH. Cada una de estas mediciones es muy costosa en tiempo y dinero. dat_ingreso &lt;- read_csv(file = &#39;datos/enigh-ejemplo.csv&#39;) ggplot(dat_ingreso, aes(x=INGTOT)) + geom_histogram(bins = 100) + scale_x_log10() Pero quizá podemos usar otras variables más fácilmente medibles para predecir el ingreso de un hogar. Por ejemplo, si consideramos el número de focos en la vivienda: ggplot(dat_ingreso, aes(x = FOCOS, y = INGTOT)) + geom_point() + scale_y_log10() + xlim(c(0,50)) O el tamaño de la localidad: ggplot(dat_ingreso, aes(x = tamaño_localidad, y = INGTOT)) + geom_boxplot() + scale_y_log10() En algunas encuestas se pregunta directamente el ingreso mensual del hogar. La respuesta es generalmente una mala estimación del verdadero ingreso, por lo que actualmente se prefiere utilizar aprendizaje para estimar a partir de otras variables que son más fielmente reportadas por encuestados (años de estudio, ocupación, número de focos en el hogar, etc.) Aprendizaje supervisado Las tareas de aprendizaje se divide en dos grandes partes: aprendizaje supervisado y aprendizaje no supervisado. Aprendizaje supervisado Construir un modelo o algoritmo para predecir o estimar un target o una variable de salida a partir de ciertas variables de entrada. Predecir y estimar, en este contexto, se refieren a cosas similares. Generalmente se usa predecir cuando se trata de variables que no son observables ahora, sino en el futuro, y estimar cuando nos interesan variables actuales que no podemos observar ahora por costos o por la naturaleza del fenómeno. Por ejemplo, para identificar a los clientes con alto riesgo de impago de tarjeta de crédito, utilizamos datos históricos de clientes que han pagado y no han pagado. Con estos datos entrenamos un algoritmo para detectar anticipadamente los clientes con alto riesgo de impago. Usualmente dividimos los problemas de aprendizaje supervisado en dos tipos, dependiendo de la variables salida: Problemas de regresión: cuando la salida es una variable numérica. El ejemplo de estimación de ingreso es un problema de regresión Problemas de clasificación: cuando la salida es una variable categórica. El ejemplo de detección de dígitos escritos a manos es un problema de clasificación. Ejemplo: predecir el rendimiento de un coche. Estimar directamente el rendimiento (km por litro de combustible) de un coche es costoso: hay que hacer varias pruebas en diversas condiciones, etc. ¿Podríamos estimar el rendimiento de un coche usando variables más accesibles, peso del coche, año de producción, etc.? library(ISLR) datos &lt;- Auto[, c(&#39;name&#39;, &#39;weight&#39;,&#39;year&#39;, &#39;mpg&#39;)] datos$peso_kg &lt;- datos$weight*0.45359237 datos$rendimiento_kpl &lt;- datos$mpg*(1.609344/3.78541178) set.seed(213) datos_muestra &lt;- sample_n(datos, 50) datos_muestra %&gt;% select(name, peso_kg, rendimiento_kpl) ## name peso_kg rendimiento_kpl ## 9 pontiac catalina 2007.1462 5.952012 ## 139 dodge coronet custom (sw) 2021.6612 5.952012 ## 248 datsun b210 gx 938.9362 16.750662 ## 229 ford granada 1598.9131 7.865159 ## 166 chevrolet monza 2+2 1461.0210 8.502874 ## 321 datsun 510 hatchback 1104.0438 15.730317 ## 5 ford torino 1564.4401 7.227443 ## 145 toyota corona 747.9738 13.179455 ## 282 mercury zephyr 6 1356.2412 8.417845 ## 297 amc spirit dl 1211.0916 11.648938 ## 19 datsun pl510 966.1517 11.478880 ## 320 mazda 626 1153.0318 13.306998 ## 218 buick opel isuzu deluxe 977.4916 12.754311 ## 1 chevrolet chevelle malibu 1589.3877 7.652587 ## 195 amc hornet 1399.3325 9.565733 ## 317 dodge aspen 1533.5958 8.120245 ## 35 plymouth satellite custom 1559.9042 6.802299 ## 356 honda prelude 1002.4391 14.327343 ## 250 oldsmobile cutlass salon brougham 1526.3383 8.460360 ## 373 pontiac phoenix 1240.5751 11.478880 ## 80 renault 12 (sw) 992.9137 11.053736 ## 201 ford granada ghia 1621.1391 7.652587 ## 202 pontiac ventura sj 1653.3442 7.865159 ## 59 dodge colt hardtop 964.3374 10.628593 ## 277 saab 99gle 1267.7907 9.183104 ## 108 amc gremlin 1265.0691 7.652587 ## 329 mercedes-benz 240d 1474.1752 12.754311 ## 220 plymouth arrow gs 1043.2625 10.841165 ## 209 plymouth volare premier v8 1787.1539 5.526868 ## 263 chevrolet monte carlo landau 1553.5539 8.162759 ## 178 audi 100ls 1221.9778 9.778305 ## 182 honda civic cvcc 814.1983 14.029742 ## 16 plymouth duster 1285.0272 9.353162 ## 191 ford gran torino 1911.8918 6.164584 ## 113 ford pinto 1047.7984 8.077730 ## 285 dodge aspen 6 1524.0704 8.757960 ## 49 ford mustang 1423.8264 7.652587 ## 243 bmw 320i 1179.3402 9.140590 ## 271 toyota celica gt liftback 1140.7848 8.970532 ## 349 toyota tercel 929.8644 16.027918 ## 339 plymouth reliant 1129.4450 11.563909 ## 309 pontiac phoenix 1159.3821 14.242314 ## 345 plymouth champ 850.4857 16.580605 ## 91 mercury marquis brougham 2246.1894 5.101724 ## 275 audi 5000 1283.6664 8.630417 ## 46 amc hornet sportabout (sw) 1343.5406 7.652587 ## 255 ford fairmont (auto) 1344.9014 8.587903 ## 7 chevrolet impala 1974.9412 5.952012 ## 378 plymouth horizon miser 963.8838 16.155461 ## 6 ford galaxie 500 1969.0445 6.377156 Y podríamos comenzar graficando rendimiento contra peso. Cada punto representa un coche distinto. En esta gráfica vemos que los valores de rendimiento varían según según peso de una manera sistemática: cuanto más grande es el peso, más bajo es el rendimiento: library(ggplot2) ggplot(datos_muestra, aes(x=peso_kg, y=rendimiento_kpl)) + geom_point() Podemos entonces ajustar una curva, que para cada nivel de peso da un valor de rendimiento que se ‘aleja lo menos posible’ de los valores de rendimiento cercanos. Por ejemplo: según la curva roja, ¿cómo haríamos la predicción para un peso de 1500 kg? ggplot(datos_muestra, aes(x=peso_kg, y=rendimiento_kpl)) + geom_point() + geom_smooth(se =FALSE, colour=&#39;red&#39;, size=1.1, span=0.4, method=&#39;loess&#39;) + geom_smooth(se =FALSE, colour=&#39;gray&#39;, size=1.1, span=2, method=&#39;loess&#39;) Aprendizaje no supervisado Aprendizaje no supervisado En este caso no hay target o variable salida. Buscamos modelar y entender las relaciones entre variables y entre observaciones, o patrones importantes o interesantes en los datos. Los problemas supervisados tienen un objetivo claro: hacer las mejores predicciones posibles bajo ciertas restricciones. Los problemas no supervisados tienden a tener objetivos más vagos, y por lo mismo pueden ser más difíciles. Ejemplo: tipos de coches en el mercado Quisieramos encontrar categorías de coches tales que: las categorías son diferentes entre sí, y los coches en una misma categoría son similares entre sí. Esta agrupación nos permite entender la estructura general de los datos, cómo están organizados en términos de similitud de características. En este ejemplo, encontramos un plano de máxima variabilidad donde proyectamos los coches, y después formamos grupos de coches similares: autos &lt;- Auto %&gt;% select(mpg, displacement, horsepower, acceleration) comps_autos &lt;- princomp(autos, cor = TRUE) clust &lt;- hclust(dist(comps_autos$scores[,1:2]), method = &#39;ward.D&#39;) autos$grupo &lt;- cutree(clust, k = 4) autos$Comp.1 &lt;- comps_autos$scores[,1] autos$Comp.2 &lt;- comps_autos$scores[,2] autos$nombre &lt;- Auto$name ggplot(autos, aes(x=Comp.1, y=Comp.2, colour=factor(grupo), label=nombre)) + geom_point() ¿Cómo interpretamos los grupos? head(filter(autos, grupo==1)) ## mpg displacement horsepower acceleration grupo Comp.1 Comp.2 ## 1 18 307 130 12.0 1 -1.817719 -0.5042535 ## 2 15 350 165 11.5 1 -2.800712 -0.3938195 ## 3 18 318 150 11.0 1 -2.310357 -0.7966085 ## 4 16 304 150 12.0 1 -2.213807 -0.3989781 ## 5 17 302 140 10.5 1 -2.225309 -0.9183779 ## 6 15 429 198 10.0 1 -3.900596 -0.6915313 ## nombre ## 1 chevrolet chevelle malibu ## 2 buick skylark 320 ## 3 plymouth satellite ## 4 amc rebel sst ## 5 ford torino ## 6 ford galaxie 500 head(filter(autos, grupo==3)) ## mpg displacement horsepower acceleration grupo Comp.1 Comp.2 ## 1 22 198 95 15.5 3 0.01913364 0.090471378 ## 2 18 199 97 15.5 3 -0.26705470 0.339015545 ## 3 21 200 85 16.0 3 0.16412490 0.315611651 ## 4 21 199 90 15.0 3 -0.05362631 0.004579963 ## 5 19 232 100 13.0 3 -0.79359758 -0.413938751 ## 6 16 225 105 15.5 3 -0.63973365 0.517394423 ## nombre ## 1 plymouth duster ## 2 amc hornet ## 3 ford maverick ## 4 amc gremlin ## 5 amc gremlin ## 6 plymouth satellite custom head(filter(autos, grupo==2)) ## mpg displacement horsepower acceleration grupo Comp.1 Comp.2 ## 1 24 113 95 15.0 2 0.50234800 -0.3800473 ## 2 27 97 88 14.5 2 0.79722704 -0.7509781 ## 3 24 107 90 14.5 2 0.52837050 -0.5437610 ## 4 26 121 113 12.5 2 -0.04757934 -1.2605758 ## 5 27 97 88 14.5 2 0.79722704 -0.7509781 ## 6 28 140 90 15.5 2 0.76454526 -0.4100595 ## nombre ## 1 toyota corona mark ii ## 2 datsun pl510 ## 3 audi 100 ls ## 4 bmw 2002 ## 5 datsun pl510 ## 6 chevrolet vega 2300 head(filter(autos, grupo==4)) ## mpg displacement horsepower acceleration grupo Comp.1 Comp.2 ## 1 26 97 46 20.5 4 2.2421696 1.1703377 ## 2 25 110 87 17.5 4 1.0737328 0.3205227 ## 3 25 104 95 17.5 4 0.9902507 0.3021997 ## 4 22 140 72 19.0 4 1.1727317 1.0419917 ## 5 30 79 70 19.5 4 2.0927389 0.5620939 ## 6 31 71 65 19.0 4 2.1920905 0.3319627 ## nombre ## 1 volkswagen 1131 deluxe sedan ## 2 peugeot 504 ## 3 saab 99e ## 4 chevrolet vega (sw) ## 5 peugeot 304 ## 6 toyota corolla 1200 1.2 Aprendizaje Supervisado Por el momento nos concentramos en problemas supervisados de regresión, es decir predicción de variables numéricas. ¿Cómo entendemos el problema de predicción? 1.2.1 Proceso generador de datos (modelo teórico) Para entender lo que estamos intentando hacer, pensaremos en términos de modelos probabilísticos que generan los datos. La idea es que estos representan los procesos que generan los datos o las observaciones. Si \\(Y\\) es la respuesta que queremos predecir, y \\(X\\) es una entrada que queremos usar para predecir \\(Y\\), consideramos que las variables aleatorias \\(Y\\) y \\(X\\) están relacionadas como sigue: \\[Y=f(X)+\\epsilon,\\] donde \\(\\epsilon\\) es una término de error aleatorio que no depende de \\(X\\), y que tiene valor esperado \\(\\textrm{E}(\\epsilon)=0\\). \\(f\\) expresa la relación sistemática que hay entre \\(Y\\) y \\(X\\): para cada valor posible de \\(X\\), la contribución de \\(X\\) a \\(Y\\) es \\(f(X)\\). Pero \\(X\\) no determina a \\(Y\\), como en el ejemplo anterior de rendimiento de coches. Entonces agregamos una error aleatorio \\(\\epsilon\\), con media cero (si la media no es cero podemos agregar una constante a \\(f\\)), que no contiene información acerca de \\(X\\) (independiente de \\(X\\)). \\(\\epsilon\\) representa, por ejemplo, el efecto de variables que no hemos medido o procesos aleatorios que determinan la respuesta. Ejemplo Vamos a usar simulación para entender estas ideas: supongamos que \\(X\\) es el número de años de estudio de una persona y \\(Y\\) es su ingreso mensual. En primer lugar, estas son el número de años de estudio de 8 personas: x &lt;- c(1,7,10,0,0,5,9,13,2,4,17,18,1,2) Ahora supondremos que la dependencia de Y de X está dada por \\(Y=f(X)+\\epsilon\\) por una función \\(f\\) que no conocemos (esta función está determinada por el fenómeno) f &lt;- function(x){ ifelse(x &lt; 10, 1000*sqrt(x), 1000*sqrt(10)) } El ingreso no se determina únicamente por número de años de estudio. Suponemos entonces que hay algunas variables adicional que perturba los niveles de \\(f(X)\\) por una cantidad aleatoria. Los valores que observamos de \\(Y\\) están dados entonces por \\(Y=f(X)+\\epsilon\\). Entonces podríamos obtener, por ejemplo: x_g &lt;- seq(0,20,0.5) y_g &lt;- f(x_g) dat_g &lt;- data.frame(x = x_g, y = y_g) set.seed(281) error &lt;- rnorm(length(x), 0, 500) y &lt;- f(x) + error datos &lt;- data_frame(x = x, y = y) datos$y_media &lt;- f(datos$x) ggplot(datos, aes(x = x, y = y)) + geom_point() + geom_line(data=dat_g, colour = &#39;blue&#39;, size = 1.1) + geom_segment(aes(x = x, xend = x, y = y, yend = y_media), col=&#39;red&#39;) En problemas de aprendizaje nunca conocemos esta \\(f\\) verdadera, aunque quizá sabemos algo acerca de sus propiedades (por ejemplo, continua, de variación suave). Lo que tenemos son los datos, que también podrían haber resultado en (para otra muestra de personas, por ejemplo): set.seed(28015) error &lt;- rnorm(length(x), 0, 500) y &lt;- f(x) + error datos &lt;- data.frame(x = x, y = y) ggplot(datos, aes(x = x, y = y)) + geom_point() La siguiente observación nos da una idea de lo que intentamos hacer, aunque todavía es vaga y requiere refinamiento: Bajo los supuestos del modelo Y = f(X)+ϵ, aprender de los datos significa intentar recuperar o estimar la forma de la función f que no conocemos. f representa la relación sistemática entre Y y X. ¿Qué tan bien podemos estimar esa \\(f\\) que no conocemos, con los datos disponibles? ¿Qué significa estimar bien? Incluso este ejemplo tan simple muestra las dificultades que vamos a enfrentar, y la importancia de determinar con cuidado qué tanta información tenemos, y qué tan buenas pueden ser nuestras predicciones. 1.3 Predicciones La idea es entonces producir una estimación de f que nos permita hacer predicciones. Si denotamos por \\(\\hat{f}\\) a una estimación de \\(f\\) construida a partir de los datos, podemos hacer predicciones aplicando \\(\\hat{f}\\) a valores de \\(X\\). La predicción de Y la denotamos por \\(\\hat{Y}\\), y \\[\\hat{Y}=\\hat{f}(X).\\] El error de predicción (residual) está dado por el valor observado menos la predicción: \\[Y-\\hat{Y}.\\] En nuestro ejemplo anterior, podríamos construir, por ejemplo, una recta ajustada por mínimos cuadrados: curva_1 &lt;- geom_smooth(data=datos, method = &quot;lm&quot;, se=FALSE, color=&quot;red&quot;, formula = y ~ x, size = 1.1) ggplot(datos, aes(x = x, y = y)) + geom_point() + curva_1 En este caso \\(\\hat{f}\\) es una recta, y la podemos usar para hacer predicciones. Por ejemplo, si tenemos una observación con \\(x_0=8\\) años de estudio, nuestra predicción del ingreso \\(\\hat{y}=\\hat{f}(8)\\) sería lineal &lt;- lm(y ~ x,data = datos) pred_1 &lt;- predict(lineal, newdata = data.frame(x=8)) pred_1 ## 1 ## 2193.561 ggplot(datos, aes(x = x, y = y)) + geom_point() + curva_1 + geom_segment(x = 0, xend = 8, y = pred_1, yend = pred_1, colour = &#39;salmon&#39;) + geom_segment(x = 8, xend = 8, y = 0, yend = pred_1, colour = &#39;salmon&#39;) + annotate(&#39;text&#39;, x = 0.5, y = pred_1 + 100, label = round(pred_1, 1)) + geom_point( x= 8, y =3200, col=&#39;green&#39;, size = 4) Si observamos que para esta observación con \\(x_0=8\\), resulta que el correspondiente ingreso es \\(y_0=3200\\), entonces el error sería y_0 &lt;- 3200 y_0 - pred_1 ## 1 ## 1006.439 En aprendizaje buscamos que estos errores sean lo más cercano a cero que sea posible. 1.4 Cuantificación de error o precisión El elemento faltante para definir la tarea de aprendizaje supervisado es qué significa aproximar bien a \\(f\\), o tener predicciones precisas. Para esto definimos una función de pérdida: \\[L(Y, \\hat{f}(X)),\\] que nos dice cuánto nos cuesta hacer la predicción \\(\\hat{f}(X)\\) cuando el verdadero valor es \\(Y\\) y las variables de entrada son \\(X\\). Una opción conveniente para problemas de regresión es la pérdida cuadrática: \\[L(Y, \\hat{f}(X)) = (Y - \\hat{f}(X))^2\\] Esta es una cantidad aleatoria, de modo que en algunos casos este error puede ser más grande o más chico. Usualmente buscamos una \\(\\hat{f}\\) de modo que el error promedio sea chico: \\[Err = E (Y - \\hat{f}(X))^2 \\] Nota: Intenta demostrar que bajo error cuadrático medio y suponiendo el modelo aditivo \\(Y=f(X)+\\epsilon\\), el mejor predictor de \\(Y\\) es \\(f(x)= E[Y|X=x]\\). Es decir: lo que nos interesa es aproximar lo mejor que se pueda la esperanza condicional 1.5 Tarea de aprendizaje supervisado Ahora tenemos los elementos para definir con precisión el problema de aprendizaje supervisado. Consideramos un proceso generador de datos \\((X,Y)\\). En primer lugar, tenemos datos de los que vamos a aprender. Supongamos que tenemos un conjunto de datos etiquetados (generados según \\((X,Y)\\)) \\[{\\mathcal L}=\\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \\ldots, (x^{(N)}, y^{(N)}) \\}\\] que llamamos conjunto de entrenamiento. Nótese que usamos minúsculas para denotar observaciones particulares de \\((X,Y)\\). Un algoritmo de aprendizaje es una regla que asigna a cada conjunto de entrenamiento \\({\\mathcal L}\\) una función \\(\\hat{f}\\): \\[{\\mathcal L} \\to \\hat{f}.\\] El desempeño del predictor particular \\(\\hat{f}\\) se mide como sigue: si en el futuro observamos otra muestra \\({\\mathcal T}\\) (que podemos llamar muestra de prueba) \\[{\\mathcal T}=\\{ (x_0^{(1)},y_0^{(1)}),(x_0^{(2)},y_0^{(2)}), \\ldots, (x_0^{(m)}, y_0^{(m)}) \\}\\] entonces decimos que el error de predicción (cuadrático) de \\(\\hat{f}\\) para el ejemplo \\((x_0^{(j)},y_0^{(j)})\\) está dado por \\[(y_0^{(j)} - \\hat{f}(x_0^{(j)}))^2\\] y el error sobre la muestra \\({\\mathcal T}\\) es \\[\\hat{Err} = \\frac{1}{m}\\sum_{j=1}^m (y_0^{(j)} - \\hat{f}(x_0^{(j)}))^2\\] Es muy importante considerar dos muestras separadas en esta definición: No tiene mucho sentido medir el desempeño de nuestro algoritmo sobre la muestra de entrenamiento, pues el algoritmo puede ver las etiquetas. Considerar el error sobre una muestra diferente a la de entrenamiento nos permite evaluar si nuestro algoritmo generaliza, que se puede pensar como “verdadero” aprendizaje. Nótese que \\(\\hat{Err}\\) es una estimación de \\(Err\\) (por la ley de los grandes números, si \\({\\mathcal T}\\) es muestra i.i.d. de \\((X,Y)\\)). También consideramos el error de entrenamiento, dado por \\[\\overline{err} = \\frac{1}{N}\\sum_{i=1}^N (y^{(i)} - \\hat{f}(x^{(i)}))^2\\] - Pregunta: ¿Por qué \\(\\overline{err}\\) no necesariamente es una buena estimación de \\(Err\\)? 1.5.0.1 Ejemplo En el ejemplo que hemos estado usando, ¿que curva preferirías para predecir, la gris, la roja o la azul? ¿Cuál tiene menor error de entrenamiento? set.seed(280572) error &lt;- rnorm(length(x), 0, 500) y &lt;- f(x) + error datos_entrena &lt;- data.frame(x=x, y=y) head(datos_entrena) ## x y ## 1 1 86.22033 ## 2 7 2353.75863 ## 3 10 3078.71029 ## 4 0 -397.80229 ## 5 0 424.73363 ## 6 5 3075.92998 curva.1 &lt;- geom_smooth(data=datos_entrena, method = &quot;loess&quot;, se=FALSE, color=&quot;gray&quot;, span=1, size=1.1) curva.2 &lt;- geom_smooth(data=datos_entrena, method = &quot;loess&quot;, se=FALSE, color=&quot;red&quot;, span=0.3, size=1.1) curva.3 &lt;- geom_smooth(data=datos_entrena, method = &quot;lm&quot;, se=FALSE, color=&quot;blue&quot;, size=1.1) ggplot(datos_entrena, aes(x=x, y=y)) + geom_point() + curva.1 + curva.2 + curva.3 Calculamos los errores de entrenamiento de cada curva: mod_rojo &lt;- loess(y ~ x, data = datos_entrena, span=0.3) mod_gris &lt;- loess(y ~ x, data = datos_entrena, span=1) mod_recta &lt;- lm(y ~ x, data = datos_entrena) df_mods &lt;- data_frame(nombre = c(&#39;recta&#39;, &#39;rojo&#39;,&#39;gris&#39;)) df_mods$modelo &lt;- list(mod_recta, mod_rojo, mod_gris) error_f &lt;- function(df){ function(mod){ preds &lt;- predict(mod, newdata = df) round(sqrt(mean((preds-df$y)^2))) } } error_ent &lt;- error_f(datos_entrena) df_mods &lt;- df_mods %&gt;% mutate(error_entrena = map_dbl(modelo, error_ent)) df_mods ## # A tibble: 3 x 3 ## nombre modelo error_entrena ## &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; ## 1 recta &lt;S3: lm&gt; 782 ## 2 rojo &lt;S3: loess&gt; 189 ## 3 gris &lt;S3: loess&gt; 389 El error de entrenamiento es considerablemente menor para la curva roja, y es más grande para la recta. Sin embargo, consideremos que tenemos una nueva muestra (de prueba). set.seed(218052272) x_0 &lt;- sample(0:13, 100, replace = T) error &lt;- rnorm(length(x_0), 0, 500) y_0 &lt;- f(x_0) + error datos_prueba &lt;- data_frame(x = x_0, y = y_0) datos_prueba ## # A tibble: 100 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 9 2156.1160 ## 2 11 3227.0968 ## 3 3 2382.2805 ## 4 10 3481.5850 ## 5 7 2732.8020 ## 6 7 2325.9217 ## 7 12 3463.7795 ## 8 0 -563.9233 ## 9 10 3295.5705 ## 10 0 365.9880 ## # ... with 90 more rows error_p &lt;- error_f(datos_prueba) df_mods &lt;- df_mods %&gt;% mutate(error_prueba = map_dbl(modelo, error_p)) df_mods ## # A tibble: 3 x 4 ## nombre modelo error_entrena error_prueba ## &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 recta &lt;S3: lm&gt; 782 801 ## 2 rojo &lt;S3: loess&gt; 189 628 ## 3 gris &lt;S3: loess&gt; 389 520 1.5.1 Observaciones El mejor modelo entrenamiento es uno que “sobreajusta” a los datos, pero es el peor con una muestra de prueba. La curva roja aprende del una componente de ruido del modelo - lo cual realmente no es aprendizaje. El modelo de la recta no es bueno en entrenamiento ni en prueba. Este modelo no tiene la capacidad para aprender de la señal en los datos. El mejor modelo en la muestra de prueba es uno que está entre la recta y la curva roja en términos de flexibilidad. Nuestra intuición para escoger el modelo gris desde el principio se refleja en que generaliza mejor que los otros, y eso a su vez se refleja en un error de prueba más bajo. 1.6 ¿Por qué tenemos errores? ¿De dónde provienen los errores en la predicción? Si establemos que el error es una función creciente de \\(Y-\\hat{Y}\\), vemos que \\[ Y-\\hat{Y} = f(X) + \\epsilon - \\hat{f}(X)= (f(X) - \\hat{f}(X)) + \\epsilon,\\] donde vemos que hay dos componentes que pueden hacer grande a \\(Y-\\hat{Y}\\): La diferencia \\(f(X) - \\hat{f}(X)\\) está asociada a error reducible, pues depende de qué tan bien estimemos \\(f(X)\\) con \\(\\hat{f}(X)\\) El error aleatorio \\(\\epsilon\\), asociado a error irreducible. Cualquiera de estas dos cantidades pueden hacer que nuestras predicciones no sean precisas. En nuestro ejemplo anterior, el error reducible: Es grande para el modelo rojo, pues responde demasiado fuerte a ruido en los datos (tiene varianza alta). Es grande para el modelo de la recta, pues no tiene capacidad para acercarse a la verdadera curva (está sesgado). En aprendizaje supervisado, nuestro objetivo es reducir el error reducible tanto como sea posible (obtener la mejor estimación de f). No podemos hacer nada acerca del error irreducible, pues este se debe a aleatoriedad en el fenómeno o a variables que no conocemos. Notación Las observaciones o datos que usaremos para construir nuestras estimaciones las denotamos como sigue. Cada observación (o caso, o ejemplo) está dada por el valor de una variable de entrada \\(X\\) y un valor de la variable de salida \\(Y\\). Cuando tenemos \\(N\\) ejemplos de entrenamiento, los escribimos como los pares \\[(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}) \\ldots, (x^{(N)},y^{(N)})\\]. Cuando los datos de entrada contienen \\(p\\) variables o atributos, escribimos \\[x^{(i)} = (x_1^{(i)}, x_2^{(i)},\\ldots, x_p^{(i)})\\] Escribimos también la matriz de entradas de dimensión Nxp: \\[\\underline{X} = \\left ( \\begin{array}{cccc} x_1^{(1)} &amp; x_2^{(1)} &amp; \\ldots &amp; x_p^{(1)} \\\\ x_1^{(2)} &amp; x_2^{(2)} &amp; \\ldots &amp; x_p^{(2)}\\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ x_1^{(N)} &amp; x_2^{(N)} &amp; \\ldots &amp; x_p^{(N)} \\\\ \\end{array} \\right)\\] y \\[\\underline{y} =(y^{(1)},y^{(2)}, \\ldots, y^{(N)})^t.\\] Adicionalmente, usamos la notación \\[{\\mathcal L}=\\{ (x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \\ldots, (x^{(N)},y^{(N)}) \\}\\] para denotar al conjunto de datos con los que construimos nuestro modelo. A este conjunto le llamaremos conjunto o muestra de entrenamiento (learning set) 1.7 ¿Cómo estimar f? Los métodos para construir la estimación \\(\\hat{f}\\) se dividen en dos grandes grupos: paramétricos y no paramétricos. 1.7.0.1 Métodos paramétricos En los métodos paramétricos seleccionamos, usando los datos, una \\(\\hat{f}\\) de una colección de modelos que pueden ser descritos por un número fijo de parámetros. Por ejemplo, podríamos establecer que la función \\(f\\) tiene la forma: \\[f(x_1,x_2) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2,\\] que son funciones lineales en dos variables. En este caso, tenemos tres parámetros \\((\\beta_0,\\beta_1,\\beta_2)\\), que describen a la familia completa. Usando los datos de entrenamiento, entrenamos este modelo para encontrar \\((\\beta_0,\\beta_1,\\beta_2)\\) tales que \\[y^{(i)} \\approx \\beta_0 + \\beta_1 x_1^{(i)} + \\beta_2 x_2^{(i)},\\] es decir, el modelo ajustado regresa valores cercanos a los observados. Esta aproximación se traducirá en un problema de optimización. En general, este enfoque es muy tratable numéricamente pues el problema se reduce a estimar tres valores numéricos, en lugar de intentar estimar una función \\(f\\) arbitraria. Su desventaja es que quizá ningún miembro familia de modelos establecida (por ejemplo, modelos lineales) puede aproximar razonablemente bien a la verdadera función \\(f\\). Es decir, estos métodos tienen sesgo potencial grande. Por ejemplo: mod_1 &lt;- lm(mpg~year+weight, data=Auto) Auto$fit &lt;- predict(mod_1) weight &lt;- seq(1600, 5200, by=100) year &lt;- seq(70,82, by=1) z &lt;- outer(weight, year, function(x,y){ predict(mod_1, newdata=data.frame(weight=x,year=y))}) layout(t(1:2)) res &lt;- persp(weight, year, z, theta=50,phi=10, col=&#39;red&#39;, lty=1) points(trans3d(x=Auto[,&#39;weight&#39;],y=Auto[,&#39;year&#39;],z= Auto$mpg, pmat=res), col=2-(residuals(mod_1)&gt;0), cex=0.5,pch=16) res &lt;- persp(weight, year, z, theta=-20,phi=10, col=&#39;red&#39;, lty=1) points(trans3d(x=Auto[,&#39;weight&#39;],y=Auto[,&#39;year&#39;],z= Auto$mpg, pmat=res), col=2-(residuals(mod_1)&gt;0), cex=0.5,pch=16) dat.grid &lt;- expand.grid(weight=weight, year = year) values &lt;- predict(mod_1, dat.grid) dat.grid$pred.mpg &lt;- values ggplot(dat.grid, aes(x=weight, y=pred.mpg)) + facet_wrap(~year,nrow=2)+ geom_point(data=Auto, aes(x=weight, y=mpg),alpha=0.5)+ geom_line(colour=&#39;red&#39;,size=1.1) + geom_hline(yintercept=30, col=&#39;gray&#39;) Por otro lado, si la estructura rígida de estos modelos describe aproximadamente el comportamiento de los datos, estos modelos nos pueden proteger contra sobreajustar los datos, es decir, incorporar en nuestra estimación de \\(\\hat{f}\\) aspectos del error (\\(\\epsilon\\)), lo que tiene como consecuencia también predicciones pobres. Métodos no paramétricos Los métodos no paramétricos suponen menos acerca de la forma funcional de \\(f\\), y su número de parámetros depende del tamaño de los datos que estamos considerando. Potencialmente, estos métodos pueden aproximar formas funcionales mucho más generales, pero típicamente requieren de más datos para obtener resultados razonables. 1.7.0.2 Ejemplo En este ejemplo usamos regresión loess, que funciona haciendo promedios locales ponderados de puntos de entrenamiento cercanos a donde queremos hacer predicciones. La ponderación se hace en función de la distancia de los puntos de entrenamiento al punto donde queremos predecir. mod_2 &lt;- loess(mpg~year+weight, data=Auto, family=&#39;symmetric&#39;, degree=1, span=0.15) Auto$fit &lt;- predict(mod_2) weight &lt;- seq(1600, 5200, by=100) year &lt;- seq(70,82, by=1) dat.grid &lt;- expand.grid(weight = weight, year = year) values &lt;- predict(mod_2, dat.grid) %&gt;% data.frame %&gt;% gather(year, value, year.70:year.82) dat.grid$pred.mpg &lt;- values$value ggplot(dat.grid, aes(x=weight, y=pred.mpg)) + facet_wrap(~year,nrow=2)+ geom_point(data=Auto, aes(x=weight, y=mpg),alpha=0.5)+ geom_line(colour=&#39;red&#39;,size=1.1) + geom_hline(yintercept=30, col=&#39;gray&#39;) 1.8 Resumen Aprendizaje de máquina: algoritmos que aprenden de los datos para predecir cantidades numéricas, o clasificar (aprendizaje supervisado), o para encontrar estructura en los datos (aprendizaje no supervisado). En aprendizaje supervisado, el esquema general es: Un algoritmo aprende de una muestra de entrenamiento \\({\\mathcal L}\\), que es generada por el proceso generador de datos que nos interesa. Eso quiere decir que produce una función \\(\\hat{f}\\) (a partir de \\({\\mathcal L}\\)) que nos sirve para hacer predicciones \\(x \\to \\hat{f}(x)\\) de \\(y\\) El error de predicción del algoritmo es \\(Err\\), que mide en promedio qué tan lejos están las predicciones de valores reales. Para estimar esta cantidad usamos una muestra de prueba \\({\\mathcal T}\\), que es independiente de \\({\\mathcal L}\\). Esta es porque nos interesa el desempeño futuro de \\(\\hat{f}\\) para nuevos casos que el algoritmo no ha visto (esto es aprender). El error en la muestra de entrenamiento no necesariamente es buen indicador del desempeño futuro de nuestro algoritmo. Para obtener las mejores predicciones posibles, es necesario que el algoritmo sea capaz de capturar patrones en los datos, pero no tanto que tienda a absorber ruido en la estimación - es un balance de complejidad y rigidez. En términos estadísticos, se trata de un balance de varianza y sesgo. Hay dos tipos de métodos generales: paramétricos y no paramétricos. Los paramétricos seleccionan un número finito de parámetros para construir \\(\\hat{f}\\), el número de parámetros de los no paramétricos depende del conjunto de entrenamiento. 1.9 Tarea En el ejemplo simple que vimos en la sección 1.5, utilizamos una sola muestra de entrenamiento para evaluar el algoritmo. ¿Será posible que escogimos una muestra atípica? - Corre el ejemplo con otra muestra y reporta tus resultados de error de entrenamiento y error de prueba para los tres métodos. - Opcional (difícil): evalúa los tres métodos comparando estos valores para un número grande de distintas simulaciones de los datos de entrenamiento. "],
["regresion.html", "Clase 2 Regresión lineal 2.1 Introducción 2.2 Aprendizaje de coeficientes (ajuste) 2.3 Descenso en gradiente 2.4 Descenso en gradiente para regresión lineal 2.5 Normalización de entradas 2.6 Interpretación de modelos lineales 2.7 Solución analítica 2.8 ¿Por qué el modelo lineal funciona bien (muchas veces)? Tarea", " Clase 2 Regresión lineal 2.1 Introducción Consideramos un problema de regresión con entradas \\(X=(X_1,X_2,\\ldots, X_p)\\) y salida \\(Y\\). Una de las maneras más simples que podemos intentar para predecir \\(Y\\) en función de las \\(X_j\\)´s es mediante una suma ponderada de los valores de las \\(X_j&#39;s\\), usando una función \\[f_\\beta (X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p,\\] Nuestro trabajo será entonces, dada una muestra de entrenamiento \\({\\mathcal L}\\), encontrar valores apropiados de las \\(\\beta\\)’s, para construir un predictor: \\[\\hat{f}(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\hat{\\beta}_2 X_2 \\cdots + \\hat{\\beta} X_p\\] y usaremos esta función \\(\\hat{f}\\) para hacer predicciones \\(\\hat{Y} =\\hat{f}(X)\\). 2.1.0.1 Ejemplos Queremos predecir las ventas futuras anuales \\(Y\\) de un supermercado que se va a construir en un lugar dado. Las variables que describen el lugar son \\(X_1 = trafico\\_peatones\\), \\(X_2=trafico\\_coches\\). En una aproximación simple, podemos suponer que la tienda va a capturar una fracción de esos tráficos que se van a convertir en ventas. Quisieramos predecir con una función de la forma \\[f_\\beta (peatones, coches) = \\beta_0 + \\beta_1\\, peatones + \\beta_2\\, coches.\\] Por ejemplo, después de un análisis estimamos que \\(\\hat{\\beta}_0 = 1000000\\) (ventas base) \\(\\hat{\\beta}_1 = (200)*0.02 = 4\\) \\(\\hat{\\beta}_2 = (300)*0.01 =3\\) Entonces haríamos predicciones con \\[\\hat{f}(peatones, coches) = 1000000 + 4\\,peatones + 3\\, coches\\] El modelo lineal es más flexible de lo que parece en una primera aproximación, porque tenemos libertad para construir las variables de entrada a partir de nuestros datos. Por ejemplo, si tenemos una tercera variable \\(estacionamiento\\) que vale 1 si hay un estacionamiento cerca o 0 si no lo hay, podríamos definir las variables \\(X_1= peatones\\) \\(X_2 = coches\\) \\(X_3 = estacionamiento\\) \\(X_4 = coches*estacionamiento\\) Donde la idea de agregar \\(X_4\\) es que si hay estacionamiento entonces vamos a capturar una fracción adicional del trafico de coches, y la idea de \\(X_3\\) es que la tienda atraerá más nuevas visitas si hay un estacionamiento cerca. Buscamos ahora modelos de la forma \\[f_\\beta(X_1,X_2,X_3,X_4) = \\beta_0 + \\beta_1X_1 + \\beta_2 X_2 + \\beta_3 X_3 +\\beta_4 X_4\\] y podríamos obtener después de nuestra análisis las estimaciones \\(\\hat{\\beta}_0 = 800000\\) (ventas base) \\(\\hat{\\beta}_1 = 4\\) \\(\\hat{\\beta}_2 = (300)*0.005 = 1.5\\) \\(\\hat{\\beta}_3 = 400000\\) \\(\\hat{\\beta}_4 = (300)*0.02 = 6\\) y entonces haríamos predicciones con el modelo \\[\\hat{f} (X_1,X_2,X_3,X_4) = 800000 + 4\\, X_1 + 1.5 \\,X_2 + 400000\\, X_3 +6\\, X_4\\] 2.2 Aprendizaje de coeficientes (ajuste) En el ejemplo anterior, los coeficientes fueron calculados (o estimados) usando experiencia, argumentos teóricos, o quizá otras fuentes de datos (como estudios o encuestas, conteos, etc.) Ahora quisiéramos construir un algoritmo para aprender estos coeficientes del modelo \\[f_\\beta (X_1) = \\beta_0 + \\beta_1 X_1 + \\cdots \\beta_p X_p\\] a partir de una muestra de entrenamiento \\[{\\mathcal L}=\\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \\ldots, (x^{(N)}, y^{(N)}) \\}\\] El criterio de ajuste (algoritmo de aprendizaje) más usual para regresión lineal es el de mínimos cuadrados. Construimos las predicciones (ajustados) para la muestra de entrenamiento: \\[\\hat{y}^{(i)} = f_\\beta (x^{(i)}) = \\beta_0 + \\beta_1 x_1^{(i)}+ \\cdots + \\beta_p x_p^{(i)}\\] Y consideramos las diferencias de los ajustados con los valores observados: \\[e^{(i)} = y^{(i)} - f_\\beta (x^{(i)})\\] La idea entonces es minimizar la suma de los residuales al cuadrado, para intentar que la función ajustada pase lo más cercana a los puntos de entrenamiento que sea posible. Si \\[RSS(\\beta) = \\sum_{i=1}^N (y^{(i)} - f_\\beta(x^{(i)}))^2\\] Queremos resolver Mínimos cuadrados \\[\\min_{\\beta} RSS(\\beta) = \\min_{\\beta}\\sum_{i=1}^N (y^{(i)} - f_\\beta(x^{(i)}))^2\\] 2.2.0.1 Ejemplo Consideremos library(readr) library(dplyr) library(knitr) prostata &lt;- read_csv(&#39;datos/prostate.csv&#39;) %&gt;% select(lcavol, lpsa, train) kable(head(prostata), format = &#39;html&#39;) lcavol lpsa train -0.5798185 -0.4307829 TRUE -0.9942523 -0.1625189 TRUE -0.5108256 -0.1625189 TRUE -1.2039728 -0.1625189 TRUE 0.7514161 0.3715636 TRUE -1.0498221 0.7654678 TRUE prostata_entrena &lt;- filter(prostata, train) ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() En este caso, buscamos ajustar el modelo (tenemos una sola entrada) \\(f_{\\beta} (X_1) = \\beta_0 + \\beta_1 X_1\\), que es una recta. Los cálculos serían como sigue: rss_calc &lt;- function(datos){ y &lt;- datos$lpsa x &lt;- datos$lcavol fun_out &lt;- function(beta){ y_hat &lt;- beta[1] + beta[2]*x e &lt;- (y - y_hat) rss &lt;- sum(e^2) 0.5*rss } fun_out } Nuestra función rss es entonces: rss &lt;- rss_calc(prostata_entrena) Por ejemplo, si consideramos \\((\\beta_0, \\beta_1) = (0, 1.5)\\), obtenemos beta &lt;- c(0,1.5) rss(beta) ## [1] 61.63861 Que corresponde a la recta ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() + geom_abline(slope = beta[2], intercept = beta[1], col =&#39;red&#39;) Podemos comparar con \\((\\beta_0, \\beta_1) = (1, 1)\\), obtenemos beta &lt;- c(1,1) rss(beta) ## [1] 27.11781 ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() + geom_abline(slope = beta[2], intercept = beta[1], col =&#39;red&#39;) Ahora minimizamos. Podríamos hacer res_opt &lt;- optim(c(0,0), rss, method = &#39;BFGS&#39;) beta_hat &lt;- res_opt$par beta_hat ## [1] 1.5163048 0.7126351 res_opt$convergence ## [1] 0 ggplot(prostata_entrena, aes(x = lcavol, y = lpsa)) + geom_point() + geom_abline(slope = 1, intercept = 1, col =&#39;red&#39;) + geom_abline(slope = beta_hat[2], intercept = beta_hat[1]) 2.3 Descenso en gradiente Aunque el problema de mínimos cuadrados se puede resolver analíticamente, proponemos un método numérico básico que es efectivo y puede escalarse a problemas grandes de manera relativamente simple: descenso en gradiente, o descenso máximo. Supongamos que una función \\(h(x)\\) es convexa y tiene un mínimo. La idea de descenso en gradiente es comenzar con un candidato inicial \\(z_0\\) y calcular la derivada en \\(z^{(0)}\\). Si \\(h(z^{(0)})&gt;0\\), la función es creciente en \\(z^{(0)}\\) y nos movemos ligeramente a la izquierda para obtener un nuevo candidato \\(z^{(1)}\\). si \\(h(z^{(0)})&lt;0\\), la función es decreciente en \\(z^{(0)}\\) y nos movemos ligeramente a la derecha para obtener un nuevo candidato \\(z^{(1)}\\). Iteramos este proceso hasta que la derivada es cercana a cero (estamos cerca del óptimo). Si \\(\\eta&gt;0\\) es una cantidad chica, podemos escribir \\[z^{(1)} = z^{(0)} - \\eta \\,h&#39;(z^{(0)}).\\] Nótese que cuando la derivada tiene magnitud alta, el movimiento de \\(z^{(0)}\\) a \\(z^{(1)}\\) es más grande, y siempre nos movemos una fracción de la derivada. En general hacemos \\[z^{(j+1)} = z^{(j)} - \\eta\\,h&#39;(z^{(j)})\\] para obtener una sucesión \\(z^{(0)},z^{(1)},\\ldots\\). Esperamos a que \\(z^{(j)}\\) converja para terminar la iteración. 2.3.0.1 Ejemplo Si tenemos h &lt;- function(x) x^2 + (x - 2)^2 - log(x^2 + 1) Calculamos (a mano): h_deriv &lt;- function(x) 2 * x + 2 * (x - 2) - 2*x/(x^2 + 1) Ahora iteramos con \\(\\eta = 0.4\\) y valor inicial \\(z_0=5\\) z_0 &lt;- 5 eta &lt;- 0.4 descenso &lt;- function(n, z_0, eta, h_deriv){ z &lt;- matrix(0,n, length(z_0)) z[1, ] &lt;- z_0 for(i in 1:(n-1)){ z[i+1, ] &lt;- z[i, ] - eta * h_deriv(z[i, ]) } z } z &lt;- descenso(20, 5, 0.1, h_deriv) z ## [,1] ## [1,] 5.000000 ## [2,] 3.438462 ## [3,] 2.516706 ## [4,] 1.978657 ## [5,] 1.667708 ## [6,] 1.488834 ## [7,] 1.385872 ## [8,] 1.326425 ## [9,] 1.291993 ## [10,] 1.272002 ## [11,] 1.260375 ## [12,] 1.253606 ## [13,] 1.249663 ## [14,] 1.247364 ## [15,] 1.246025 ## [16,] 1.245243 ## [17,] 1.244788 ## [18,] 1.244523 ## [19,] 1.244368 ## [20,] 1.244277 Y vemos que estamos cerca de la convergencia. curve(h, -3, 6) points(z[,1], h(z)) text(z[1:6], h(z[1:6]), pos = 3) 2.3.1 Selección de tamaño de paso \\(\\eta\\) Si hacemos \\(\\eta\\) muy chico, el algoritmo puede tardar mucho en converger: z &lt;- descenso(20, 5, 0.01, h_deriv) curve(h, -3, 6) points(z, h(z)) text(z[1:6], h(z[1:6]), pos = 3) Si hacemos \\(\\eta\\) muy grande, el algoritmo puede divergir: z &lt;- descenso(20, 5, 1.5, h_deriv) z ## [,1] ## [1,] 5.000000e+00 ## [2,] -1.842308e+01 ## [3,] 9.795302e+01 ## [4,] -4.837345e+02 ## [5,] 2.424666e+03 ## [6,] -1.211733e+04 ## [7,] 6.059265e+04 ## [8,] -3.029573e+05 ## [9,] 1.514792e+06 ## [10,] -7.573955e+06 ## [11,] 3.786978e+07 ## [12,] -1.893489e+08 ## [13,] 9.467445e+08 ## [14,] -4.733723e+09 ## [15,] 2.366861e+10 ## [16,] -1.183431e+11 ## [17,] 5.917153e+11 ## [18,] -2.958577e+12 ## [19,] 1.479288e+13 ## [20,] -7.396442e+13 Es necesario ajustar el tamaño de paso para cada problema particular. Si la convergencia es muy lenta, podemos incrementarlo. Si las iteraciones divergen, podemos disminuirlo 2.3.2 Funciones de varias variables Si ahora \\(h(z)\\) es una función de \\(p\\) variables, podemos intentar la misma idea usando el gradiente. Por cálculo sabemos que el gradiente apunta en la dirección de máximo crecimiento local. El gradiente es el vector columna con las derivadas parciales de \\(h\\): \\[\\nabla h(z) = \\left( \\frac{\\partial h}{\\partial z_1}, \\frac{\\partial h}{\\partial z_2}, \\ldots, \\frac{\\partial h}{\\partial z_p} \\right)^t\\] Y el paso de iteración, dado un valor inicial \\(z_0\\) y un tamaño de paso \\(\\eta &gt;0\\) es \\[z^{(i+1)} = z^{(i)} - \\eta \\nabla h(z^{(i)})\\] Las mismas consideraciones acerca del tamaño de paso \\(\\eta\\) aplican en el problema multivariado. h &lt;- function(z) { z[1]^2 + z[2]^2 - z[1] * z[2] } h_gr &lt;- function(z_1,z_2) apply(cbind(z_1, z_2), 1, h) grid_graf &lt;- expand.grid(z_1 = seq(-3, 3, 0.1), z_2 = seq(-3, 3, 0.1)) grid_graf &lt;- grid_graf %&gt;% mutate( val = apply(cbind(z_1,z_2), 1, h)) gr_contour &lt;- ggplot(grid_graf, aes(x = z_1, y = z_2, z = val)) + geom_contour(binwidth = 1.5, aes(colour = ..level..)) gr_contour El gradiente está dado por h_grad &lt;- function(z){ c(2*z[1] - z[2], 2*z[2] - z[1]) } Podemos graficar la dirección de máximo descenso para diversos puntos. Estas direcciones son ortogonales a la curva de nivel que pasa por cada uno de los puntos: grad_1 &lt;- h_grad(c(0,-2)) grad_2 &lt;- h_grad(c(1,1)) eta &lt;- 0.2 gr_contour + geom_segment(aes(x=0.0, xend=0.0-eta*grad_1[1], y=-2, yend=-2-eta*grad_1[2]), arrow = arrow(length = unit(0.2,&quot;cm&quot;)))+ geom_segment(aes(x=1, xend=1-eta*grad_2[1], y=1, yend=1-eta*grad_2[2]), arrow = arrow(length = unit(0.2,&quot;cm&quot;)))+ coord_fixed(ratio = 1) Y aplicamos descenso en gradiente: inicial &lt;- c(3, 1) iteraciones &lt;- descenso(20, inicial , 0.1, h_grad) iteraciones ## [,1] [,2] ## [1,] 3.0000000 1.0000000 ## [2,] 2.5000000 1.1000000 ## [3,] 2.1100000 1.1300000 ## [4,] 1.8010000 1.1150000 ## [5,] 1.5523000 1.0721000 ## [6,] 1.3490500 1.0129100 ## [7,] 1.1805310 0.9452330 ## [8,] 1.0389481 0.8742395 ## [9,] 0.9185824 0.8032864 ## [10,] 0.8151946 0.7344874 ## [11,] 0.7256044 0.6691094 ## [12,] 0.6473945 0.6078479 ## [13,] 0.5787004 0.5510178 ## [14,] 0.5180621 0.4986843 ## [15,] 0.4643181 0.4507536 ## [16,] 0.4165298 0.4070347 ## [17,] 0.3739273 0.3672807 ## [18,] 0.3358699 0.3312173 ## [19,] 0.3018177 0.2985609 ## [20,] 0.2713102 0.2690305 ggplot(data= grid_graf) + geom_contour(binwidth = 1.5, aes(x = z_1, y = z_2, z = val, colour = ..level..)) + geom_point(data = data.frame(iteraciones), aes(x=X1, y=X2), colour = &#39;red&#39;) 2.4 Descenso en gradiente para regresión lineal Vamos a escribir ahora el algoritmo de descenso en gradiente para regresión lineal. Igual que en los ejemplos anteriores, tenemos que precalcular el gradiente. Una vez que esto esté terminado, escribir la iteración es fácil. Recordamos que queremos minimizar (dividiendo entre dos para simplificar más adelante) \\[RSS(\\beta) = \\frac{1}{2}\\sum_{i=1}^N (y^{(i)} - f_\\beta(x^{(i)}))^2\\] La derivada de la suma es la suma de las derivadas, así nos concentramos en derivar uno de los términos \\[ \\frac{1}{2}(y^{(i)} - f_\\beta(x^{(i)}))^2 \\] Usamos la regla de la cadena para obtener \\[ \\frac{1}{2}\\frac{\\partial}{\\partial \\beta_j} (y^{(i)} - f_\\beta(x^{(i)}))^2 = -(y^{(i)} - f_\\beta(x^{(i)})) \\frac{\\partial f_\\beta(x^{(i)})}{\\partial \\beta_j}\\] Ahora recordamos que \\[f_{\\beta} (x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p\\] Y vemos que tenemos dos casos. Si \\(j=0\\), \\[\\frac{\\partial f_\\beta(x^{(i)})}{\\partial \\beta_0} = 1\\] y si \\(j=1,2,\\ldots, p\\) entonces \\[\\frac{\\partial f_\\beta(x^{(i)})}{\\partial \\beta_j} = x_j^{(i)}\\] Entonces: \\[\\frac{\\partial f_\\beta(x^{(i)})}{\\partial \\beta_0} = -(y^{(i)} - f_\\beta(x^{(i)}))\\] y \\[\\frac{\\partial f_\\beta(x^{(i)})}{\\partial \\beta_j} = - x_j^{(i)}(y^{(i)} - f_\\beta(x^{(i)}))\\] Y sumando todos los términos (uno para cada caso de entrenamiento): Gradiente para regresión lineal Sea \\(e^{(i)} = y_{(i)} - f_{\\beta} (x^{(i)})\\). Entonces \\[\\begin{equation} \\frac{\\partial RSS(\\beta)}{\\partial \\beta_0} = - \\sum_{i=1}^N e^{(i)} \\tag{2.1} \\end{equation}\\] \\[\\begin{equation} \\frac{\\partial RSS(\\beta)}{\\partial \\beta_j} = - \\sum_{i=1}^N x_j^{(i)}e^{(i)} \\tag{2.2} \\end{equation}\\] para \\(j=1,2,\\ldots, p\\). Nótese que cada punto de entrenamiento contribuye al cálculo del gradiente - la contribución es la dirección de descenso de error para ese punto particular de entrenamiento. Nos movemos entonces en una dirección promedio, para intentar hacer el error total lo más chico posible. Podemos implementar ahora estos cálculos. Aunque podríamos escribir ciclos para hacer estos cálculos, es mejor hacer los cálculos en forma matricial, de manera que aprovechamos rutinas de álgebra lineal eficiente. El cálculo del gradiente es como sigue: grad_calc &lt;- function(x_ent, y_ent){ salida_grad &lt;- function(beta){ f_beta &lt;- as.matrix(cbind(1, x_ent)) %*% beta e &lt;- y_ent - f_beta grad_out &lt;- -as.numeric(t(cbind(1,x_ent)) %*% e) names(grad_out) &lt;- c(&#39;Intercept&#39;, colnames(x_ent)) grad_out } salida_grad } grad &lt;- grad_calc(prostata_entrena[, 1, drop = FALSE], prostata_entrena$lpsa) grad(c(0,1)) ## Intercept lcavol ## -76.30319 -70.93938 grad(c(1,1)) ## Intercept lcavol ## -9.303187 17.064556 Podemos checar nuestro cálculo del gradiente: delta &lt;- 0.001 (rss(c(1 + delta,1)) - rss(c(1,1)))/delta ## [1] -9.269687 (rss(c(1,1+delta)) - rss(c(1,1)))/delta ## [1] 17.17331 Y ahora iteramos para obtener iteraciones &lt;- descenso(30, c(0,0), 0.005, grad) iteraciones ## [,1] [,2] ## [1,] 0.0000000 0.0000000 ## [2,] 0.8215356 1.4421892 ## [3,] 0.7332652 0.9545169 ## [4,] 0.8891507 1.0360252 ## [5,] 0.9569494 0.9603012 ## [6,] 1.0353555 0.9370937 ## [7,] 1.0977074 0.9046239 ## [8,] 1.1534587 0.8800287 ## [9,] 1.2013557 0.8576489 ## [10,] 1.2430547 0.8385314 ## [11,] 1.2791967 0.8218556 ## [12,] 1.3105688 0.8074114 ## [13,] 1.3377869 0.7948709 ## [14,] 1.3614051 0.7839915 ## [15,] 1.3818983 0.7745509 ## [16,] 1.3996803 0.7663595 ## [17,] 1.4151098 0.7592518 ## [18,] 1.4284979 0.7530844 ## [19,] 1.4401148 0.7477329 ## [20,] 1.4501947 0.7430895 ## [21,] 1.4589411 0.7390604 ## [22,] 1.4665303 0.7355643 ## [23,] 1.4731155 0.7325308 ## [24,] 1.4788295 0.7298986 ## [25,] 1.4837875 0.7276146 ## [26,] 1.4880895 0.7256328 ## [27,] 1.4918224 0.7239132 ## [28,] 1.4950614 0.7224211 ## [29,] 1.4978719 0.7211265 ## [30,] 1.5003106 0.7200031 Y checamos que efectivamente el error total de entrenamiento decrece apply(iteraciones, 1, rss) ## [1] 249.60960 51.70986 32.49921 28.96515 27.22475 25.99191 25.07023 ## [8] 24.37684 23.85483 23.46181 23.16591 22.94312 22.77538 22.64910 ## [15] 22.55401 22.48242 22.42852 22.38794 22.35739 22.33438 22.31706 ## [22] 22.30402 22.29421 22.28681 22.28125 22.27706 22.27390 22.27153 ## [29] 22.26974 22.26839 Notación y forma matricial Usando la notación de la clase anterior (agregando una columna de unos al principio): \\[\\underline{X} = \\left ( \\begin{array}{ccccc} 1 &amp; x_1^{(1)} &amp; x_2^{(1)} &amp; \\ldots &amp; x_p^{(1)} \\\\ 1 &amp; x_1^{(2)} &amp; x_2^{(2)} &amp; \\ldots &amp; x_p^{(2)}\\\\ 1&amp; \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 1 &amp; x_1^{(N)} &amp; x_2^{(N)} &amp; \\ldots &amp; x_p^{(N)} \\\\ \\end{array} \\right)\\] y \\[\\underline{y} =(y^{(1)},y^{(2)}, \\ldots, y^{(N)})^t.\\] Como \\[\\underline{e} = \\underline{y} - \\underline{X}\\beta\\] tenemos entonces (de las fórmulas (2.1) y (2.2)): \\[\\begin{equation} \\nabla RSS(\\beta) = \\underline{X}^t(\\underline{X}\\beta - \\underline{y}) = -\\underline{X}^t \\underline{e} \\tag{2.3} \\end{equation}\\] 2.5 Normalización de entradas La convergencia de descenso en gradiente (y también el desempeño numérico para otros algoritmos) puede dificultarse cuando las escalas tienen escalas muy diferentes. En este ejemplo simple, una variable tiene desviación estándar 10 y otra 1: x2 &lt;- rnorm(100, 0, 1) x1 &lt;- rnorm(100, 0, 10) + 5 * x2 y &lt;- 0.1 * x1 + x2 + rnorm(100, 0, 1) dat &lt;- data_frame(x1, x2, y) rss &lt;- function(beta) mean((as.matrix(dat[, 1:2]) %*% beta - y)^2) grid_beta &lt;- expand.grid(beta1 = seq(-10, 10, 0.5), beta2 = seq(-10, 10, 0.5)) rss_1 &lt;- apply(grid_beta, 1, rss) dat_x &lt;- data.frame(grid_beta, rss_1) ggplot(dat_x, aes(x = beta1, y = beta2, z = rss_1)) + geom_contour() En algunas direcciones el gradiente es muy grande, y en otras chico. Esto implica que la convergencia puede ser muy lenta en algunas direcciones, puede diverger en otras, y que hay que ajustar el paso \\(\\eta &gt; 0\\) con cuidado, dependiendo de dónde comiencen las iteraciones. Una normalización usual es con la media y desviación estándar, donde hacemos, para cada variable de entrada \\(j=1,2,\\ldots, p\\) \\[ x_j^{(i)} = \\frac{ x_j^{(i)} - \\bar{x}_j}{s_j}\\] donde \\[\\bar{x}_j = \\frac{1}{N} \\sum_{i=1}^N x_j^{(i)}\\] \\[s_j = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^N (x_j^{(i)}- \\bar{x}_j )^2}\\] es decir, centramos y normalizamos por columna. Otra opción común es restar el mínimo y dividir entre la diferencia del máximo y el mínimo, de modo que las variables resultantes toman valores en \\([0,1]\\). Entonces escalamos antes de ajustar: x1_s = (x1 - mean(x1))/sd(x1) x2_s = (x2 - mean(x2))/sd(x2) dat &lt;- data_frame(x1_s, x2_s, y) rss &lt;- function(beta) mean((as.matrix(dat[, 1:2]) %*% beta - y)^2) grid_beta &lt;- expand.grid(beta1 = seq(-10, 10, 0.5), beta2 = seq(-10, 10, 0.5)) rss_1 &lt;- apply(grid_beta, 1, rss) dat_x &lt;- data.frame(grid_beta, rss_1) ggplot(dat_x, aes(x = beta1, y = beta2, z = rss_1)) + geom_contour() Nótese que los coeficientes ajustados serán diferentes a los del caso no normalizado. Cuando normalizamos antes de ajustar el modelo, las predicciones deben hacerse con entradas normalizadas. La normalización se hace con los mismos valores que se usaron en el entrenamiento (y no recalculando medias y desviaciones estándar con el conjunto de prueba). En cuanto a la forma funcional del predictor f, el problema con entradas normalizadas es equivalente al de las entradas no normalizadas. Asegúrate de esto escribiendo cómo correponden los coeficientes de cada modelo normalizado con los coeficientes del modelo no normalizado. 2.6 Interpretación de modelos lineales Muchas veces se considera que la facilidad de interpretación es una fortaleza del modelo lineal. Esto es en parte cierto, pero hay algunas consideraciones importantes que debemos tomar en cuenta. La interpretación más sólida es la de las predicciones: podemos decir por qué una predicción es alta o baja. Consideremos el ejemplo de cáncer de prostata, por ejemplo: library(tidyr) prostate_completo &lt;- read_csv(file = &#39;datos/prostate.csv&#39;) pr_entrena &lt;- filter(prostate_completo, train) pr_entrena &lt;- pr_entrena %&gt;% mutate(id = 1:nrow(pr_entrena)) #normalizamos pr_entrena_s &lt;- pr_entrena %&gt;% select(id, lcavol, age, lpsa) %&gt;% gather(variable, valor, lcavol:age) %&gt;% group_by(variable) %&gt;% mutate(media = mean(valor), desv = sd(valor)) %&gt;% mutate(valor_s = (valor - media)/desv) pr_modelo &lt;- pr_entrena_s %&gt;% select(id, lpsa, variable, valor_s) %&gt;% spread(variable, valor_s) mod_pr &lt;- lm( lpsa ~ lcavol + age , data = pr_modelo ) round(coefficients(mod_pr), 2) ## (Intercept) lcavol age ## 2.45 0.88 0.02 y observamos el rango de \\(lpsa\\): round(summary(pr_modelo$lpsa), 2) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.43 1.67 2.57 2.45 3.37 5.48 Ahora podemos interpretar el predictor: Cuando las variables lcavol y age están en sus media, la predicción de lpsa es 2.5 Si lcavol sube 1 desviación estándar por encima de la media, el predictor de lpsa sube alrededor de 0.9 unidades (de un rango de alrededor de 6 unidades) Si age sube 1 desviación estándar por encima de su media, el predictor de lpsa sube 0.02, lo cual es un movimiento muy chico considerando la variación de lpsa. Así podemos explicar cada predicción - considerando qué variables aportan positiva y cuáles negativamente a la predicción. El camino más seguro es limitarse a hacer este tipo de análisis de las predicciones. Hablamos de entender la estructura predictiva del problema con los datos que tenemos - y no intentamos ir hacia la explicación del fenómeno. Cualquier otra interpretación requiere mucho más cuidados, y requiere una revisión de la especificación correcta del modelo. Parte de estos cuidados se estudian en un curso de regresión desde el punto de vista estadístico, por ejemplo: Variación muestral. Es necesario considerar la variación en nuestras estimaciones de los coeficientes para poder concluir acerca de su relación con el fenómeno (tratable desde punto de vista estadístico, pero hay que checar supuestos). Quizá el error de estimación del coeficiente de lcavol es 2 veces su magnitud - difícilmente podemos concluir algo acerca la relación de lcavol. Efectos no lineales: si la estructura del problema es altamente no lineal, los coeficientes de un modelo lineal no tienen una interpretación clara en relación al fenómeno. Esto también es parcialmente tratable con diagnósticos. set.seed(2112) x &lt;- rnorm(20) y &lt;- x^2 summary(lm(y ~x)) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.7462 -0.5022 -0.3313 0.3435 1.6273 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.85344 0.17570 4.857 0.000127 *** ## x 0.04117 0.18890 0.218 0.829929 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7484 on 18 degrees of freedom ## Multiple R-squared: 0.002632, Adjusted R-squared: -0.05278 ## F-statistic: 0.0475 on 1 and 18 DF, p-value: 0.8299 Otros cuidados adicionales se requieren si queremos hacer afirmaciones causales: Variables omitidas: si faltan algunas variables cruciales en el fenómeno que nos interesa, puede ser muy difícil interpretar el resto de los coeficientes en términos del fenómeno Ejemplo: Supongamos que queremos predecir cuánto van a gastar en televisiones samsung ciertas personas que llegan a Amazon. Una variable de entrada es el número de anuncios de televisiones Samsung que recibieron antes de llegar a Amazon. El coeficiente de esta variable es alto (significativo, etc.), así que concluimos que el anuncio causa compras de televisiones Samsung. ¿Qué está mal aquí? El modelo no está mal, sino la interpretación. Cuando las personas están investigando acerca de televisiones, recibe anuncios. La razón es que esta variable nos puede indicar más bien quién está en proceso de compra de una televisión samsung (reciben anuncios) y quién no (no hacen búsquedas relevantes, así que no reciben anuncios). El modelo está mal especificado porque no consideramos que hay otra variable importante, que es el interés de la persona en compra de TVs Samsung. En general, la recomendación es que las interpretaciones causales deben considerarse como preliminares (o sugerencias), y se requiere más análisis y consideraciones antes de poder tener interpretaciones causales sólidas. Ejercicio En el siguiente ejercicio intentamos predecir el porcentaje de grasa corporal (una medición relativamente cara) usando mediciones de varias partes del cuerpo, edad, peso y estatura. Ver script bodyfat_ejercicio.R library(tidyr) dat_grasa &lt;- read_csv(file = &#39;datos/bodyfat.csv&#39;) head(dat_grasa) ## # A tibble: 6 x 14 ## grasacorp edad peso estatura cuello pecho abdomen cadera muslo ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12.3 23 154.25 67.75 36.2 93.1 85.2 94.5 59.0 ## 2 6.1 22 173.25 72.25 38.5 93.6 83.0 98.7 58.7 ## 3 25.3 22 154.00 66.25 34.0 95.8 87.9 99.2 59.6 ## 4 10.4 26 184.75 72.25 37.4 101.8 86.4 101.2 60.1 ## 5 28.7 24 184.25 71.25 34.4 97.3 100.0 101.9 63.2 ## 6 20.9 24 210.25 74.75 39.0 104.5 94.4 107.8 66.0 ## # ... with 5 more variables: rodilla &lt;dbl&gt;, tobillo &lt;dbl&gt;, biceps &lt;dbl&gt;, ## # antebrazo &lt;dbl&gt;, muñeca &lt;dbl&gt; nrow(dat_grasa) ## [1] 252 2.7 Solución analítica El problema de mínimos cuadrados tiene una solución de forma cerrada. A partir del gradiente (2.3), podemos igual a cero y resolver (chécalo) para obtener: \\[\\begin{equation*} \\hat{\\beta} = \\left (\\underline{X}\\underline{X}^t \\right)^{-1} \\underline{X}^t\\underline{y} \\end{equation*}\\] Paquetes como lm de R usan como base esta expresión, pero los cálculos se hacen mediante descomposiciones matriciales para más estabilidad (productos de matrices e inversiones). Aunque es posible escalar y/o paralelizar estos cálculos matriciales para problemas grandes, los procedimientos son más delicados. Nuestro enfoque de descenso máximo tiene la ventaja de que es fácil de entender, usar, aplicar a otros problemas con éxito, y además puede escalarse trivialmente, como veremos más adelante (por ejemplo, descenso estocástico). ¡Aunque siempre que se pueda es buena idea usar lm! 2.8 ¿Por qué el modelo lineal funciona bien (muchas veces)? Regresión lineal es un método muy simple, y parecería que debería haber métodos más avanzados que lo superen fácilmente. Para empezar, es poco creíble que el modelo \\[f(X) = b_0 + b_1X_1 + \\cdots b_p X_p\\] se cumple exactamente para el fenómeno que estamos tratando. Pero regresión lineal muchas veces supera a métodos s que intentan construir predictores más complejos. Una de las primeras razones es que podemos ver la aproximación lineal como una aproximación de primer orden a la verdadera \\(f(X)\\), y muchas veces eso es suficiente para producir predicciones razonables. Adicionalmente, otras veces sólo tenemos suficientes datos para hacer una aproximación de primer orden, aún cuando la verdadera \\(f(X)\\) no sea lineal, y resulta que esta aproximación da buenos resultados. Esto es particularmente cierto en problemas de dimensión alta, como veremos a continuación. 2.8.1 k vecinos más cercanos Un método popular, con buen desempeño en varios ejemplos, es el de k-vecinos más cercanos, que consiste en hacer aproximaciones locales directas de \\(f(X)\\). Sea \\({\\mathcal L}\\) un conjunto de entrenamiento. Para \\(k\\) entera fija, y \\(x_0\\) una entrada donde queremos predecir, definimos a \\(N_k(x_0)\\) como el conjunto de los \\(k\\) elementos de \\({\\mathcal L}\\) que tienen \\(x^{(i)}\\) más cercana a \\(x_0\\). Hacemos la predicción \\[\\hat{f}(x_0) = \\frac{1}{k}\\sum_{x^{(i)} \\in N_k(x_0)} y^{(i)}\\] Es decir, promediamos las \\(k\\) \\(y\\)’s con \\(x\\)’s más cercanas a donde queremos predecir. Ejemplo library(ISLR) datos &lt;- Auto[, c(&#39;name&#39;, &#39;weight&#39;,&#39;year&#39;, &#39;mpg&#39;)] datos$peso_kg &lt;- datos$weight*0.45359237 datos$rendimiento_kpl &lt;- datos$mpg*(1.609344/3.78541178) nrow(datos) ## [1] 392 Vamos a separa en muestra de entrenamiento y de prueba estos datos. Podemos hacerlo como sigue (2/3 para entrenamiento aproximadamente en este caso, así obtenemos alrededor de 100 casos para prueba): set.seed(213) datos$muestra_unif &lt;- runif(nrow(datos), 0, 1) datos_entrena &lt;- filter(datos, muestra_unif &gt; 1/3) datos_prueba &lt;- filter(datos, muestra_unif &lt;= 1/3) nrow(datos_entrena) ## [1] 274 nrow(datos_prueba) ## [1] 118 ggplot(datos_entrena, aes(x = peso_kg, y = rendimiento_kpl)) + geom_point() Consideremos un modelo de \\(k=15\\) vecinos más cercanos. La función de predicción ajustada es entonces: library(kknn) # nótese que no normalizamos entradas - esto también es importante # hacer cuando hacemos vecinos más cercanos, pues en otro caso # las variables con escalas más grandes dominan el cálculo mod_15vmc &lt;- kknn(rendimiento_kpl ~ peso_kg, train = datos_entrena, test = data_frame(peso_kg=seq(700,2200, by = 10)), k=15) dat_graf &lt;- data_frame(peso_kg = seq(700,2200, by = 10), rendimiento_kpl = predict(mod_15vmc)) ggplot(datos_entrena, aes(x = peso_kg, y = rendimiento_kpl)) + geom_point(alpha=0.6) + geom_line(data=dat_graf, col=&#39;red&#39;, size = 1.2) Y para \\(k=5\\) vecinos más cercanos: mod_5vmc &lt;- kknn(rendimiento_kpl ~ peso_kg, train = datos_entrena, test = data_frame(peso_kg=seq(700,2200, by = 10)), k = 5) dat_graf &lt;- data_frame(peso_kg = seq(700,2200, by = 10), rendimiento_kpl = predict(mod_5vmc)) ggplot(datos_entrena, aes(x = peso_kg, y = rendimiento_kpl)) + geom_point(alpha=0.6) + geom_line(data=dat_graf, col=&#39;red&#39;, size = 1.2) En nuestro caso, los errores de prueba son mod_3vmc &lt;- kknn(rendimiento_kpl ~ peso_kg, train = datos_entrena, test = datos_prueba, k = 3) mod_15vmc &lt;- kknn(rendimiento_kpl ~ peso_kg, train = datos_entrena, test = datos_prueba, k = 15) (mean((datos_prueba$rendimiento_kpl-predict(mod_3vmc))^2)) ## [1] 3.346934 (mean((datos_prueba$rendimiento_kpl-predict(mod_15vmc))^2)) ## [1] 2.697658 Pregunta: ¿Cómo escogerías una \\(k\\) adecuada para este problema? Recuerda que adecuada significa que se reduzca a mínimo posible el error de predicción. Como ejercicio, compara los modelos con \\(k = 2, 25, 200\\) utilizando una muestra de prueba. ¿Cuál se desempeña mejor? Da las razones de el mejor o peor desempeño: recuerda que el desempeño en predicción puede sufrir porque la función estimada no es suficiente flexible para capturar patrones importantes, pero también porque parte del ruido se incorpora en la predicción. Por los ejemplos anteriores, vemos que k-vecinos más cercanos puede considerarse como un aproximador universal, que puede adaptarse a cualquier patrón importante que haya en los datos. Entonces, ¿cuál es la razón de utilizar otros métodos como regresión? ¿Por qué el desempeño de regresión sería superior? La maldición de la dimensionalidad El método de k-vecinos más cercanos funciona mejor cuando hay muchas \\(x\\) cercanas a \\(x0\\), de forma que el promedio sea estable (muchas \\(x\\)), y extrapolemos poco (\\(x\\) cercanas). Cuando \\(k\\) es muy chica, nuestras estimaciones son ruidosas, y cuando \\(k\\) es grande y los vecinos están lejos, entonces estamos sesgando la estimación local con datos lejanos a nuestra región de interés. El problema es que en dimensión alta, casi cualquier conjunto de entrenamiento (independientemente del tamaño) sufre fuertemente por uno o ambas dificultades del problema. Ejemplo Consideremos que la salida Y es determinística \\(Y = e^{-8\\sum_{j=1}^p x_j^2}\\). Vamos a usar 1-vecino más cercano para hacer predicciones, c on una muestra de entrenamiento de 1000 casos. Generamos $x^{i}‘s uniformes en \\([ 1,1]\\), para \\(p = 2\\), y calculamos la respuesta \\(Y\\) para cada caso: fun_exp &lt;- function(x) exp(-8*sum(x^2)) x_1 &lt;- runif(1000, -1, 1) x_2 &lt;- runif(1000, -1, 1) dat &lt;- data_frame(x_1 = x_1, x_2 = x_2) dat$y &lt;- apply(dat, 1, fun_exp) ggplot(dat, aes(x = x_1, y = x_2, colour = y)) + geom_point() La mejor predicción en \\(x_0 = (0,0)\\) es \\(f((0,0)) = 1\\). Eñ vecino más cercano al origen es dist_origen &lt;- apply(dat, 1, function(x) sqrt(sum(head(x, -1)^2))) mas_cercano_indice &lt;- which.min(dist_origen) mas_cercano &lt;- dat[mas_cercano_indice, ] mas_cercano ## # A tibble: 1 x 3 ## x_1 x_2 y ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.03268542 0.01006107 0.9906871 Nuestra predicción es entonces \\(\\hat{f}(0)=\\) 0.9906871, que es bastante cercano al valor verdadero (1). Ahora intentamos hacer lo mismo para dimensión \\(p=8\\). dat_lista &lt;- lapply(1:8, function(i) runif(1000, -1, 1)) dat &lt;- Reduce(cbind, dat_lista) %&gt;% data.frame dat$y &lt;- apply(dat, 1, fun_exp) dist_origen &lt;- apply(dat, 1, function(x) sqrt(sum(head(x, -1)^2))) mas_cercano_indice &lt;- which.min(dist_origen) mas_cercano &lt;- dat[mas_cercano_indice, ] mas_cercano ## init V2 V3 V4 V5 V6 ## 239 0.1612183 0.4117209 0.2546389 -0.226929 0.0774977 0.03897632 ## V7 V8 y ## 239 -0.4959736 0.0382697 0.01073141 Y el resultado es un desastre. Nuestra predicción es mas_cercano$y ## [1] 0.01073141 Necesitariamos una muestra de alrededor de un millón de casos para obtener resultados no tan malos (pruébalo). ¿Qué es lo que está pasando? La razón es que en dimensiones altas, los puntos de la muestra de entrenamiento están muy lejos unos de otros, y están cerca de la frontera, incluso para tamaños de muestra relativamente grandes como n = 1000. Cuando la dimensión crece, la situación empeora exponencialmente. En dimensiones altas, todos los conjuntos de entrenamiento factibles se distribuyen de manera rala en el espacio de entradas. Ahora intentamos algo similar con una función que es razonable aproximar con una función lineal: fun_cubica &lt;- function(x) 0.5 * (1 + x[1])^3 set.seed(821) sims_1 &lt;- lapply(1:40, function(i) runif(1000, -0.5, 0.5) ) dat &lt;- data.frame(Reduce(cbind, sims_1)) dat$y &lt;- apply(dat, 1, fun_cubica) dist_origen &lt;- apply(dat[, 1:40], 1, function(x) sqrt(sum(x^2))) mas_cercano_indice &lt;- which.min(dist_origen) dat$y[mas_cercano_indice] ## [1] 0.09842398 Este no es un resultado muy bueno. Sin embargo, mod_lineal &lt;- lm(y ~ ., data = dat) origen &lt;- data.frame(matrix(rep(0,40), 1, 40)) names(origen) &lt;- names(dat)[1:40] predict(mod_lineal, newdata = origen) ## 1 ## 0.6251876 Donde podemos ver que típicamente la predicción de regresión es mucho mejor que la de 1 vecino más cercano. Esto es porque el modelo explota la estructura aproximadamente lineal del problema. Nota: corre este ejemplo varias veces con semilla diferente. Lo que sucede más específicamente es que en regresión lineal utilizamos todos los datos para hacer nuestra estimación en cada predicción. Si la estructura del problema es aproximadamente lineal, entonces regresión lineal explota la estructura para hacer pooling de toda la infromación para construir predicción con sesgo y varianza bajas. Tarea Para este ejemplo usaremos los datos de https://archive.ics.uci.edu/ml/machine-learning-databases/housing/. El objetivo es predecir el valor mediano de las viviendas en áreas del censo de Estados Unidos, utilizando variables relacionadas con criminalidad, ambiente, tipo de viviendas, etc. Separa la muestra en dos partes: unos 400 para entrenamiento y el resto para prueba. Describe las variables en la muestra de prueba (rango, media, mediana, por ejemplo). Construye un modelo lineal para predecir MEDV en términos de las otras variables. Utiliza descenso en gradiente para estimar los coeficientes con los predictores estandarizados. Verifica tus resultados con la función lm. Evalúa el error de entrenamiento \\(\\overline{err}\\) de tu modelo, y evalúa después la estimación del error de predicción \\(\\hat{Err}\\) con la muestra de prueba. Utiliza la raíz del la media de los errores al cuadrado. (Adicional) Construye un modelo de 1,5,20 y 50 vecinos más cercanos, y evalúa su desempeño. ¿Cuál es la mejor \\(k\\) para reducir el error de prueba? "],
["logistica.html", "Clase 3 Regresión logística 3.1 El problema de clasificación 3.2 Estimación de probabilidades de clase 3.3 Error para modelos de clasificación 3.4 Regresión logística 3.5 Aprendizaje de coeficientes para regresión logística (binomial). 3.6 Observaciones adicionales 3.7 Ejercicio: datos de diabetes", " Clase 3 Regresión logística 3.1 El problema de clasificación Una variabla \\(G\\) categórica o cualitativa toma valores que no son numéricos. Por ejemplo, si \\(G\\) denota el estado del contrato de celular de un cliente dentro de un año, podríamos tener \\(G\\in \\{ activo, cancelado\\}\\). En un problema de clasificación buscamos predecir una variable respuesta categórica \\(G\\) en función de otras variables de entrada \\(X=(X_1,X_2,\\ldots, X_p)\\). Ejemplos Predecir si un cliente cae en impago de una tarjeta de crédito, de forma que podemos tener \\(G=corriente\\) o \\(G=impago\\). Variables de entrada podrían ser \\(X_1=\\) porcentaje de saldo usado, \\(X_2=\\) atrasos en los úlltimos 3 meses, \\(X_3=\\) edad, etc En nuestro ejemplo de reconocimiento de dígitos tenemos \\(G\\in\\{ 0,1,\\ldots, 9\\}\\). Nótese que los` dígitos no se pueden considerar como valores numéricos (son etiquetas). Tenemos que las entradas \\(X_j\\) para \\(j=1,2,\\ldots, 256\\) son valores de cada pixel (imágenes blanco y negro). En reconocimiento de imágenes quiza tenemos que \\(G\\) pertenece a un conjunto que típicamente contiene miles de valores (manzana, árbol, pluma, perro, coche, persona, cara, etc.). Las \\(X_j\\) son valores de pixeles de la imagen para tres canales (rojo, verde y azul). Si las imágenes son de 100x100, tendríamos 30,000 variables de entrada. ¿Qué estimar en problemas de clasificación? En problemas de regresión, consideramos modelos de la forma \\(Y= f(X) + \\epsilon\\), y vimos que podíamos plantear el problema de aprendizaje supervisado como uno donde el objetivo es estimar lo mejor que podamos la función \\(f\\) mediante un estimador \\(\\hat{f}\\). Usamos entonces \\(\\hat{f}\\) para hacer predicciónes. En el caso de regresión: \\(f(X)\\) es la relación sistemática de \\(Y\\) en función de \\(X\\) Dada \\(X\\), la variable observada \\(Y\\) es una variable aleatoria (\\(\\epsilon\\) depende de otras variables que no conocemos) No podemos usar un modelo así en clasificación pues \\(G\\) no es numérica. Sin embargo, podemos pensar que \\(X\\) nos da cierta información probabilística acerca de las clases que pueden ocurrir: \\(P(G|X)\\) es la probabilidad condicional de observar \\(G\\) si tenemos \\(X\\). Esto es la información sistemática de \\(G\\) en función de \\(X\\) Dada \\(X\\), la clase observada \\(G\\) es una variable aleatoria (depende de otras variables que no conocemos). En analogía con el problema de regresión, quisiéramos estimar las probabilidades condicionales \\(P(G|X)\\), que es la parte sistemática de la relación de \\(G\\) en función de \\(X\\). Normalmente codificamos las clases \\(g\\) con una etiqueta numérica, de modo que \\(G\\in\\{1,2,\\ldots, K\\}\\): Ejemplo (Impago de tarjetas de crédito) Supongamos que \\(X=\\) porcentaje del crédito máximo usado, y \\(G\\in\\{1, 2\\}\\), donde \\(1\\) corresponde al corriente y \\(2\\) representa impago. Podríamos tener, por ejemplo: \\[\\begin{align*} p_1(10\\%) &amp;= P(G=1|X=10\\%) = 0.95 \\\\ p_2(10\\%) &amp;= P(G=2|X=10\\%) = 0.05 \\end{align*}\\] y \\[\\begin{align*} p_1(95\\%) &amp;= P(G=1|X=95\\%) = 0.70 \\\\ p_2(95\\%) &amp;= P(G=2|X=95\\%) = 0.30 \\end{align*}\\] En resumen: En problemas de clasificación queremos estimar la parte sistemática de la relación de \\(G\\) en función \\(X\\), que en este caso quiere decir que buscamos estimar las probabilidades condicionales: \\[\\begin{align*} p_1(x) &amp;= P(G=1|X=x), \\\\ p_2(x) &amp;= P(G=2|X=x), \\\\ \\vdots &amp; \\\\ p_K(x) &amp;= P(G=K|X=x) \\end{align*}\\] para cada valor \\(x\\) de las entradas. A partir de estas probabilidades de clase podemos producir un clasificador de varias maneras (las discutiremos más adelante). La forma más simple es usando el clasificador de Bayes: Dadas las probabilidades condicionales \\(p_1(x),p_2(x),\\ldots, p_K(x)\\), el clasificador de Bayes asociado está dado por \\[G (x) = \\arg\\max_{g} p_g(x)\\] Es decir, clasificamos en la clase que tiene máxima probabilidad de ocurrir. Ejemplo (Impago de tarjetas de crédito) Supongamos que \\(X=\\) porcentaje del crédito máximo usado, y \\(G\\in\\{1, 2\\}\\), donde \\(1\\) corresponde al corriente y \\(2\\) representa impago. Las probabilidades condicionales de clase para la clase al corriente podrían ser, por ejemplo: \\(p_1(x) = P(G=1|X = x) =0.95\\) si \\(x &lt; 15\\%\\) \\(p_1(x) = P(G=1|X = x) = 0.95 - 0.007(x-15)\\) si \\(x&gt;=15\\%\\) Estas son probabilidades, pues hay otras variables que influyen en que un cliente permanezca al corriente o no en sus pagos más allá de información contenida en el porcentaje de crédito usado. Nótese que estas probabilidades son diferentes a las no condicionadas, por ejempo, podríamos tener que a total \\(P(G=1)=0.83\\). p_1 &lt;- function(x){ ifelse(x &lt; 15, 0.95, 0.95 - 0.007 * (x - 15)) } curve(p_1, 0,100, xlab = &#39;Porcentaje de crédito máximo&#39;, ylab = &#39;p_1(x)&#39;, ylim = c(0,1)) ¿Por qué en este ejemplo ya no mostramos la función \\(p_2(x)\\)? Si usamos el clasificador de Bayes, tendríamos por ejemplo que si \\(X=10\\%\\), como \\(p_1(10\\%) = 0.95\\) y \\(p_2(10\\%)=0.05\\), nuestra predicción de clase sería \\(G(10\\%) = 1\\) (al corriente), pero si \\(X=70\\%\\), \\(G(70\\%) = 1\\) (impago), pues \\(p_1(70\\%) = 0.57\\) y \\(p_2(70\\%) = 0.43\\). 3.2 Estimación de probabilidades de clase ¿Cómo estimamos ahora las probabilidades de clase a partir de una muestra de entrenamiento? Veremos por ahora dos métodos: k-vecinos más cercanos y regresión logística. Ejemplo Vamos a generar unos datos con el modelo simple del ejemplo anterior: library(dplyr) library(tidyr) library(kknn) set.seed(1933) x &lt;- pmin(rexp(500,1/30),100) probs &lt;- p_1(x) g &lt;- ifelse(rbinom(length(x), 1, probs)==1 ,1, 2) dat_ent &lt;- data_frame(x = x, p_1 = probs, g = factor(g)) dat_ent %&gt;% select(x, g) ## # A tibble: 500 x 2 ## x g ## &lt;dbl&gt; &lt;fctr&gt; ## 1 0.5320942 1 ## 2 25.3910853 1 ## 3 37.4805755 1 ## 4 20.8732917 1 ## 5 70.8899113 2 ## 6 14.8300636 1 ## 7 49.4363507 1 ## 8 20.9386771 1 ## 9 35.4585176 1 ## 10 9.8302441 1 ## # ... with 490 more rows Como este problema es de dos clases, podemos graficar como sigue: graf_1 &lt;- ggplot(dat_ent, aes(x = x)) + geom_jitter(aes(colour = g, y = as.numeric(g==&#39;1&#39;)), width=0, height=0.1) graf_1 3.2.1 k-vecinos más cercanos Podemos extender fácilmente k vecinos más cercanos para ver un ejemplo de cómo estimar las probabilidades de clase \\(p_g(x)\\). La idea general es igual que en regresión: Supongamos que tenemos un conjunto de entrenamiento \\[{\\mathcal L}=\\{ (x^{(1)},g^{(1)}),(x^{(2)},g^{(2)}), \\ldots, (x^{(N)}, g^{(N)}) \\}\\] La idea es que si queremos predecir en \\(x_0\\), busquemos varios \\(k\\) vecinos más cercanos a \\(x_0\\), y estimamos entonces \\(p_g(x)\\) como la proporción de casos tipo \\(g\\) que hay entre los \\(k\\) vecinos de \\(x_0\\). Vemos entonces que este método es un intento de hacer una aproximación directa de las probabilidades condicionales de clase. Podemos escribir esto como: k vecinos más cercanos para clasificación Estimamos contando los elementos de cada clase entre los \\(k\\) vecinos más cercanos: \\[\\hat{p}_g (x_0) = \\frac{1}{k}\\sum_{x^{(i)} \\in N_k(x_0)} I( g^{(i)} = g),\\] para \\(g=1,2,\\ldots, K\\), donde \\(N_k(x_0)\\) es el conjunto de \\(k\\) vecinos más cercanos en \\({\\mathcal L}\\) de \\(x_0\\), y \\(I(g^{(i)}=g)=1\\) cuando \\(g^{(i)}=g\\), y cero en otro caso (indicadora). Ejemplo Regresamos a nuestro problema de impago. Vamos a intentar estimar la probabilidad condicional de estar al corriente usando k vecinos más cercanos (curva roja): graf_data &lt;- data_frame(x = seq(0,100, 1)) vmc &lt;- kknn(g ~ x, train = dat_ent, k = 60, test = graf_data, kernel = &#39;rectangular&#39;) graf_data$p_1 &lt;- vmc$prob[ ,1] graf_verdadero &lt;- data_frame(x = 0:100, p_1 = p_1(x)) graf_1 + geom_line(data = graf_data, aes(y = p_1), colour = &#39;red&#39;, size=1.2) + geom_line(data = graf_verdadero, aes(y = p_1)) + ylab(&#39;Probabilidad al corriente&#39;) + xlab(&#39;% crédito usado&#39;) Igual que en el caso de regresión, ahora tenemos qué pensar cómo validar nuestra estimación, pues no vamos a tener la curva negra real para comparar. Ejemplo Consideremos datos de diabetes en mujeres Pima: A population of women who were at least 21 years old, of Pima Indian heritage and living near Phoenix, Arizona, was tested for diabetes according to World Health Organization criteria. The data were collected by the US National Institute of Diabetes and Digestive and Kidney Diseases. We used the 532 complete records after dropping the (mainly missing) data on serum insulin. npreg number of pregnancies. glu plasma glucose concentration in an oral glucose tolerance test. bp diastolic blood pressure (mm Hg). skin triceps skin fold thickness (mm). bmi body mass index (weight in kg/(height in m)^2). ped diabetes pedigree function. age age in years. type Yes or No, for diabetic according to WHO criteria. diabetes_ent &lt;- as_data_frame(MASS::Pima.tr) diabetes_pr &lt;- as_data_frame(MASS::Pima.te) diabetes_ent ## # A tibble: 200 x 8 ## npreg glu bp skin bmi ped age type ## * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fctr&gt; ## 1 5 86 68 28 30.2 0.364 24 No ## 2 7 195 70 33 25.1 0.163 55 Yes ## 3 5 77 82 41 35.8 0.156 35 No ## 4 0 165 76 43 47.9 0.259 26 No ## 5 0 107 60 25 26.4 0.133 23 No ## 6 5 97 76 27 35.6 0.378 52 Yes ## 7 3 83 58 31 34.3 0.336 25 No ## 8 1 193 50 16 25.9 0.655 24 No ## 9 3 142 80 15 32.4 0.200 63 No ## 10 2 128 78 37 43.3 1.224 31 Yes ## # ... with 190 more rows Intentaremos predecir diabetes dependiendo del BMI: library(ggplot2) ggplot(diabetes_ent, aes(x = bmi, y= as.numeric(type==&#39;Yes&#39;), colour = type)) + geom_point() Usamos \\(20\\) vecinos más cercanos para estimar \\(p_g(x)\\): graf_data &lt;- data_frame(bmi = seq(20,45, 1)) vmc_5 &lt;- kknn(type ~ bmi, train = diabetes_ent, k = 20, test = graf_data, kernel = &#39;rectangular&#39;) graf_data$Yes &lt;- vmc_5$prob[ ,&quot;Yes&quot;] graf_data$No &lt;- vmc_5$prob[ ,&quot;No&quot;] graf_data &lt;- graf_data %&gt;% gather(type, prob, Yes:No) ggplot(diabetes_ent, aes(x = bmi, y= as.numeric(type==&#39;Yes&#39;), colour = type)) + geom_point() + geom_line(data = filter(graf_data, type ==&#39;Yes&#39;) , aes(x=bmi, y = prob, colour=type, group = type)) + ylab(&#39;Probabilidad diabetes&#39;) 3.3 Error para modelos de clasificación En regresión, vimos que la pérdida cuadrática era una buena opción para ajustar modelos (descenso en gradiente, por ejemplo), y también para evaluar su desempeño. Ahora necesitamos una pérdida apropiada para trabajar con modelos de clasificación. Consideremos entonces que tenemos una estimación \\(\\hat{p}_g(x)\\) de las probabilidad de clase \\(P(G=g|X=x)\\). Supongamos que observamos ahora \\((x, g)\\). Si \\(\\hat{p}_{g}(x)\\) es muy cercana a uno, deberíamos penalizar poco, pues dimos probabilidad alta a \\(G=g\\). Si \\(\\hat{p}_{g}(x)\\) es chica, deberíamos penalizar más, pues dimos probabilidad baja a \\(G=g\\). Si \\(\\hat{p}_{g}(x)\\) es muy cercana a cero, y observamos \\(G=g\\), deberíamos hacer una penalización muy alta (convergiendo a \\(\\infty\\), pues no es aceptable que sucedan eventos con probabilidad estimada extremadamente baja). Quisiéramos encontrar una función \\(h\\) apropiada, de forma que la pérdida al observar \\((x, g)\\) sea \\[s(\\hat{p}_{g}(x)),\\] y que cumpla con los puntos arriba señalados. Entonces tenemos que \\(s\\) debe ser una función continua y decreciente en \\([0,1]\\) Podemos poner \\(s(1)=0\\) (no hay pérdida si ocurre algo con probabilidad 1) \\(s(p)\\) debe ser muy grande is \\(p\\) es muy chica. Una opción analíticamente conveniente es \\[s(z) = - 2log(z)\\] s &lt;- function(z){ -2*log(z)} curve(s, 0, 1) Y entonces la pérdida (que llamamos devianza) que construimos está dada, para \\((x,g)\\) observado y probabilidades estimadas \\(\\hat{p}_g(x)\\) por \\[ - 2\\log(\\hat{p}_g(x)) \\] Su valor esperado (según el proceso que genera los datos) es nuestra medición del desempeño del modelo \\(\\hat{p}_g (x)\\): \\[-2E\\left [ \\log(\\hat{p}_G(X)) \\right ]\\] Observaciones: Ojo: el nombre de devianza se utiliza de manera diferente en distintos lugares (pero para cosas similares). Usamos el factor 2 por razones históricas (la medida de devianza definida en estadística tiene un 2, para usar más fácilmente en pruebas de hipótesis relacionadas con comparaciones de modelos). Para nuestros propósitos, podemos usar o no el 2. No es fácil interpretar la devianza, pero es útil para comparar modelos. Veremos otras medidas más fáciles de intrepretar más adelante. Compara la siguiente definición con la que vimos para modelos de regresión: Sea \\[{\\mathcal L}=\\{ (x^{(1)},g^{(1)}),(x^{(2)},g^{(2)}), \\ldots, (x^{(N)}, g^{(N)}) \\}\\] una muestra de entrenamiento, a partir de las cuales construimos mediante un algoritmo funciones estimadas \\(\\hat{p}_{g} (x)\\) para \\(g=1,2,\\ldots, K\\). La devianza promedio de entrenamiento está dada por \\[\\begin{equation} \\overline{err} = - \\frac{2}{N}\\sum_{i=1}^N log(\\hat{p}_{g^{(i)}} (x^{(i)})) \\tag{3.1} \\end {equation}\\] Sea \\[{\\mathcal T}=\\{ (x_0^{(1)},g_0^{(1)}),(x_0^{(2)},g_0^{(2)}), \\ldots, (x_0^{(m)}, g_0^{(m)}) \\}\\] una muestra de prueba. La devianza promedio de prueba es \\[\\begin{equation} \\hat{Err} = - \\frac{2}{m}\\sum_{i=1}^m log(\\hat{p}_{g_0^{(i)}} (x_0^{(i)})) \\end {equation}\\] que es una estimación de la devianza de predicción \\[-2E\\left [ \\log(\\hat{p}_G(X)) \\right ]\\] Ejemplo Regresamos a nuestros ejemplo de impago de tarjetas de crédito. Primero calculamos la devianza de entrenamiento s &lt;- function(x) -2*log(x) vmc &lt;- kknn(g ~ x, train = dat_ent, k = 60, test = dat_ent, kernel = &#39;rectangular&#39;) dat_dev &lt;- dat_ent %&gt;% select(x,g) dat_dev$hat_p_1 &lt;- predict(vmc, type =&#39;prob&#39;)[,1] dat_dev$hat_p_2 &lt;- predict(vmc, type =&#39;prob&#39;)[,2] dat_dev &lt;- dat_dev %&gt;% mutate(hat_p_g = ifelse(g==1, hat_p_1, hat_p_2)) Nótese que dependiendo de qué clase observamos (columna \\(g\\)), extraemos la probabilidad correspondiente a la columna hat_p_g: head(dat_dev, 50) ## # A tibble: 50 x 5 ## x g hat_p_1 hat_p_2 hat_p_g ## &lt;dbl&gt; &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.5320942 1 0.9666667 0.03333333 0.9666667 ## 2 25.3910853 1 0.8833333 0.11666667 0.8833333 ## 3 37.4805755 1 0.8500000 0.15000000 0.8500000 ## 4 20.8732917 1 0.9000000 0.10000000 0.9000000 ## 5 70.8899113 2 0.6000000 0.40000000 0.4000000 ## 6 14.8300636 1 0.9333333 0.06666667 0.9333333 ## 7 49.4363507 1 0.8000000 0.20000000 0.8000000 ## 8 20.9386771 1 0.9000000 0.10000000 0.9000000 ## 9 35.4585176 1 0.7500000 0.25000000 0.7500000 ## 10 9.8302441 1 0.9333333 0.06666667 0.9333333 ## # ... with 40 more rows Ahora aplicamos la función \\(s\\) que describimos arriba, y promediamos sobre el conjunto de entrenamiento: dat_dev &lt;- dat_dev %&gt;% mutate(dev = s(hat_p_g)) dat_dev %&gt;% ungroup %&gt;% summarise(dev_entrena = mean(dev)) ## # A tibble: 1 x 1 ## dev_entrena ## &lt;dbl&gt; ## 1 0.6997961 Recordemos que la devianza de entrenamiento no es la cantidad que evalúa el desempeño del modelo. Hagamos el cálculo entonces para una muestra de prueba: set.seed(1213) x &lt;- pmin(rexp(1000,1/30),100) probs &lt;- p_1(x) g &lt;- ifelse(rbinom(length(x), 1, probs)==1 ,1, 2) dat_prueba &lt;- data_frame(x = x, g = factor(g)) vmc &lt;- kknn(g ~ x, train = dat_ent, k = 60, test = dat_prueba, kernel = &#39;rectangular&#39;) dat_dev &lt;- dat_prueba %&gt;% select(x,g) dat_dev$hat_p_1 &lt;- predict(vmc, type =&#39;prob&#39;)[,1] dat_dev$hat_p_2 &lt;- predict(vmc, type =&#39;prob&#39;)[,2] dat_dev &lt;- dat_dev %&gt;% mutate(hat_p_g = ifelse(g==1, hat_p_1, hat_p_2)) dat_dev &lt;- dat_dev %&gt;% mutate(dev = s(hat_p_g)) dat_dev %&gt;% ungroup %&gt;% summarise(dev_prueba = mean(dev)) ## # A tibble: 1 x 1 ## dev_prueba ## &lt;dbl&gt; ## 1 0.7113815 3.3.1 Ejercicio Utiliza 5, 20, 60, 200 y 400 vecinos más cercanos para nuestro ejemplo de tarjetas de crédito. ¿Cuál tiene menor devianza de prueba? ¿Cuál tiene menor devianza de entrenamiento? Grafica el mejor que obtengas y otros dos modelos malos. ¿Por qué crees que la devianza es muy grande para los modelos malos? Nota: ten cuidado con probabilidades iguales a 0 o 1, pues en en estos casos la devianza puede dar \\(\\infty\\). Puedes por ejemplo hacer que las probabilidades siempre estén en \\([\\epsilon, 1-\\epsilon]\\) para \\(\\epsilon&gt;0\\) chica. Empieza con el código en clase_3_ejercicio.R. 3.3.2 Error de clasificación y función de pérdida 0-1 Otra medida común para medir el error de un clasificador es el error de clasificación, que también llamamos probabilidad de clasificación incorrecta, o error bajo pérdida 0-1. Si \\(\\hat{G}\\) es un clasificador (que puede ser construido a partir de probabilidades de clase), decimos que su error de clasificación es \\[P(\\hat{G}\\neq G)\\] Aunque esta definición aplica para cualquier clasificador, podemos usarlo para clasificadores construidos con probabilidades de clase de la siguiente forma: Sean \\(\\hat{p}_g(x)\\) probabilidades de clase estimadas. El clasificador asociado está dado por \\[\\hat{G} (x) = \\arg\\max_g \\hat{p}_g(x)\\] Podemos estimar su error de clasificación \\(P(\\hat{G} \\neq G)\\) con una muestra de prueba \\[{\\mathcal T}=\\{ (x_0^{(1)},g_0^{(1)}),(x_0^{(2)},g_0^{(2)}), \\ldots, (x_0^{(m)}, g_0^{(m)})\\] mediante \\[\\hat{Err} = \\frac{1}{m} \\sum_{j=i}^m I(\\hat{G}(x_0^{(i)}) \\neq g_0^{(i)}),\\] es decir, la proporción de casos de prueba que son clasificados incorrectamente. Ejemplo Veamos cómo se comporta en términos de error de clasificación nuestro último modelo: dat_dev$hat_G &lt;- predict(vmc) dat_dev %&gt;% mutate(correcto = hat_G == g) %&gt;% ungroup %&gt;% summarise(p_correctos = mean(correcto)) %&gt;% mutate(error_clasif = 1 - p_correctos) ## # A tibble: 1 x 2 ## p_correctos error_clasif ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.851 0.149 vmc_2 &lt;- kknn(g ~ x, train = dat_ent, k = 3, test = dat_prueba, kernel = &#39;rectangular&#39;) dat_dev$hat_G &lt;- predict(vmc_2) dat_dev %&gt;% mutate(correcto = hat_G == g) %&gt;% ungroup %&gt;% summarise(p_correctos = mean(correcto)) %&gt;% mutate(error_clasif = 1 - p_correctos) ## # A tibble: 1 x 2 ## p_correctos error_clasif ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.82 0.18 3.3.3 Discusión: relación entre devianza y error de clasificación Cuando utilizamos devianza, el mejor desempeño se alcanza cuando las probabilidades \\(\\hat{p}_g (x)\\) están bien calibradas, es decir, están cercanas a las probabilidades verdaderas \\(p_g (x)\\). Esto se puede ver demostrando que las probabilidades \\(\\hat{p}_g (x)\\) que minimizan la devianza \\[-2E(\\log (\\hat{p}_G (X))) = -2E_X \\left[ \\sum_{k=1}^K p_g(X)\\log\\hat{p}_g(X) \\right]\\] son precisamente \\(\\hat{p}_g (x)=p_g (x)\\). Por otro lado, si consideramos el error de clasificación \\(P(\\hat{G}\\neq G)\\), es posible demostrar que se minimiza cuando \\(\\hat{G} = G_{bayes}\\), donde \\[{G}_{bayes} (x) = \\arg\\max_g {p}_g(x).\\] En consecuencia, cuando las \\(\\hat{p}_g(x)\\) estimadas están cercanas a las verdaderas \\(p_g (x)\\) (que es lo que intentamos hacer cuando usamos devianza), el clasificador \\(\\hat{G}(x)\\) producido a partir de las \\(\\hat{p}_g(x)\\) deberá estar cercano a \\(G_{bayes}(x)\\), que es el clasificador que minimiza el error de clasificación. Este argumento explica que buscar modelos con devianza baja no está alineado con buscar modelos con error de clasificación bajo. Cuando sea posible, es mejor trabajar con probabilidades de clase y devianza que solamente con clasificadores y error de clasificación. Hay varias razones para esto: Tenemos una medida de qué tan seguros estamos en la clasificación (por ejemplo, \\(p_1 = 0.55\\) en vez de \\(p_1 = 0.995\\)). La salida de probabilides es un insumo más útil para tareas posteriores (por ejemplo, si quisiéramos ofrecer las 3 clases más probables en clasificación de imágenes). Permite hacer selección de modelos de manera más atinada: por ejemplo, dada una misma tasa de correctos, preferimos aquellos modelos que lo hacen con probabilidades que discriminan más (más altas cuando está en lo correcto y más bajas cuando se equivoca). 3.4 Regresión logística En \\(k\\) vecinos más cercanos, intentamos estimar directamente con promedios las probabilidades de clase. Regresión logística (y otros métodos, como redes neuronales), son ajustados intentando minimizar la devianza de entrenamiento. Esto es necesario si queremos aprovechar la estructura adicional que estos modelos aportan (recordemos el caso de regresión lineal: intentamos minimizar el error de entrenamiento para estimar nuestro predictor, y así podíamos explotar apropiadamente la estructura lineal del problema). Regresión logística es un método lineal de clasificación, en el sentido de que produce fronteras lineales de decisión para el clasificador asociado. Ejemplo Mostramos aquí una frontera de decisión de regresión logística y una de k vecinos más cercanos: knitr::include_graphics(path = c(&quot;imagenes/clas_lineal.png&quot;, &quot;imagenes/clas_nolineal.png&quot;)) 3.4.1 Regresión logística simple Vamos a construir el modelo de regresión logística (binaria) para una sola entrada. Suponemos que tenemos una sola entrada \\(X_1\\), y que \\(G\\in\\{1,2\\}\\). Nos convendrá crear una nueva variable \\(Y\\) dada por \\(Y=1\\) si \\(G=2\\), \\(Y=0\\) si \\(G=1\\). Nótese que intentar estimar las probabilidades de clase \\(p_1(x)\\) de forma lineal con \\[p_1(x)=\\beta_0+\\beta_1 x_1\\] tiene el defecto de que el lado derecho puede producir valores fuera de \\([0,1]\\). La idea es entonces aplicar una función \\(h\\) simple que transforme la recta real al intervalo \\([0,1]:\\) \\[p_1(x) = h(\\beta_0+\\beta_1 x_1),\\] donde \\(h\\) es una función que toma valores en \\([0,1]\\). ¿Cúal es la función más simple que hace esto? 3.4.2 Función logística Comenzamos con el caso más simple, poniendo \\(\\beta_0=0\\) y \\(\\beta_1=1\\), de modo que \\[p_1(x)=h(x).\\] ¿Cómo debe ser \\(h\\) para garantizar que \\(h(x)\\) está entre 0 y 1 para toda \\(x\\)? No van a funcionar polinomios, por ejemplo, porque para un polinomio cuando \\(x\\) tiende a infinito, el polinomio tiende a \\(\\infty\\) o a \\(-\\infty\\). Hay varias posibilidades, pero una de las más simples es tomar (ver gráfica al margen): La función logística está dada por \\[h(x)=\\frac{e^x}{1+e^x}\\] h &lt;- function(x){exp(x)/(1+exp(x)) } curve(h, from=-6, to =6) ``` Esta función comprime adecuadamente (para nuestros propósitos) el rango de todos los reales dentro del intervalo \\([0,1]\\). El modelo de regresión logística simple está dado por \\[p_1(x)=p_1(x;\\beta)= h(\\beta_0+\\beta_1x_1)= \\frac{e^{\\beta_0+\\beta_1x_1}}{1+ e^{\\beta_0+\\beta_1x_1}},\\] y \\[p_0(x)=p_0(x;\\beta)=1-p_1(x;\\beta),\\] donde \\(\\beta=(\\beta_0,\\beta_1)\\). Este es un modelo paramétrico con 2 parámetros. Ejercicio Demostrar que, si \\(p_1(x)\\) está dado como en la ecuación anterior, entonces también podemos escribir: \\[p_o(x)=\\frac{1}{1+e^{\\beta_0+\\beta_1x_1}}.\\] Graficar las funciones \\(p_1(x;\\beta)\\) para distintos valores de \\(\\beta_0\\) y \\(\\beta_1\\). 3.4.2.1 Ejemplo En nuestro ejemplo: graf_data &lt;- data_frame(x = seq(0,100, 1)) vmc_graf &lt;- kknn(g ~ x, train = dat_ent, k = 60, test = graf_data, kernel = &#39;rectangular&#39;) graf_data$p_1 &lt;- vmc_graf$prob[ ,1] graf_verdadero &lt;- data_frame(x = 0:100, p_1 = p_1(x)) graf_1 + geom_line(data = graf_data, aes(y = p_1), colour = &#39;red&#39;, size=1.2) + geom_line(data = graf_verdadero, aes(y = p_1)) + ylab(&#39;Probabilidad al corriente&#39;) + xlab(&#39;% crédito usado&#39;) Ahora intentaremos ajustar a mano (intenta cambiar las betas para p_mod_1 y p_mod_2 en el ejemplo de abajo) algunos modelos logísticos para las probabilidades de clase: h &lt;- function(z) exp(z)/(1+exp(z)) p_logistico &lt;- function(beta_0, beta_1){ p &lt;- function(x){ z &lt;- beta_0 + beta_1*x h(z) } } p_mod_1 &lt;- p_logistico(-20, 1) p_mod_2 &lt;- p_logistico(3, -0.04) graf_data &lt;- graf_data %&gt;% mutate(p_mod_1 = p_mod_1(x), p_mod_2 = p_mod_2(x)) graf_1 + geom_line(data = graf_data, aes(y = p_mod_2), colour = &#39;red&#39;, size=1.2) + geom_line(data = graf_data, aes(y = p_mod_1), colour = &#39;orange&#39;, size=1.2) + geom_line(data = graf_verdadero, aes(y = p_1)) + ylab(&#39;Probabilidad al corriente&#39;) + xlab(&#39;% crédito usado&#39;) Podemos usar también la función glm de R para ajustar los coeficientes: mod_1 &lt;- glm(g==1 ~ x, data = dat_ent, family = &#39;binomial&#39;) coef(mod_1) ## (Intercept) x ## 3.24467326 -0.04353428 p_mod_final &lt;- p_logistico(coef(mod_1)[1], coef(mod_1)[2]) graf_data &lt;- graf_data %&gt;% mutate(p_mod_f = p_mod_final(x)) graf_1 + geom_line(data = graf_data, aes(y = p_mod_f), colour = &#39;red&#39;, size=1.2) + geom_line(data = graf_data, aes(y = p_mod_1), colour = &#39;orange&#39;, size=1.2) + geom_line(data = graf_verdadero, aes(y = p_1)) + ylab(&#39;Probabilidad al corriente&#39;) + xlab(&#39;% crédito usado&#39;) 3.4.3 Regresión logística Ahora escribimos el modelo cuando tenemos más de una entrada. La idea es la misma: primero combinamos las variables linealmente usando pesos \\(\\beta\\), y despúes comprimimos a \\([0,1]\\) usando la función logística: El modelo de regresión logística está dado por \\[p_1(x)=p_1(x;\\beta)= h(\\beta_0+\\beta_1x_1 + \\beta_2x_2 +\\cdots + \\beta_p x_p),\\] y \\[p_0(x)=p_0(x;\\beta)=1-p_1(x;\\beta),\\] donde \\(\\beta=(\\beta_0,\\beta_1, \\ldots, \\beta_p)\\). 3.5 Aprendizaje de coeficientes para regresión logística (binomial). Ahora veremos cómo aprender los coeficientes con una muestra de entrenamiento. La idea general es : Usamos la devianza de entrenamiento como medida de ajuste Usamos descenso en gradiente para minimizar esta devianza y aprender los coeficientes. Sea entonces \\({\\mathcal L}\\) una muestra de entrenamiento: \\[{\\mathcal L}=\\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \\ldots, (x^{(N)}, y^{(N)}) \\}\\] Donde \\(y=1\\) o \\(y=0\\) son las dos clases. Escribimos también \\[p_1(x)=p_1(x;\\beta)= h(\\beta_0+\\beta_1x_1 + \\beta_2x_2 +\\cdots + \\beta_p x_p),\\] y definimos la devianza sobre el conjunto de entrenamiento \\[D(\\beta) = -2\\sum_{i=1}^N \\log(p_{y^{(i)}} (x^{(i)})).\\] Los coeficientes estimados por regresión logística están dados por \\[\\hat{\\beta} = \\arg\\min_\\beta D(\\beta)\\] Para minimizar utilizaremos descenso en gradiente (aunque hay más opciones). La última expresión para \\(D(\\beta)\\) puede ser difícil de operar, pero podemos reescribir como: \\[D(\\beta) = -2\\sum_{i=1}^N y^{(i)} \\log(p_{1} (x^{(i)})) + (1-y^{(i)}) \\log(p_{0} (x^{(i)})).\\] Para hacer descenso en gradiente, necesitamos encontrar \\(\\frac{\\partial D}{\\beta_j}\\) para \\(j=1,2,\\ldots,p\\). Igual que en regresión lineal, comenzamos por calcular la derivada de un término: \\[D^{(i)} (\\beta) = y^{(i)} \\log(p_{1} (x^{(i)})) + (1-y^{(i)}) \\log(1-p_{1} (x^{(i)}))\\] Calculamos primero las derivadas de \\(p_1 (x^{(i)};\\beta)\\) (demostrar la siguiente ecuación): \\[\\frac{\\partial p_1}{\\partial \\beta_0} = {p_1(x^{(i)})(1-p_1(x^{(i)}))},\\] y \\[\\frac{\\partial p_1}{\\partial \\beta_j} = p_1(x^{(i)})(1-p_1(x^{(i)}))x_j^{(i)},\\] Así que \\[\\begin{align*} \\frac{\\partial D^{(i)}}{\\partial \\beta_j} &amp;= \\frac{y^{(i)}}{(p_1(x^{(i)}))}\\frac{\\partial p_1}{\\partial \\beta_j} - \\frac{1- y^{(i)}}{(1-p_1(x^{(i)}))}\\frac{\\partial p_1}{\\partial \\beta_j} \\\\ &amp;= \\left( \\frac{y^{(i)} - p_1(x^{(i)})}{(p_1(x^{(i)}))(1-p_1(x^{(i)}))} \\right )\\frac{\\partial p_1}{\\partial \\beta_j} \\\\ &amp; = \\left ( y^{(i)} - p_1(x^{(i)}) \\right ) x_j^{(i)} \\\\ \\end{align*}\\] para \\(j=0,1,\\ldots,p\\), usando la convención de \\(x_0^{(i)}=1\\). Podemos sumar ahora sobre la muestra de entrenamiento para obtener \\[ \\frac{\\partial D}{\\partial\\beta_j} = - 2\\sum_{i=1}^N (y^{(i)}-p(x^{(i)}))x_j^{(i)}\\] De modo que, Para un paso \\(\\eta&gt;0\\) fijo, la iteración de descenso para regresión logística para el coeficiente \\(\\beta_j\\) es: \\[\\beta_{j}^{(k+1)} = \\beta_j^{(k)} + {2\\eta} \\sum_{i=1}^N (y^{(i)}-p(x^{(i)}))x_j^{(i)}\\] para \\(j=0,1,\\ldots, p\\), donde fijamos \\(x_0^{(i)}=1\\). Podríamos usar las siguientes implementaciones, que representan cambios menores de lo que hicimos en regresión lineal: devianza_calc &lt;- function(x, y){ dev_fun &lt;- function(beta){ p_beta &lt;- h(as.matrix(cbind(1, x)) %*% beta) -2*sum(y*log(p_beta) + (1-y)*log(1-p_beta)) } dev_fun } grad_calc &lt;- function(x_ent, y_ent){ salida_grad &lt;- function(beta){ p_beta &lt;- h(as.matrix(cbind(1, x_ent)) %*% beta) e &lt;- y_ent - p_beta grad_out &lt;- -2*as.numeric(t(cbind(1,x_ent)) %*% e) names(grad_out) &lt;- c(&#39;Intercept&#39;, colnames(x_ent)) grad_out } salida_grad } descenso &lt;- function(n, z_0, eta, h_deriv){ z &lt;- matrix(0,n, length(z_0)) z[1, ] &lt;- z_0 for(i in 1:(n-1)){ z[i+1, ] &lt;- z[i, ] - eta * h_deriv(z[i, ]) } z } Ejemplo Probemos nuestros cálculos con el ejemplo de 1 entrada de tarjetas de crédito. dat_ent$y &lt;- as.numeric(dat_ent$g==1) dat_ent &lt;- dat_ent %&gt;% ungroup %&gt;% mutate(x_s = (x - mean(x))/sd(x)) devianza &lt;- devianza_calc(dat_ent[, &#39;x_s&#39;, drop = FALSE], dat_ent$y) grad &lt;- grad_calc(dat_ent[, &#39;x_s&#39;, drop = FALSE], dat_ent$y) grad(c(0,1)) ## Intercept x_s ## -354.2728 363.2408 grad(c(0.5,-0.1)) ## Intercept x_s ## -217.8069 140.9315 Verificamos cálculo de gradiente: (devianza(c(0.5+0.0001,-0.1)) - devianza(c(0.5,-0.1)))/0.0001 ## [1] -217.7951 (devianza(c(0.5,-0.1+0.0001)) - devianza(c(0.5,-0.1)))/0.0001 ## [1] 140.9435 Y hacemos descenso: iteraciones &lt;- descenso(200, z_0=c(0,0), eta = 0.001, h_deriv = grad) tail(iteraciones, 20) ## [,1] [,2] ## [181,] 2.017177 -1.085143 ## [182,] 2.017177 -1.085143 ## [183,] 2.017178 -1.085144 ## [184,] 2.017178 -1.085144 ## [185,] 2.017178 -1.085144 ## [186,] 2.017178 -1.085144 ## [187,] 2.017178 -1.085144 ## [188,] 2.017179 -1.085144 ## [189,] 2.017179 -1.085144 ## [190,] 2.017179 -1.085144 ## [191,] 2.017179 -1.085144 ## [192,] 2.017179 -1.085144 ## [193,] 2.017179 -1.085145 ## [194,] 2.017179 -1.085145 ## [195,] 2.017179 -1.085145 ## [196,] 2.017180 -1.085145 ## [197,] 2.017180 -1.085145 ## [198,] 2.017180 -1.085145 ## [199,] 2.017180 -1.085145 ## [200,] 2.017180 -1.085145 plot(apply(iteraciones, 1, devianza)) matplot(iteraciones) Comparamos con glm: mod_1 &lt;- glm(y~x_s, data=dat_ent, family = &#39;binomial&#39;) coef(mod_1) ## (Intercept) x_s ## 2.017181 -1.085146 mod_1$deviance ## [1] 351.676 devianza(iteraciones[200,]) ## [1] 351.676 Nótese que esta devianza está calculada sin dividir intre entre el número de casos. Podemos calcular la devianza promedio de entrenamiento haciendo: devianza(iteraciones[200,])/nrow(dat_ent) ## [1] 0.703352 3.6 Observaciones adicionales Máxima verosimilitud Es fácil ver que este método de estimación de los coeficientes (minimizando la devianza de entrenamiento) es el método de máxima verosimilitud. La verosimilitud de la muestra de entrenamiento está dada por: \\[L(\\beta) =\\prod_{i=1}^N p_{y^{(i)}} (x^{(i)})\\] Y la log verosimilitud es \\[l(\\beta) =\\sum_{i=1}^N \\log(p_{y^{(i)}} (x^{(i)})).\\] Así que ajustar el modelo minimizando la expresión (3.1) es los mismo que hacer máxima verosimilitud (condicional a los valores de \\(x\\)). Normalización Igual que en regresión lineal, en regresión logística conviene normalizar las entradas antes de ajustar el modelo Desempeño de regresión logística como método de aprendizaje Igual que en regresión lineal, regresión logística supera a métodos más sofisticados o nuevos en numerosos ejemplos. Las razones son similares: la rigidez de regresión logística es una fortaleza cuando la estructura lineal es una buena aproximación. 3.6.0.1 Solución analítica El problema de regresión logística no tiene solución analítica. Paquetes como glm utilizan métodos numéricos (Newton-Raphson para regresión logística, por ejemplo). 3.6.0.2 Interpretación de modelos logísticos Todas las precauciones que mencionamos en modelos lineales aplican para los modelos logísticos (aspectos estadísticos del ajuste, relación con fenómeno de interés, argumentos de causalidad). Igual que en regresión lineal, podemos explicar el comportamiento de las probabilidades de clase ajustadas, pero es un poco más difícil por la no linealidad introducida por la función logística. Ejemplo Consideremos el modelo ajustado: head(dat_ent) ## # A tibble: 6 x 5 ## x p_1 g y x_s ## &lt;dbl&gt; &lt;dbl&gt; &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.5320942 0.9500000 1 1 -1.1098309 ## 2 25.3910853 0.8772624 1 1 -0.1125285 ## 3 37.4805755 0.7926360 1 1 0.3724823 ## 4 20.8732917 0.9088870 1 1 -0.2937750 ## 5 70.8899113 0.5587706 2 0 1.7128107 ## 6 14.8300636 0.9500000 1 1 -0.5362196 coeficientes &lt;- iteraciones[200,] coeficientes ## [1] 2.017180 -1.085145 Como centramos todas las entradas, la ordenada al origen se interpreta como la probabilidad de clase cuando todas las variables están en su media: h(coeficientes[1]) ## [1] 0.8825891 Esto quiere decir que la probabilidad de estar al corriente ds de 87% cuando la variable \\(x\\) está en su media. Si \\(x\\) se incrementa en una desviación estándar, la cantidad \\[z = \\beta_0 + \\beta_1x\\] baja por la cantidad coeficientes[2] ## [1] -1.085145 Y la probabilidad de estar al corriente cambia a 70%: h(coeficientes[1]+ coeficientes[2]) ## [1] 0.7174879 Nótese que una desviación estándar de \\(x\\) equivale a sd(dat_ent$x) ## [1] 24.92623 Ojo: En regresión lineal, las variables contribuyen independientemente de otras al predictor. Eso no pasa en regresión logística debido a la no linealidad introducida por la función logística \\(h\\). Por ejemplo, imaginemos el modelo: \\[p(z) = h(0.5 + 0.2 x_1 -0.5 x_2),\\] y suponemos las entradas normalizadas. Si todas las variables están en su media, la probabilidad de clase 1 es h(0.5) ## [1] 0.6224593 Si todas las variables están en su media, y cambiamos en 1 desviación estándar la variable \\(x_1\\), la probabilidad de clase 1 es: h(0.5+0.2) ## [1] 0.6681878 Y el cambio en puntos de probabilidad es: h(0.5+0.2) - h(0.5) ## [1] 0.04572844 Pero si la variable \\(x_2 = -1\\), por ejemplo, el cambio en probabilidad es de h(0.5+ 0.2 + 0.5*1) - h(0.5 + 0.5*1) ## [1] 0.0374662 3.7 Ejercicio: datos de diabetes Ya están divididos los datos en entrenamiento y prueba diabetes_ent &lt;- as_data_frame(MASS::Pima.tr) diabetes_pr &lt;- as_data_frame(MASS::Pima.te) diabetes_ent ## # A tibble: 200 x 8 ## npreg glu bp skin bmi ped age type ## * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fctr&gt; ## 1 5 86 68 28 30.2 0.364 24 No ## 2 7 195 70 33 25.1 0.163 55 Yes ## 3 5 77 82 41 35.8 0.156 35 No ## 4 0 165 76 43 47.9 0.259 26 No ## 5 0 107 60 25 26.4 0.133 23 No ## 6 5 97 76 27 35.6 0.378 52 Yes ## 7 3 83 58 31 34.3 0.336 25 No ## 8 1 193 50 16 25.9 0.655 24 No ## 9 3 142 80 15 32.4 0.200 63 No ## 10 2 128 78 37 43.3 1.224 31 Yes ## # ... with 190 more rows diabetes_ent$id &lt;- 1:nrow(diabetes_ent) diabetes_pr$id &lt;- 1:nrow(diabetes_pr) Normalizamos library(dplyr) library(tidyr) datos_norm &lt;- diabetes_ent %&gt;% gather(variable, valor, npreg:age) %&gt;% group_by(variable) %&gt;% summarise(media = mean(valor), de = sd(valor)) normalizar &lt;- function(datos, datos_norm){ datos %&gt;% gather(variable, valor, npreg:age) %&gt;% left_join(datos_norm) %&gt;% mutate(valor_s = (valor - media)/de) %&gt;% select(id, type, variable, valor_s) %&gt;% spread(variable, valor_s) } diabetes_ent_s &lt;- normalizar(diabetes_ent, datos_norm) diabetes_pr_s &lt;- normalizar(diabetes_pr, datos_norm) x_ent &lt;- diabetes_ent_s %&gt;% select(age:skin) %&gt;% as.matrix p &lt;- ncol(x_ent) y_ent &lt;- diabetes_ent_s$type == &#39;Yes&#39; grad &lt;- grad_calc(x_ent, y_ent) iteraciones &lt;- descenso(1000, rep(0,p+1), 0.001, h_deriv = grad) matplot(iteraciones) diabetes_coef &lt;- data_frame(variable = c(&#39;Intercept&#39;,colnames(x_ent)), coef = iteraciones[1000,]) diabetes_coef ## # A tibble: 8 x 2 ## variable coef ## &lt;chr&gt; &lt;dbl&gt; ## 1 Intercept -0.95583051 ## 2 age 0.45200719 ## 3 bmi 0.51263229 ## 4 bp -0.05472949 ## 5 glu 1.01705067 ## 6 npreg 0.34734305 ## 7 ped 0.55927529 ## 8 skin -0.02247172 Ahora calculamos devianza de prueba y error de clasificación: x_prueba &lt;- diabetes_pr_s %&gt;% select(age:skin) %&gt;% as.matrix y_prueba &lt;- diabetes_pr_s$type == &#39;Yes&#39; dev_prueba &lt;- devianza_calc(x_prueba, y_prueba) dev_prueba(iteraciones[1000,])/nrow(x_prueba) ## [1] 0.8813972 Y para el error clasificación de prueba, necesitamos las probabilidades de clase ajustadas: beta &lt;- iteraciones[1000, ] p_beta &lt;- h(as.matrix(cbind(1, x_prueba)) %*% beta) y_pred &lt;- as.numeric(p_beta &gt; 0.5) mean(y_prueba != y_pred) ## [1] 0.1987952 Tarea La tarea está en el documento scripts/tarea_3.Rmd del repositorio. "],
["mas-sobre-problemas-de-clasificacion.html", "Clase 4 Más sobre problemas de clasificación 4.1 Análisis de error para clasificadores binarios 4.2 Perfil de un clasificador binario y curvas ROC 4.3 Regresión logística para problemas de más de 2 clases 4.4 Descenso en gradiente para regresión multinomial logística", " Clase 4 Más sobre problemas de clasificación En esta parte presentamos técnicas adicionales para evaluar el desempeño de un modelo. En la parte anterior vimos que La devianza es una buena medida para ajustar y evaluar el desempeño de un modelo y comparar modelos, y utiliza las probabilidades de clase. Sin embargo, es una medida de dificil de interpretar en cuanto a los errores que podemos esperar del modelo. Por otro lado, la tasa de clasificación incorrecta puede usarse para evaluar el desempeño de un clasificador (incluyendo uno derivado de probabilidades de clase), puede interpretarse con facilidad, pero se queda corta en muchas aplicaciones. Una deficiencia grande de esta medida es que, contrario al problema de regresión, hay errores de clasificación que son cualitativamente diferentes. Ejemplo Por ejemplo, diagnosticar a alguien con una enfermedad cuando no la tiene tiene consecuencias distintas a diagnosticar como libre de enfermedad a alguien que la tiene. Estas consecuencias dependen de cómo son son los tratamientos consecuentes, de y qué tan peligrosa es la enfermedad. Cuando usamos un buscador como Google, es cualitativamente diferente que el buscador omita resultados relevantes a que nos presente resultados irrelevantes. ¿Otros ejemplos? En general, los costos de los distintos errores son distintos, y en muchos problemas quiséramos entenderlos y controlarlos individualmente. Aunque en teoría podríamos asignar costos a los errores y definir una función de pérdida apropiada, en la práctica esto muchas veces no es tan fácil o deseable. Podemos, sin embargo, reportar el tipo de errores que ocurren Matriz de confusión. Sea \\(\\hat{G}\\) un clasificador binario. La matriz de confusión \\(C\\) de \\(\\hat{G}\\) está dada por $C_{i,j} = \\(\\text{Número de casos de la clase verdadera j que son clasificados como clase i por el clasificador}\\) Ejemplo En un ejemplo de tres clases, podríamos obtener la matriz de confusión: A B C A.pred 50 2 0 B.pred 20 105 10 C.pred 20 10 30 Esto quiere decir que de 90 casos de clase \\(A\\), sólo clasificamos a 50 en la clase correcta, de 117 casos de clase \\(B\\), acertamos en 105, etcétera. Podemos ver esta tabla de distintas formas, por ejemplo, usando porcentajes por columna, nos dice cómo se distribuyen los casos de cada clase: knitr::kable(round(prop.table(tabla_1, 2),2)) A B C A.pred 0.56 0.02 0.00 B.pred 0.22 0.90 0.25 C.pred 0.22 0.09 0.75 Mientras que una tabla de porcentajes por renglón nos muestra qué pasa cada vez que hacemos una predicción dada: knitr::kable(round(prop.table(tabla_1, 1),2)) A B C A.pred 0.96 0.04 0.00 B.pred 0.15 0.78 0.07 C.pred 0.33 0.17 0.50 Ahora pensemos cómo podría sernos de utilidad esta tabla. Discute El clasificador fuera uno de severidad de emergencias en un hospital, donde A=requiere atención inmediata B=urgente C=puede posponerse. El clasificador fuera de tipos de cliente de un negocio. Por ejemplo, A = cliente de gasto potencial alto, B=cliente medio, C=abandonador. Imagínate que tiene un costo intentar conservar a un abandonador, y hay una inversión alta para tratar a los clientes A. La tasa de incorrectas es la misma en los dos ejemplos, pero la adecuación del clasificador es muy diferente. 4.1 Análisis de error para clasificadores binarios Cuando la variable a predecir es binaria (dos clases), podemos etiquetar una clase como positivo y otra como negativo. En el fondo no importa cómo catalogemos cada clase, pero para problemas particulares una asignación puede ser más natural. Por ejemplo, en diagnóstico de enfermedades, positivo=tiene la enfermedad, en análisis de crédito, positivo=cae en impago, en sistemas de recomendacion, positivo = le gusta el producto X, en recuperación de textos, positivo=el documento es relevante a la búsqueda, etc. Hay dos tipos de errores en un clasificador binario (positivo - negativo): Falsos positivos (fp): clasificar como positivo a un caso negativo. Falsos negativos (fn): clasificar como negativo a un caso positivo. A los casos clasificados correctamente les llamamos positivos verdaderos (pv) y negativos verdaderos (nv). La matriz de confusion es entonces library(dplyr) tabla &lt;- data_frame(&#39;-&#39; = c(&#39;positivo.pred&#39;,&#39;negativo.pred&#39;,&#39;total&#39;), &#39;positivo&#39;=c(&#39;pv&#39;,&#39;fn&#39;,&#39;pos&#39;), &#39;negativo&#39;=c(&#39;fp&#39;,&#39;nv&#39;,&#39;neg&#39;), &#39;total&#39; = c(&#39;pred.pos&#39;,&#39;pred.neg&#39;,&#39;&#39;)) knitr::kable(tabla) posi tivo nega tivo tota l positivo.pred pv fp pred.pos negativo.pred fn nv pred.neg total pos neg Nótese que un clasificador bueno, en general, es uno que tiene la mayor parte de los casos en la diagonal de la matriz de confusión. Podemos estudiar a nuestro clasificador en términos de las proporciones de casos que caen en cada celda, que dependen del desempeño del clasificador en cuanto a casos positivos y negativos. La nomenclatura es confusa, pues en distintas áreas se usan distintos nombres para estas proporciones: Tasa de falsos positivos \\[\\frac{fp}{fp+nv}=\\frac{fp}{neg}\\] Tasa de falsos negativos \\[\\frac{fn}{pv+fn}=\\frac{fn}{pos}\\] Especificidad \\[\\frac{nv}{fp+nv}=\\frac{nv}{neg}\\] Sensibilidad o Recall \\[\\frac{pv}{pv+fn}=\\frac{pv}{pos}\\] Y también otras que tienen como base las predicciones: Valor predictivo positivo o Precisión \\[\\frac{vp}{vp+fp}=\\frac{vp}{pred.pos}\\] Valor predictivo negativo \\[\\frac{vn}{fn+vn}=\\frac{vn}{pred.neg}\\] Y hay varias medidas resumen que ponderan de distinta forma Tasa de clasificación incorrecta \\[\\frac{fn+fv}{neg+pos}\\] Medida F (media armónica de precisión y recall) \\[2\\frac{precision \\cdot recall}{precision + recall}\\] AUC (area bajo la curva ROC) ver más adelante Kappa \\[\\kappa = \\frac{p_o - p_e}{1-p_e},\\] donde \\(p_o =\\) tasa de correctos, y \\(p_e\\) es la probabilidad de clasificar correctamente al azar, dado por \\[p_e = \\frac{pos}{total}\\frac{pred.pos}{total} + \\frac{neg}{total}\\frac{pred.neg}{total}\\] Dependiendo de el tema y el objetivo hay medidas más naturales que otras: En pruebas clínicas, se usa típicamente sensibilidad y especificidad (proporción de positivos que detectamos y proporción de negativos que descartamos). En búsqueda y recuperación de documentos (positivo=el documento es relevante, negativo=el documento no es relevante), se usa precisión y recall (precisión=de los documentos que entregamos (predicción positiva), cuáles son realmente positivos/relevantes, y recall=de todos los documentos relevantes, cuáles devolvemos). Aquí la tasa de falsos positivos (de todos los negativos, cuáles se predicen positivos), por ejemplo, no es de ayuda pues generalmente son bajas y no discriminan el desempeño de los clasificadores. La razón es que típicamente hay una gran cantidad de negativos, y se devuelven relativamente pocos documentos, de forma que la tasa de falsos positivos generalmente es muy pequeña. \\(\\kappa\\) señala un problema importante cuando interpretamos tasas de correctos. Por ejemplo, supongamos que hay un 85% de positivos y un 15% de negativos. Si nuestro clasificador clasifica todo a positivo, nuestra tasa de correctos sería 85% - pero nuestro clasificador no está aprovechando los datos. En este caso, \\[p_e = 0.85(1) + 0.15(0)= 0.85\\], y tenemos que \\(\\kappa = 0\\) (similar al azar). Supongamos por otra parte que escogemos 50% del tiempo positivo al azar. Esto quiere decir que tendríamos \\(p_o=0.5\\). Pero \\[p_e = 0.85(0.50) + 0.15(0.50) = 0.50,\\] de modo que otra vez \\(\\kappa = 0\\). \\(\\kappa\\) es un valor entre 0 y 1 que mide qué tan superior es nuestro clasificador a uno dado al azar (uno que la predicción no tiene qué ver con la clase verdadera). Ejercicio ¿Qué relaciones hay entre las cantidades mostradas arriba? Por ejemplo: Escribe la tasa de clasificación incorrecta en términos de especificidad y sensibilidad. También intenta escribir valor predictivo positivo y valor predictivo negativo en términos de sensibilidad y especificidad. Cada clasificador tiene un balance distinto especificidad-sensibliidad. Muchas veces no escogemos clasificadores por la tasa de incorrectos solamente, sino que intentamos buscar un balance adecuado entre el comportamiento de clasificación para positivos y para negativos. Ejercicio Calcular la matriz de confusión (sobre la muestra de prueba) para el clasificador logístico de diabetes en términos de glucosa. Calcula adicionalmente con la muestra de prueba sus valores de especificidad y sensibilidad, y precisión y recall. library(dplyr) library(tidyr) library(ggplot2) diabetes_ent &lt;- as_data_frame(MASS::Pima.tr) diabetes_pr &lt;- as_data_frame(MASS::Pima.te) mod_1 &lt;- glm(type ~ glu, data = diabetes_ent, family = &#39;binomial&#39;) preds_prueba &lt;- predict(mod_1, newdata = diabetes_pr, type =&#39;response&#39;) 4.1.1 Punto de corte para un clasificador binario ¿Qué sucede cuando el perfil de sensibilidad y especificidad de un clasificador binario no es apropiado para nuestros fines? Recordemos que una vez que hemos estimado con \\(\\hat{p}_1(x)\\), nuestra regla de clasificación es: Predecir positivo si \\(\\hat{p}_1(x) &gt; 0.5\\), Predecir negativo si \\(\\hat{p}_1(x) \\leq 0.5.\\) Esto sugiere una regla alternativa: Para \\(0 &lt; d &lt; 1\\), podemos utilizar nuestras estimaciones \\(\\hat{p}_1(x)\\) para construir un clasificador alternativo poniendo: Predecir positivo si \\(\\hat{p}_1(x) &gt; d\\), Predecir negativo si \\(\\hat{p}_1(x) \\leq d\\). Distintos valores de \\(d\\) dan distintos perfiles de sensibilidad-especificidad para una misma estimación de las probabilidades condicionales de clase: Para minimizar la tasa de incorrectos conviene poner \\(d = 0.5\\). Sin embargo, es común que este no es el único fin de un clasificador bueno (pensar en ejemplo de fraude). Cuando incrementamos d, quiere decir que exigimos estar más seguros de que un caso es positivo para clasificarlo como positivo. Eso quiere decir que la especifidad va a ser más grande (entre los negativos verdaderos va a haber menos falsos positivos). Sin embargo, la sensibilidad va a ser más chica pues captamos menos de los verdaderos positivos. Ejemplo Por ejemplo, si en el caso de diabetes incrementamos el punto de corte a 0.7: table(preds_prueba &gt; 0.7, diabetes_pr$type) ## ## No Yes ## FALSE 220 77 ## TRUE 3 32 tab &lt;- prop.table(table(preds_prueba &gt; 0.7, diabetes_pr$type),2) tab ## ## No Yes ## FALSE 0.98654709 0.70642202 ## TRUE 0.01345291 0.29357798 La especificidad ahora 0.99 , muy alta (descartamos muy bien casos negativos), pero la sensibilidad se deteriora a 0.29 Cuando hacemos más chico d, entonces exigimos estar más seguros de que un caso es negativo para clasificarlo como negativo. Esto aumenta la sensibilidad, pero la especificidad baja. Por ejemplo, si en el caso de diabetes ponemos el punto de corte en 0.3: table(preds_prueba &gt; 0.3, diabetes_pr$type) ## ## No Yes ## FALSE 170 37 ## TRUE 53 72 tab &lt;- prop.table(table(preds_prueba &gt; 0.3, diabetes_pr$type),2) tab ## ## No Yes ## FALSE 0.7623318 0.3394495 ## TRUE 0.2376682 0.6605505 4.1.2 Espacio ROC de clasificadores Podemos visualizar el desempeño de cada uno de estos clasificadores mapeándolos a las coordenadas de tasa de falsos positivos (1-especificidad) y sensibilidad: clasif_1 &lt;- data.frame( corte = c(&#39;0.3&#39;,&#39;0.5&#39;,&#39;0.7&#39;,&#39;perfecto&#39;,&#39;azar&#39;), tasa_falsos_pos=c(0.24,0.08,0.02,0,0.7), sensibilidad =c(0.66, 0.46,0.19,1,0.7)) ggplot(clasif_1, aes(x=tasa_falsos_pos, y=sensibilidad, label=corte)) + geom_point() + geom_abline(intercept=0, slope=1) + xlim(c(0,1)) +ylim(c(0,1)) + geom_text(hjust=-0.3, col=&#39;red&#39;)+ xlab(&#39;1-especificidad (tasa falsos pos)&#39;) Nótese que agregamos otros dos clasificadores, uno perfecto, que tiene tasa de falsos positivos igual a 0 y sensibilidad igual a 1. En esta gráfica, un clasificador \\(G_2\\) que está arriba a la izquierda de \\(G_1\\) domina a \\(G_1\\), pues tiene mejor especificidad y mejor sensibilidad. Entre los clasificadores 0.3, 0.5 y 0.7 de la gráfica, no hay ninguno que domine a otro. Todos los clasificadores en la diagonal son equivalentes a un clasificador al azar. ¿Por qué? La razón es que si cada vez que vemos un nuevo caso lo clasificamos como positivo con probabilidad \\(p\\) fija y arbitraria. Esto implica que cuando veamos un caso positivo, la probabilidad de ’atinarle’ es de p (sensibilidad), y cuando vemos un negativo, la probabilidad de equivocarnos también es de p (tasa de falsos positivos). De modo que este clasificador al azar está en la diagonal. ¿Qué podemos decir acerca de clasificadores que caen por debajo de la diagonal? Estos son clasificadores particularmente malos, pues existen clasificadores con mejor especificidad y/o sensibilidad que son clasificadores al azar! Sin embargo, se puede construir un mejor clasificador volteando las predicciones, lo que cambia sensibilidad por tasa de falsos positivos. ¿Cuál de los tres clasificadores es el mejor? En términos de la tasa de incorrectos, el de corte 0.5. Sin embargo, para otros propósitos puede ser razonable escoger alguno de los otros. 4.2 Perfil de un clasificador binario y curvas ROC En lugar de examinar cada punto de corte por separado, podemos hacer el análisis de todos los posibles puntos de corte mediante la curva ROC (receiver operating characteristic, de ingeniería). Para un problema de clasificación binaria, dadas estimaciones \\(\\hat{p}(x)\\), la curva ROC grafica todos los pares de (1-especificidad, sensibilidad) para cada posible punto de corte \\(\\hat{p}(x) &gt; d\\). Ejemplo library(tabplot) mod_1 &lt;- glm(type ~ glu, diabetes_ent, family = &#39;binomial&#39;) diabetes_pr$probs_prueba_1 &lt;- predict(mod_1, newdata = diabetes_pr, type = &quot;response&quot;) head(arrange(diabetes_pr, desc(probs_prueba_1))) ## # A tibble: 6 x 9 ## npreg glu bp skin bmi ped age type probs_prueba_1 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fctr&gt; &lt;dbl&gt; ## 1 2 197 70 45 30.5 0.158 53 Yes 0.8743254 ## 2 4 197 70 39 36.7 2.329 31 No 0.8743254 ## 3 8 196 76 29 37.5 0.605 57 Yes 0.8701147 ## 4 1 196 76 36 36.5 0.875 29 Yes 0.8701147 ## 5 3 193 70 31 34.9 0.241 25 Yes 0.8567582 ## 6 5 189 64 33 31.2 0.583 29 Yes 0.8371927 tableplot(diabetes_pr, sortCol = probs_prueba_1) La columna de probabilidad de la derecha nos dice en qué valores podemos cortar para obtener distintos clasificadores. Nótese que si cortamos más arriba, se nos escapan más positivos verdaderos que clasificamos como negativos, pero clasificamos a más negativos verdaderos como negativos. Lo opuesto ocurre cuando cortamos más abajo. Vamos a graficar todos los pares (1-especificidad, sensibilidad) para cada punto de corte \\(d\\) de estas probabilidades. library(ROCR) pred_rocr &lt;- prediction(diabetes_pr$probs_prueba_1, diabetes_pr$type) perf &lt;- performance(pred_rocr, measure = &quot;sens&quot;, x.measure = &quot;fpr&quot;) graf_roc_1 &lt;- data_frame(tfp = perf@x.values[[1]], sens = perf@y.values[[1]], d = perf@alpha.values[[1]]) ggplot(graf_roc_1, aes(x = tfp, y = sens, colour=d)) + geom_point() + xlab(&#39;1-especificidad&#39;) + ylab(&#39;Sensibilidad&#39;) En esta gráfica podemos ver todos los clasificadores posibles basados en las probabilidades de clase. Podemos usar estas curvas como evaluación de nuestros clasificadores, dejando para más tarde la selección del punto de corte, si esto es necesario (por ejemplo, dependiendo de los costos de cada tipo de error). También podemos definir una medida resumen del desempeño de un clasificador según esta curva: La medida AUC (area under the curve) para un clasificador es el área bajo la curva generada por los pares sensibilidad-especificidad de la curva ROC. auc_1 &lt;- performance(pred_rocr, measure = &#39;auc&#39;)@y.values auc_1 ## [[1]] ## [1] 0.7970543 También es útil para comparar modelos. Consideremos el modelo de los datos de diabetes que incluyen todas las variables: mod_2 &lt;- glm(type ~ ., diabetes_ent, family = &#39;binomial&#39;) diabetes_pr$probs_prueba_2 &lt;- predict(mod_2, newdata = diabetes_pr, type = &quot;response&quot;) head(arrange(diabetes_pr, desc(probs_prueba_2))) ## # A tibble: 6 x 10 ## npreg glu bp skin bmi ped age type probs_prueba_1 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fctr&gt; &lt;dbl&gt; ## 1 0 180 78 63 59.4 2.420 25 Yes 0.7854027 ## 2 4 197 70 39 36.7 2.329 31 No 0.8743254 ## 3 5 187 76 27 43.6 1.034 53 Yes 0.8266286 ## 4 3 173 82 48 38.4 2.137 25 Yes 0.7374869 ## 5 0 173 78 32 46.5 1.159 58 No 0.7374869 ## 6 17 163 72 41 40.9 0.817 47 Yes 0.6581611 ## # ... with 1 more variables: probs_prueba_2 &lt;dbl&gt; tableplot(diabetes_pr, sortCol = probs_prueba_2) Y graficamos juntas: library(ROCR) pred_rocr &lt;- prediction(diabetes_pr$probs_prueba_2, diabetes_pr$type) perf &lt;- performance(pred_rocr, measure = &quot;sens&quot;, x.measure = &quot;fpr&quot;) auc_2 &lt;- performance(pred_rocr, measure = &quot;auc&quot;)@y.values graf_roc_2 &lt;- data_frame(tfp = perf@x.values[[1]], sens = perf@y.values[[1]], d = perf@alpha.values[[1]]) graf_roc_2$modelo &lt;- &#39;Todas las variables&#39; graf_roc_1$modelo &lt;- &#39;Solo glucosa&#39; graf_roc &lt;- bind_rows(graf_roc_1, graf_roc_2) ggplot(graf_roc, aes(x = tfp, y = sens, colour = modelo)) + geom_point() + xlab(&#39;1-especificidad&#39;) + ylab(&#39;Sensibilidad&#39;) Comparación auc: auc_1 ## [[1]] ## [1] 0.7970543 auc_2 ## [[1]] ## [1] 0.8658823 En este ejemplo, vemos que casi no importa que perfil de especificidad y sensibilidad busquemos: el clasificador que usa todas las variables domina casi siempre al clasificador que sólo utiliza las variables de glucosa. La razón es que para cualquier punto de corte (con sensibilidad menor a 0.4) en el clasificador de una variable, existe otro clasificador en la curva roja (todas las variable), que domina al primero. La excepción es para clasificadores de valores de sensibilidad baja, con tasas de falsos positivos muy chicas: en este caso, el modelo de una variable puede ser ligeramente superior. 4.3 Regresión logística para problemas de más de 2 clases Consideramos ahora un problema con más de dos clases, de manera que \\(G ∈ {1,2,...,K}\\) (\\(K\\) clases), y tenemos \\(X = (X1 ...,Xp)\\) entradas. ¿Cómo generalizar el modelo de regresión logística a este problema? Una estrategia es la de uno contra todos: En clasificación uno contra todos, hacemos Para cada clase \\(g\\in\\{1,\\ldots,K\\}\\) entrenamos un modelo de regresión logística (binaria) \\(\\hat{p}^{(g)}(x)\\), tomando como positivos a los casos de 1 clase \\(g\\), y como negativos a todo el resto. Esto lo hacemos como en las secciones anteriores, y de manera independiente para cada clase. Para clasificar un nuevo caso \\(x\\), calculamos \\[\\hat{p}^{(1)}, \\hat{p}^{(2)},\\ldots, \\hat{p}^{(K)}\\] y clasificamos a la clase de máxima probabilidad \\[\\hat{G}(x) = \\arg\\max_g \\hat{p}^{(g)}(x)\\] Nótese que no hay ninguna garantía de que las probabilidades de clase sumen 1, pues se trata de estimaciones independientes de cada clase. En este sentido, produce estimaciones que en realidad no satisfacen las propiedades del modelo de probabilidad establecido. Sin embargo, esta estrategia es simple y en muchos casos funciona bien. 4.3.1 Regresión logística multinomial Si queremos obtener estimaciones de las probabilidades de clase que sumen uno, entonces tenemos que contruir las estimaciones de cada clase de clase de manera conjunta. Como vimos antes, tenemos que estimar, para cada \\(x\\) y \\(g\\in\\{1,\\ldots, K\\}\\), las probabilidades condicionales de clase: \\[p_g(x) = P(G = g|X = x).\\] Consideremos primero cómo funciona el modelo de regresión logística (2 clases) Tenemos que \\[p_1(x) = h(\\beta_0 + \\beta_1x_1 + \\ldots + \\beta_p x_p) = \\exp(\\beta_0 + \\beta_1x_1 + \\ldots + \\beta_p x_p)/Z \\] y \\[p_2 (x) = 1/Z\\] donde \\(Z = 1 + \\exp(\\beta_0 + \\beta_1x_1 + \\ldots + \\beta_p x_p)\\). Podemos generalizar para más de 2 clases usando una idea similar: \\[p_1(x) = \\exp(\\beta_{0,1} + \\beta_{1,1}x_1 + \\ldots + \\beta_{p,1} x_p)/Z\\] \\[p_2(x) = \\exp(\\beta_{0,2} + \\beta_{1,2}x_2 + \\ldots + \\beta_{p.2} x_p)/Z\\] hasta \\[p_{K-1}(x) = \\exp(\\beta_{0,{K-1}} + \\beta_{1,{K-1}}x_2 + \\ldots + \\beta_{p,{K-1}} x_p)/Z\\] y \\[p_K(x) = 1/Z\\] En este caso, para que las probabilidades sumen 1, necesitamos que \\[Z = 1 + \\sum_{j=1}^{K-1}\\exp(\\beta_0^j + \\beta_1^jx_2 + \\ldots + \\beta_p^j x_p)\\] Para ajustar coeficientes, usamos el mismo criterio de devianza de entrenamiento. Buscamos minimizar: \\[D(\\beta)=−2 \\sum_{i=1}^N p_{g^{(i)}}(x^{(i)}),\\] Donde \\(\\beta\\) contiene todos los coeficientes organizados en un vector de tamaño \\((p+1)(K+1)\\): \\[\\beta = ( \\beta_0^1, \\beta_1^1, \\ldots , \\beta_p^1, \\beta_0^2, \\beta_1^2, \\ldots , \\beta_p^2, \\ldots \\beta_0^{K-1}, \\beta_1^{K-1}, \\ldots , \\beta_p^{K-1} )\\] Y ahora podemos usar algún método númerico para minimizar la devianza (por ejemplo, descenso en gradiente). Cuando es muy importante tener probabilidades bien calibradas, el enfoque multinomial es más apropiado, pero muchas veces, especialmente si sólo nos interesa clasificar, los dos métodos dan resultados similares. 4.3.2 Interpretación de coeficientes Los coeficientes mostrados en la parametrización de arriba se intrepretan más fácilmente como comparaciones de la clase \\(j\\) contra la clase \\(K\\), pues \\[\\log\\left (\\frac{p_g(x)}{p_K(x)}\\right ) = \\beta_{0,{g}} + \\beta_{1,{g}}x_2 + \\ldots + \\beta_{p,{g}} x_p\\] Para comparar la clase \\(j\\) con la clase \\(k\\) notamos que \\[\\log\\left (\\frac{p_j(x)}{p_k(x)}\\right ) = (\\beta_{0,{j}}- \\beta_{0,{k}}) + (\\beta_{1,{j}}-\\beta_{1,{k}} )x_2 + \\ldots + (\\beta_{p,{j}} -\\beta_{p,{k}}) x_p\\] Así que sólo hace falta restar los coeficientes. Nótese adicionalmente que en la parametrización, podemos pensar que \\[\\beta_{0,K} = \\beta_{1,K} = \\cdots = \\beta_{p,K} = 0\\] 4.3.3 Ejemplo: Clasificación de dígitos con regresión multinomial library(readr) digitos_entrena &lt;- read_csv(&#39;datos/zip-train.csv&#39;) ## Parsed with column specification: ## cols( ## .default = col_double() ## ) ## See spec(...) for full column specifications. digitos_prueba &lt;- read_csv(&#39;datos/zip-test.csv&#39;) ## Parsed with column specification: ## cols( ## .default = col_double() ## ) ## See spec(...) for full column specifications. names(digitos_entrena)[1] &lt;- &#39;digito&#39; names(digitos_entrena)[2:257] &lt;- paste0(&#39;pixel_&#39;, 1:256) names(digitos_prueba)[1] &lt;- &#39;digito&#39; names(digitos_prueba)[2:257] &lt;- paste0(&#39;pixel_&#39;, 1:256) En este ejemplo, usamos la función multinom de nnet, que usa BFGS para hacer la optimización: library(nnet) mod_mult &lt;- multinom(digito ~ ., data = digitos_entrena, MaxNWt=100000, maxit = 20) ## # weights: 2580 (2313 variable) ## initial value 16788.147913 ## iter 10 value 2598.959017 ## iter 20 value 1494.978090 ## final value 1494.978090 ## stopped after 20 iterations Checamos para diagnóstico la matriz de confusión de entrenamiento. table(predict(mod_mult), digitos_entrena$digito) ## ## 0 1 2 3 4 5 6 7 8 9 ## 0 1153 0 5 2 3 9 1 1 7 0 ## 1 0 998 0 0 2 0 1 1 2 3 ## 2 2 0 693 1 7 2 8 3 10 2 ## 3 9 0 15 632 2 21 0 2 24 2 ## 4 3 2 9 2 621 4 4 9 10 44 ## 5 24 4 5 19 9 511 43 1 34 6 ## 6 2 0 0 0 1 3 607 0 0 0 ## 7 0 0 1 0 0 1 0 613 1 8 ## 8 1 1 3 2 2 4 0 1 451 2 ## 9 0 0 0 0 5 1 0 14 3 577 Ahora validamos con la muestra de prueba y calculamos error de clasificación: confusion_prueba &lt;- table(predict(mod_mult, newdata = digitos_prueba), digitos_prueba$digito) confusion_prueba ## ## 0 1 2 3 4 5 6 7 8 9 ## 0 335 0 3 0 3 6 4 0 3 0 ## 1 0 252 0 0 1 0 0 0 1 3 ## 2 1 1 171 4 8 0 5 2 3 2 ## 3 3 3 8 145 1 18 0 3 12 0 ## 4 3 6 7 1 176 1 2 6 8 16 ## 5 11 1 5 13 2 130 13 2 14 1 ## 6 4 1 1 0 2 0 143 0 1 0 ## 7 0 0 1 1 2 1 0 130 1 5 ## 8 1 0 2 1 2 1 3 0 118 0 ## 9 1 0 0 1 3 3 0 4 5 150 sum(diag(confusion_prueba))/sum(confusion_prueba) ## [1] 0.8719482 round(prop.table(confusion_prueba, 2),2) ## ## 0 1 2 3 4 5 6 7 8 9 ## 0 0.93 0.00 0.02 0.00 0.02 0.04 0.02 0.00 0.02 0.00 ## 1 0.00 0.95 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.02 ## 2 0.00 0.00 0.86 0.02 0.04 0.00 0.03 0.01 0.02 0.01 ## 3 0.01 0.01 0.04 0.87 0.00 0.11 0.00 0.02 0.07 0.00 ## 4 0.01 0.02 0.04 0.01 0.88 0.01 0.01 0.04 0.05 0.09 ## 5 0.03 0.00 0.03 0.08 0.01 0.81 0.08 0.01 0.08 0.01 ## 6 0.01 0.00 0.01 0.00 0.01 0.00 0.84 0.00 0.01 0.00 ## 7 0.00 0.00 0.01 0.01 0.01 0.01 0.00 0.88 0.01 0.03 ## 8 0.00 0.00 0.01 0.01 0.01 0.01 0.02 0.00 0.71 0.00 ## 9 0.00 0.00 0.00 0.01 0.02 0.02 0.00 0.03 0.03 0.85 El resultado no es muy bueno. Veremos más adelante mejores métodos para este problema. ¿Podemos interpretar el modelo? Una idea es tomar los coeficientes y graficarlos según la estructura de las imágenes: coefs &lt;- coef(mod_mult) coefs_reng &lt;- coefs[1, , drop =FALSE] coefs &lt;- rbind(coefs_reng, coefs) coefs[1 , ] &lt;- 0 dim(coefs) ## [1] 10 257 beta_df &lt;- coefs[,-1] %&gt;% as.data.frame %&gt;% mutate(digito = 0:(nrow(coefs)-1)) %&gt;% gather(pixel, valor, contains(&#39;pixel&#39;)) %&gt;% separate(pixel, into = c(&#39;str&#39;,&#39;pixel_no&#39;), sep=&#39;_&#39;) %&gt;% mutate(x = (as.integer(pixel_no)-1) %% 16, y = -((as.integer(pixel_no)-1) %/% 16)) head(beta_df) ## digito str pixel_no valor x y ## 1 0 pixel 1 0.000000000 0 0 ## 2 1 pixel 1 0.621681333 0 0 ## 3 2 pixel 1 -0.005914605 0 0 ## 4 3 pixel 1 0.044257959 0 0 ## 5 4 pixel 1 0.190966643 0 0 ## 6 5 pixel 1 -0.010655932 0 0 Podemos cruzar la tabla con sí misma para hacer comparaciones: tab_coef &lt;- beta_df %&gt;% select(digito, x, y, valor) tab_coef_1 &lt;- tab_coef names(tab_coef_1) &lt;- c(&#39;digito_1&#39;,&#39;x&#39;,&#39;y&#39;,&#39;valor_1&#39;) tab_cruzada &lt;- full_join(tab_coef_1, tab_coef) %&gt;% mutate(dif = valor_1 - valor) ## Joining, by = c(&quot;x&quot;, &quot;y&quot;) tab_cruzada &lt;- tab_cruzada %&gt;% group_by(digito, digito_1) %&gt;% mutate(dif_s = (dif - mean(dif))/sd(dif)) %&gt;% mutate(dif_p = pmin(pmax(dif_s, -2), 2)) ggplot(tab_cruzada, aes(x=x, y=y)) + geom_tile(aes(fill = dif_p)) + facet_grid(digito_1~digito)+scale_fill_distiller(palette = &quot;Spectral&quot;) Discusión Nótese que no corrimos el modelo hasta convergencia. Vamos a hacerlo ahora: mod_mult &lt;- multinom(digito ~ ., data = digitos_entrena, MaxNWt=100000, maxit = 500) ## # weights: 2580 (2313 variable) ## initial value 16788.147913 ## iter 10 value 2598.959017 ## iter 20 value 1494.978090 ## iter 30 value 903.291402 ## iter 40 value 443.785686 ## iter 50 value 260.626756 ## iter 60 value 190.835491 ## iter 70 value 160.773160 ## iter 80 value 114.048146 ## iter 90 value 88.746976 ## iter 100 value 76.302570 ## iter 110 value 63.400188 ## iter 120 value 54.375215 ## iter 130 value 46.291174 ## iter 140 value 38.303470 ## iter 150 value 28.822810 ## iter 160 value 17.888648 ## iter 170 value 9.531256 ## iter 180 value 2.985614 ## iter 190 value 0.714996 ## iter 200 value 0.209654 ## iter 210 value 0.066710 ## iter 220 value 0.030412 ## iter 230 value 0.014036 ## iter 240 value 0.006702 ## iter 250 value 0.004146 ## iter 260 value 0.001844 ## iter 270 value 0.001128 ## iter 280 value 0.000744 ## iter 290 value 0.000462 ## iter 300 value 0.000308 ## iter 310 value 0.000265 ## iter 320 value 0.000231 ## final value 0.000076 ## converged confusion_prueba &lt;- table(predict(mod_mult, newdata = digitos_prueba), digitos_prueba$digito) confusion_prueba ## ## 0 1 2 3 4 5 6 7 8 9 ## 0 332 0 6 2 4 2 1 2 7 2 ## 1 0 242 1 3 3 4 1 2 0 2 ## 2 2 2 148 5 5 0 4 3 3 0 ## 3 4 1 9 128 4 10 0 3 2 4 ## 4 3 5 8 0 149 8 6 7 5 2 ## 5 0 1 3 11 5 116 8 0 10 1 ## 6 5 7 4 3 10 4 144 0 4 1 ## 7 2 1 3 1 4 1 1 125 2 4 ## 8 6 3 14 7 6 10 4 0 132 3 ## 9 5 2 2 6 10 5 1 5 1 158 sum(diag(confusion_prueba))/sum(confusion_prueba) ## [1] 0.8340807 round(prop.table(confusion_prueba, 2),2) ## ## 0 1 2 3 4 5 6 7 8 9 ## 0 0.92 0.00 0.03 0.01 0.02 0.01 0.01 0.01 0.04 0.01 ## 1 0.00 0.92 0.01 0.02 0.02 0.02 0.01 0.01 0.00 0.01 ## 2 0.01 0.01 0.75 0.03 0.02 0.00 0.02 0.02 0.02 0.00 ## 3 0.01 0.00 0.05 0.77 0.02 0.06 0.00 0.02 0.01 0.02 ## 4 0.01 0.02 0.04 0.00 0.74 0.05 0.04 0.05 0.03 0.01 ## 5 0.00 0.00 0.02 0.07 0.02 0.72 0.05 0.00 0.06 0.01 ## 6 0.01 0.03 0.02 0.02 0.05 0.02 0.85 0.00 0.02 0.01 ## 7 0.01 0.00 0.02 0.01 0.02 0.01 0.01 0.85 0.01 0.02 ## 8 0.02 0.01 0.07 0.04 0.03 0.06 0.02 0.00 0.80 0.02 ## 9 0.01 0.01 0.01 0.04 0.05 0.03 0.01 0.03 0.01 0.89 Y nota que el error es más grande que cuando nos detuvimos antes. Discute en clase: Grafica los coeficientes para este segundo modelo ¿En cuál de los dos modelos es más fácil interpretar los coeficientes? ¿En cuál es menor el error? ¿Cuál crees que es el problema de este segundo modelo comparado con el primero? ¿Por qué crees que sucede? ¿Cómo podríamos corregir este problema? 4.4 Descenso en gradiente para regresión multinomial logística Supondremos \\(K\\) clases, numeradas de \\(0,1,\\ldots, K-1\\). OJO: al aplicar este código debes ser cuidadoso con las etiquetas de clase. pred_ml &lt;- function(x, beta){ p &lt;- ncol(x) K &lt;- length(beta)/(p+1) + 1 beta_mat &lt;- matrix(beta, K - 1, p + 1 , byrow = TRUE) u_beta &lt;- exp(as.matrix(cbind(1, x)) %*% t(beta_mat)) Z &lt;- 1 + apply(u_beta, 1, sum) p_beta &lt;- cbind(u_beta, 1)/Z as.matrix(p_beta) } devianza_calc &lt;- function(x, y){ dev_fun &lt;- function(beta){ p_beta &lt;- pred_ml(x, beta) p &lt;- sapply(1:nrow(x), function(i) p_beta[i, y[i]+1]) -2*sum(log(p)) } dev_fun } grad_calc &lt;- function(x_ent, y_ent){ p &lt;- ncol(x_ent) K &lt;- length(unique(y_ent)) y_fact &lt;- factor(y_ent) # matriz de indicadoras de clase y_dummy &lt;- model.matrix(~-1 + y_fact) salida_grad &lt;- function(beta){ p_beta &lt;- pred_ml(x_ent, beta) e_mat &lt;- (y_dummy - p_beta)[, -K] grad_out &lt;- -2*(t(cbind(1,x_ent)) %*% e_mat) as.numeric(grad_out) } salida_grad } descenso &lt;- function(n, z_0, eta, h_deriv, dev_fun){ z &lt;- matrix(0,n, length(z_0)) z[1, ] &lt;- z_0 for(i in 1:(n-1)){ z[i+1, ] &lt;- z[i, ] - eta * h_deriv(z[i, ]) if(i %% 100 == 0){ print(paste0(i, &#39; Devianza: &#39;, dev_fun(z[i+1, ]))) } } z } x_ent &lt;- digitos_entrena %&gt;% select(contains(&#39;pixel&#39;)) %&gt;% as.matrix y_ent &lt;- digitos_entrena$digito x_ent_s &lt;- scale(x_ent) medias &lt;- attr(x_ent_s, &#39;scaled:center&#39;) sd &lt;- attr(x_ent_s, &#39;scaled:scale&#39;) x_pr &lt;- digitos_prueba %&gt;% select(contains(&#39;pixel&#39;)) %&gt;% as.matrix y_pr &lt;- digitos_prueba$digito beta &lt;- runif(257*9) dev_ent &lt;- devianza_calc(x_ent_s, y_ent) grad &lt;- grad_calc(x_ent_s, y_ent) dev_ent(beta) ## [1] 252780.4 Hacemos algunas revisiiones del gradiente: beta_2 &lt;- beta epsilon &lt;- 0.00001 beta_2[1000] &lt;- beta[1000] + epsilon (dev_ent(beta_2) - dev_ent(beta))/epsilon ## [1] -713.7182 grad(beta)[1000] ## [1] -713.7199 Ya ahora podemos hacer descenso: iteraciones &lt;- descenso(2000, rep(0, 257*9), eta=0.001, h_deriv = grad, dev_fun = dev_ent) ## [1] &quot;100 Devianza: 817.809554010357&quot; ## [1] &quot;200 Devianza: 408.010947736697&quot; ## [1] &quot;300 Devianza: 289.951542494061&quot; ## [1] &quot;400 Devianza: 227.805737974779&quot; ## [1] &quot;500 Devianza: 190.43408903327&quot; ## [1] &quot;600 Devianza: 165.487702748531&quot; ## [1] &quot;700 Devianza: 147.301091651991&quot; ## [1] &quot;800 Devianza: 133.221066964653&quot; ## [1] &quot;900 Devianza: 121.903186824327&quot; ## [1] &quot;1000 Devianza: 112.560175747607&quot; ## [1] &quot;1100 Devianza: 104.688785448699&quot; ## [1] &quot;1200 Devianza: 97.9483674585563&quot; ## [1] &quot;1300 Devianza: 92.0984398108757&quot; ## [1] &quot;1400 Devianza: 86.9631199039947&quot; ## [1] &quot;1500 Devianza: 82.4103777725155&quot; ## [1] &quot;1600 Devianza: 78.3393471851082&quot; ## [1] &quot;1700 Devianza: 74.6718258066508&quot; ## [1] &quot;1800 Devianza: 71.3463032980959&quot; ## [1] &quot;1900 Devianza: 68.3137316150628&quot; x_pr_s &lt;- scale(x_pr, center = medias, scale = sd) probas &lt;- pred_ml(x_pr_s, iteraciones[2000,]) clase &lt;- apply(probas, 1, which.max) table(clase - 1, y_pr ) ## y_pr ## 0 1 2 3 4 5 6 7 8 9 ## 0 347 0 4 1 3 3 1 1 7 0 ## 1 0 252 0 0 4 0 0 1 0 1 ## 2 2 1 168 4 7 1 7 2 5 0 ## 3 2 5 5 148 2 6 0 2 1 0 ## 4 4 0 5 1 168 2 2 5 2 3 ## 5 1 0 2 8 3 139 3 0 6 1 ## 6 0 3 2 1 3 2 156 0 2 0 ## 7 1 1 4 1 3 0 0 133 2 3 ## 8 1 1 8 1 3 5 1 0 136 3 ## 9 1 1 0 1 4 2 0 3 5 166 1 - mean(clase-1 != y_pr) ## [1] 0.9033383 Tarea 4 Ver scripts/tarea_4.Rmd. "],
["regularizacion.html", "Clase 5 Regularización 5.1 Sesgo y varianza de predictores 5.2 Regularización ridge 5.3 Entrenamiento, Validación y Prueba 5.4 Regularización lasso 5.5 Tarea", " Clase 5 Regularización Los métodos para ajustar modelos lineales que vimos en secciones anteriores (mínimos cuadrados y minimización de devianza) 5.1 Sesgo y varianza de predictores Consideremos el problema de regresión, donde el proceso que genera los datos está dado por \\[Y = f(X) + \\epsilon\\] Consideremos que queremos hacer predicciones para una \\(X=x_0\\) particular, de modo que el error es \\[Y - \\hat{f}(x_0) = (f(x_0) - \\hat{f}(x_0)) + \\epsilon\\] Como discutimos antes, no podemos hacer nada por la variación de \\(\\epsilon\\). La pregunta es entonces ¿por qué podría pasar que \\(\\hat{f}(x_0)\\) estuviera lejos de \\(\\hat{f}(x_0)\\)? Recordemos que \\(\\hat{f}(x_0)\\) depende de una muestra de entrenamiento \\({\\mathcal L}\\), de modo que: Puede ser que \\(\\hat{f}(x_0)\\) está consistentemente lejos de \\(f(x_0)\\), independientemente de cuál es la muestra de entrenamiento. Puede ser que \\(\\hat{f}(x_0)\\) varía mucho dependiendo de la muestra de entrenamiento, y en consecuencia es poco probable que \\(\\hat{f}(x_0)\\) esté cerca de \\(f(x_0)\\). Es posible demostrar que \\[E\\left ( (f(x_0)-\\hat{f}(x_0))^2 \\right) = (f(x_0) - E(\\hat{f}(x_0)))^2 + Var (\\hat{f}(x_0))\\] donde los valores esperados y varianza son sobre posibles muestras de entrenamiento. Al primer término le llamamos sesgo : Qué tan lejos en promedio están las estimaciones de nuestro modelo del verdadero valor, y al segundo término le llamamos varianza: qué tanto varían las estimaciones del modelo. Ambas pueden ser razones por las que obtengamos predicciones malas. Ejemplo Consideremos dos métodos: regresión lineal y regresión polinomial (pensemos que es un tipo de ajuste de curvas). Para ilustrar los conceptos de sesgo y varianza simularemos varios posibles muestras de entrenamiento: library(dplyr) library(tidyr) library(purrr) library(ggplot2) f &lt;- function(x){ sin(6*x)} sim_data &lt;- function(n = 15){ x &lt;- runif(n, 0, 1) y &lt;- f(x) + rnorm(n, 0, 0.4) data_frame(x = x, y = y) } dat &lt;- sim_data(n = 100) plot(dat$x,dat$y) set.seed(92114) sims &lt;- data_frame(rep = 1:10) sims &lt;- sims %&gt;% group_by(rep) %&gt;% mutate(data = list(data = sim_data())) %&gt;% unnest Regresión lineal en \\(x\\) nos da diferencias consistentes entre predicciones y observaciones (es un método que sufre de sesgo): ggplot(sims, aes(x=x, y=y)) + geom_point() + facet_wrap(~rep) + geom_smooth(formula = y~x, method =&#39;lm&#39;, colour = &#39;red&#39;, se = FALSE) + ylim(c(-3,3)) Mientras que regresión polinomial nos da diferencias variables y grandes entre predicciones y observaciones (es un método que sufre de varianza): ggplot(sims, aes(x=x, y=y)) + geom_point() + facet_wrap(~rep) + geom_smooth(formula = y~ poly(x, 5, raw = TRUE), method =&#39;lm&#39;, colour = &#39;red&#39;, se = FALSE) + ylim(c(-3,3)) En este ejemplo, ambos métodos se desempeñan mal, pero por razones distintas. El primer método sufre de sesgo: es un método rígido que no aprende de patrones en los datos. El segundo método sufre de varianza: es un método flexible que aprende ruido. Cada uno de estos problemas requiere soluciones diferentes. En esta parte veremos métodos de regularización, que sirven para reducir la varianza con lo que esperamos sean costos menores de sesgo. 5.1.1 Sesgo y varianza en modelos lineales Aunque típicamente pensamos que los modelos lineales son métodos simples, con estructura rígida, y que tienden a sufrir más por sesgo que por varianza (parte de la razón por la que existen métodos más flexibles como bosques aleatorios, redes nueronales, etc.), hay varias razones por las que los métodos lineales pueden sufrir de varianza alta: Cuando la muestra de entrenamiento es relativamente chica (\\(N\\) chica), la varianza puede ser alta. Cuando el número de entradas \\(p\\) es grande, podemos también sufrir de varianza grande (pues tenemos muchos parámetros para estimar). Cuando hay variables correlacionadas en las entradas la varianza también puede ser alta. En estos casos, conviene buscar maneras de reducir varianza - generalmente a costa de un incremento de sesgo. Ejemplo Consideramos regresión logística. En primer lugar, supondremos que tenemos un problema con \\(n=400\\) y \\(p=100\\), y tomamos como modelo para los datos (sin ordenada al origen): \\[p_1(x)=h\\left(\\sum_{j=1}^{100} \\beta_j x_j\\right ),\\] donde \\(h\\) es la función logística. Nótese que este es el verdadero modelo para los datos. Para producir datos de entrenamiento, primero generamos las betas fijas, y después, utilizando estas betas, generamos 400 casos de entrenamiento. Generamos las betas: h &lt;- function(x){ 1 / (1 + exp(-x))} set.seed(2805) beta &lt;- rnorm(100,0,0.1) names(beta) &lt;- paste0(&#39;V&#39;, 1:length(beta)) head(beta) ## V1 V2 V3 V4 V5 ## -0.119875530 0.034627590 -0.081818069 0.014920959 0.040160152 ## V6 ## 0.002043735 Con esta función simulamos datos de entrenamiento (400) y datos de prueba (5000). sim_datos &lt;- function(n, m, beta){ p &lt;- length(beta) #n = casos de entrenamiento, m= casos de prueba, p=num variables mat &lt;- matrix(rnorm((n+m)*p, 0, 0.5), n+m, p) + rnorm(n + m) prob &lt;- h(mat %*% beta) y &lt;- rbinom(n + m, 1, prob) dat &lt;- as.data.frame(mat) dat$y &lt;- y dat$entrena &lt;- FALSE dat$entrena[1:n] &lt;- TRUE dat } set.seed(9921) datos &lt;- sim_datos(n = 400, m = 2000, beta = beta) Y ahora ajustamos el modelo de regresión logística: mod_1 &lt;- glm(y ~ -1 + ., datos %&gt;% filter(entrena) %&gt;% select(-entrena), family = &#39;binomial&#39;) ¿Qué tan buenas fueron nuestras estimaciones? qplot(beta, mod_1$coefficients) + xlab(&#39;Coeficientes&#39;) + ylab(&#39;Coeficientes estimados&#39;) + geom_abline(intercept=0, slope =1) + xlim(c(-1.5,1.5))+ ylim(c(-1.5,1.5)) Y notamos que las estimaciones no son muy buenas. Podemos hacer otra simulación para confirmar que el problema es que las estimaciones son muy variables. Con otra muestra de entrenamiento, vemos que las estimaciones tienen varianza alta. datos_2 &lt;- sim_datos(n = 400, m = 10, beta = beta) mod_2 &lt;- glm(y ~ -1 + ., datos_2 %&gt;% filter(entrena) %&gt;% select(-entrena), family = &#39;binomial&#39;) qplot(mod_1$coefficients, mod_2$coefficients) + xlab(&#39;Coeficientes mod 1&#39;) + ylab(&#39;Coeficientes mod 2&#39;) + geom_abline(intercept=0, slope =1) + xlim(c(-1.5,1.5))+ ylim(c(-1.5,1.5)) Si repetimos varias veces: dat_sim &lt;- lapply(1:50, function(i){ salida &lt;- sim_datos(n=400, m=10, beta) mod &lt;- glm(y ~ -1 + ., salida %&gt;% filter(entrena) %&gt;% select(-entrena), family = &#39;binomial&#39;) data_frame(rep = i, vars = names(coef(mod)), coefs = coef(mod)) }) %&gt;% bind_rows head(dat_sim) ## # A tibble: 6 x 3 ## rep vars coefs ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 V1 -0.20908172 ## 2 1 V2 -0.05377387 ## 3 1 V3 0.14926973 ## 4 1 V4 0.76766084 ## 5 1 V5 0.12293745 ## 6 1 V6 -0.25678295 Vemos que hay mucha variabilidad en la estimación de los coeficientes (en rojo están los verdaderos): dat_sim &lt;- dat_sim %&gt;% mutate(vars = reorder(vars, coefs, mean)) ggplot(dat_sim, aes(x=vars, y=coefs)) + geom_boxplot() + geom_line(data=data_frame(coefs=beta, vars=names(beta)), aes(y=beta, group=1), col=&#39;red&#39;,size=1.1) + coord_flip() En la práctica, nosotros tenemos una sola muestra de entrenamiento. Así que, con una muestra de tamaño \\(n=500\\) como en este ejemplo, obtendremos típicamente resultados no muy buenos. Estos coeficientes ruidosos afectan nuestras predicciones de manera negativa. Vemos ahora lo que pasa con nuestra \\(\\hat{p}_1(x)\\) estimadas, comparándolas con \\(p_1(x)\\), para la primera simulación: dat_e &lt;- datos %&gt;% filter(entrena) dat_p &lt;- datos %&gt;% filter(!entrena) x_e &lt;- dat_e %&gt;% select(-entrena, -y) %&gt;% as.matrix x_p &lt;- dat_p %&gt;% select(-entrena, -y) %&gt;% as.matrix p_entrena &lt;- data_frame(prob_hat_1 = h(mod_1$fitted.values), prob_1 = as.numeric(h(x_e %*% beta)), clase = dat_e$y) p_prueba &lt;- data_frame(prob_hat_1 = as.numeric(h(x_p %*% (mod_1$coefficients))), prob_1 = as.numeric(h(x_p %*% beta)), clase = dat_p$y) Para los datos de entrenamiento: ggplot(p_entrena, aes(x=prob_1, y=prob_hat_1, colour=factor(clase))) + geom_point() Y con la muestra de prueba: ggplot(p_prueba, aes(x=prob_1, y=prob_hat_1, colour=factor(clase))) + geom_point() Si la estimación fuera perfecta, esta gráfica sería una diagonal. Vemos entonces que cometemos errores grandes. El problema no es que nuestro modelo no sea apropiado (logístico), pues ese es el modelo real. El problema es la variabilidad en la estimación de los coeficientes que notamos arriba. La matriz de confusión y la sensibilidad y especificidad: tab &lt;- table(p_prueba$prob_hat_1 &gt; 0.5, p_prueba$clase) prop.table(tab, margin=2) ## ## 0 1 ## FALSE 0.6055777 0.3755020 ## TRUE 0.3944223 0.6244980 5.1.2 Reduciendo varianza de los coeficientes Como el problema es la varianza, podemos atacar este problema poniendo restricciones a los coeficientes, de manera que caigan en rangos más aceptables. Una manera de hacer esto es sustituir el problema de minimización de regresión logística, que es minimizar la devianza: \\[\\min_{\\beta} D(\\beta)\\] con un problema penalizado \\[\\min_{\\beta} D(\\beta) + \\lambda\\sum_{i=1}^p \\beta_j^2\\] escogiendo un valor apropiado de \\(\\lambda\\). También es posible poner restricciones sobre el tamaño de \\(\\sum_{i=1}^p \\beta_j^2\\), lo cual es equivalente al problema de penalización. En este caso obtenemos (veremos más del paquete glmnet): library(glmnet) mod_restringido &lt;- glmnet(x = x_e, y = dat_e$y, alpha = 0, family=&#39;binomial&#39;, intercept = F, lambda = 0.1) beta_penalizado &lt;- coef(mod_restringido)[-1] # quitar intercept Y podemos ver que el tamaño de los coeficientes se redujo considerablemente: sum(beta_penalizado^2) ## [1] 0.4837593 sum(coef(mod_1)^2) ## [1] 18.2092 Los nuevos coeficientes estimados: qplot(beta, beta_penalizado) + xlab(&#39;Coeficientes&#39;) + ylab(&#39;Coeficientes estimados&#39;) + geom_abline(xintercept=0, slope =1) + xlim(c(-0.5,0.5))+ ylim(c(-0.5,0.5)) ## Warning: Ignoring unknown parameters: xintercept p_entrena$prob_hat_pen &lt;- h(x_e %*% as.numeric(beta_penalizado)) p_prueba$prob_hat_pen &lt;- h(x_p %*% as.numeric(beta_penalizado)) Para los datos de entrenamiento: ggplot(p_entrena, aes(x=prob_1, y=prob_hat_pen, colour=factor(clase))) + geom_point() Y con la muestra de prueba: ggplot(p_prueba, aes(x=prob_1, y=prob_hat_pen, colour=factor(clase))) + geom_point() tab &lt;- table(p_prueba$prob_hat_pen &gt; 0.5, p_prueba$clase) prop.table(tab, margin=2) ## ## 0 1 ## FALSE 0.6603586 0.2851406 ## TRUE 0.3396414 0.7148594 Curvas ROC de prueba: library(ROCR) pred &lt;- prediction(predictions = p_prueba$prob_hat_1, labels = p_prueba$clase) perf &lt;- performance(pred, measure = &quot;sens&quot;, x.measure = &quot;fpr&quot;) plot(perf) pred_r &lt;- prediction(predictions = p_prueba$prob_hat_pen, labels = p_prueba$clase) perf_r &lt;- performance(pred_r, measure = &quot;sens&quot;, x.measure = &quot;fpr&quot;) plot(perf_r, add =T, col =&#39;red&#39;) abline(a=0, b=1, col =&#39;gray&#39;) Sin embargo, vemos que en la muestra de entrenamiento se desempeña mejor el modelo sin penalización, como es de esperarse (el mínimo irrestricto es más bajo que el mínimo del problema con restricción). library(ROCR) pred &lt;- prediction(predictions = p_entrena$prob_hat_1, labels = p_entrena$clase) perf &lt;- performance(pred, measure = &quot;sens&quot;, x.measure = &quot;fpr&quot;) plot(perf) pred_r &lt;- prediction(predictions = p_entrena$prob_hat_pen, labels = p_entrena$clase) perf_r &lt;- performance(pred_r, measure = &quot;sens&quot;, x.measure = &quot;fpr&quot;) plot(perf_r, add =T, col =&#39;red&#39;) abline(a=0, b=1, col =&#39;gray&#39;) 5.2 Regularización ridge Arriba vimos un ejemplo de regresión penalizada tipo ridge. Recordemos que para regresión lineal, buscábamos minimizar la cantidad \\[D(\\beta)=\\frac{1}{n}\\sum_{i=1}^n (y_i -\\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij})^2\\] y en regresión logística, \\[D(\\beta)=-\\frac{2}{n}\\sum_{i=1}^n y_i \\log(h(\\beta_0 + \\sum_{j=1}^p \\beta_j x_{ij})) + (1-y_i) \\log(1 - h(\\beta_0 + \\sum_{j=1}^p \\beta_j x_{ij})) ,\\] donde los denotamos de la misma forma para unificar notación. En regresión ridge (lineal/logística), para \\(\\lambda&gt;0\\) fija minimizamos \\[D_{\\lambda}^{ridge} (\\beta)=D(\\beta) + \\lambda\\sum_{i=1}^p \\beta_j^2\\], donde suponemos que las entradas están estandarizadas (centradas y escaladas por la desviación estándar). Observaciones La idea de regresión penalizada consiste en estabilizar la estimación de los coeficientes, especialmente en casos donde tenemos muchas variables en relación a los casos de entrenamiento. La penalización no permite que varíen tan fuertemente los coeficientes. Cuando \\(\\lambda\\) es mas grande, los coeficientes se encogen más fuertemente hacia cero con respecto al problema no regularizado. En este caso, estamos reduciendo la varianza pero potencialmente incrementando el sesgo. Cuando \\(\\lambda\\) es mas chico, los coeficientes se encogen menos fuertemente hacia cero, y quedan más cercanos a los coeficientes de mínimos cuadrados/máxima verosimilitud. En este caso, estamos reduciendo el sesgo pero incrementando la varianza. Nótese que no penalizamos \\(\\beta_0\\). Es posible hacerlo, pero típicamente no lo hacemos. En regresión lineal, de esta forma garantizamos que la predicción \\(\\hat{y}\\), cuando todas las variables \\(x_j\\) toman su valor en la media, es el promedio de las \\(y_i\\)’s de entrenamiento. Igualmente en regresión logística, la probabilidad ajustada cuando las entradas toman su valor en la media es igual a \\(h(\\beta_0)\\). Que las variables estén estandarizadas es importante para que tenga sentido la penalización. Si las variables \\(x_j\\) están en distintas escalas (por ejemplo pesos y dólares), entonces también los coeficientes \\(\\beta_j\\) están en distintas escalas, y una penalización fija no afecta de la misma forma a cada coeficiente. Resolver este problema por descenso en gradiente no tienen dificultad, pues: \\[\\frac{\\partial D_{\\lambda}^{ridge} (\\beta)}{\\partial\\beta_j} = \\frac{\\partial D(\\beta)}{\\beta_j} + 2\\lambda\\beta_j\\] para \\(j=1,\\ldots, p\\), y \\[\\frac{\\partial D_{\\lambda}^{ridge} (\\beta)}{\\partial\\beta_0} = \\frac{\\partial D(\\beta)}{\\beta_0}.\\] De forma que sólo hay que hacer una modificación mínima al algoritmo de descenso en gradiente para el caso no regularizado. 5.2.1 Selección de coeficiente de regularización Seleccionamos \\(\\lambda\\) para minimizar el error de predicción, es decir, para mejorar nuestro modelo ajustado en cuanto a sus predicciones. No tiene sentido intentar escoger \\(\\lambda&gt;0\\) usando el error de entrenamiento. La razón es que siempre que aumentamos \\(\\lambda\\), obtenemos un valor mayor de la suma de cuadrados / devianza del modelo, pues \\(\\lambda\\) más grande implica que pesa menos la minimización de la suma de cuadrados /devianza en el problema de la minimización. En otras palabras, los coeficientes tienen una penalización más fuerte, de modo que el mínimo que se alcanza es mayor en términos de devianza. Intentamos escoger \\(\\lambda\\) de forma que se minimice el error de predicción, o el error de prueba (que estima el error de predicción). Ejemplo (simulación) Regresamos a nuestro problema original simulado de clasificación. La función glmnet se encarga de estandarizar variables y escoger un rango adecuado de penalizaciones \\(\\lambda\\). La función glmnet ajusta varios modelos (parámetro nlambda) para un rango amplio de penalizaciones \\(\\lambda\\): library(glmnet) mod_ridge &lt;- glmnet(x = x_e, y = dat_e$y, alpha = 0, #ridge family=&#39;binomial&#39;, intercept = F, nlambda=50) #normalmente ponemos intercept = T dim(coef(mod_ridge)) ## [1] 101 50 En primer lugar, observamos cómo se encogen los coeficientes para distintos valores de \\(\\lambda\\): plot(mod_ridge, xvar=&#39;lambda&#39;) Para escoger el valor adecuado de \\(\\lambda\\), calculamos la devianza bajo la muestra de prueba: devianza &lt;- function(p, y){ -2*mean(y * log(p) + (1-y) * log(1 - p)) } # predict en glmnet produce probabilidades para los 50 modelos preds_ridge &lt;- predict(mod_ridge, newx = x_p, type = &#39;response&#39;) %&gt;% data.frame %&gt;% mutate(id = 1:nrow(x_p)) %&gt;% gather(modelo, prob, -id) %&gt;% left_join(dat_p %&gt;% mutate(id=1:nrow(dat_p)) %&gt;% select(id, y)) ## Joining, by = &quot;id&quot; head(preds_ridge) ## id modelo prob y ## 1 1 s0 0.5 1 ## 2 2 s0 0.5 1 ## 3 3 s0 0.5 1 ## 4 4 s0 0.5 1 ## 5 5 s0 0.5 1 ## 6 6 s0 0.5 0 tail(preds_ridge) ## id modelo prob y ## 99995 1995 s49 0.50969335 1 ## 99996 1996 s49 0.46159912 1 ## 99997 1997 s49 0.40584244 1 ## 99998 1998 s49 0.01436745 0 ## 99999 1999 s49 0.45568262 1 ## 100000 2000 s49 0.73158603 1 df_lambdas &lt;- data_frame(modelo = attr(mod_ridge$a0, &#39;names&#39;), lambda = mod_ridge$lambda) devianzas_prueba &lt;- preds_ridge %&gt;% group_by(modelo) %&gt;% summarise( devianza = devianza(prob, y)) %&gt;% left_join(df_lambdas) ## Joining, by = &quot;modelo&quot; ggplot(devianzas_prueba, aes(x = lambda, y= devianza)) + scale_x_log10(breaks = round(2^seq(-5,5,1),2)) + geom_point() Buscamos entonces minimizar la devianza (evaluada en la muestra de prueba), que corresponde a tomar un valor de \\(\\lambda\\) alrededor de exp(-2). Discusión: ¿por qué la devianza de prueba tiene esta forma, que es típica para problemas de regularización? El modelo final queda como sigue: df_lambdas ## # A tibble: 50 x 2 ## modelo lambda ## &lt;chr&gt; &lt;dbl&gt; ## 1 s0 225.94322 ## 2 s1 187.22622 ## 3 s2 155.14365 ## 4 s3 128.55867 ## 5 s4 106.52921 ## 6 s5 88.27466 ## 7 s6 73.14816 ## 8 s7 60.61369 ## 9 s8 50.22710 ## 10 s9 41.62032 ## # ... with 40 more rows coefs_selec &lt;- coef(mod_ridge)[-1, &#39;s38&#39;] pred_prueba_final &lt;- h(x_p %*% coefs_selec) tab_confusion &lt;- table(pred_prueba_final &gt; 0.5, dat_p$y) tab_confusion ## ## 0 1 ## FALSE 656 289 ## TRUE 348 707 prop.table(tab_confusion, margin=2) ## ## 0 1 ## FALSE 0.6533865 0.2901606 ## TRUE 0.3466135 0.7098394 Ejemplo: variables correlacionadas Ridge es efectivo para reducir varianza inducida por variables correlacionadas. library(readr) dat_grasa &lt;- read_csv(file = &#39;datos/bodyfat.csv&#39;) head(dat_grasa) ## # A tibble: 6 x 14 ## grasacorp edad peso estatura cuello pecho abdomen cadera muslo ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12.3 23 154.25 67.75 36.2 93.1 85.2 94.5 59.0 ## 2 6.1 22 173.25 72.25 38.5 93.6 83.0 98.7 58.7 ## 3 25.3 22 154.00 66.25 34.0 95.8 87.9 99.2 59.6 ## 4 10.4 26 184.75 72.25 37.4 101.8 86.4 101.2 60.1 ## 5 28.7 24 184.25 71.25 34.4 97.3 100.0 101.9 63.2 ## 6 20.9 24 210.25 74.75 39.0 104.5 94.4 107.8 66.0 ## # ... with 5 more variables: rodilla &lt;dbl&gt;, tobillo &lt;dbl&gt;, biceps &lt;dbl&gt;, ## # antebrazo &lt;dbl&gt;, muñeca &lt;dbl&gt; nrow(dat_grasa) ## [1] 252 set.seed(127) dat_grasa$unif &lt;- runif(nrow(dat_grasa), 0, 1) dat_grasa &lt;- arrange(dat_grasa, unif) dat_grasa$id &lt;- 1:nrow(dat_grasa) bfat_e &lt;- dat_grasa[1:100,] bfat_p &lt;- dat_grasa[101:252,] xbf_e &lt;- bfat_e %&gt;% select(estatura, peso, abdomen, muslo, biceps) %&gt;% as.matrix cor(xbf_e) ## estatura peso abdomen muslo biceps ## estatura 1.00000000 0.2534694 0.0928379 0.04835578 0.1857616 ## peso 0.25346939 1.0000000 0.9059227 0.86412005 0.8273691 ## abdomen 0.09283790 0.9059227 1.0000000 0.78986726 0.7308348 ## muslo 0.04835578 0.8641200 0.7898673 1.00000000 0.7899550 ## biceps 0.18576161 0.8273691 0.7308348 0.78995504 1.0000000 ridge_bodyfat &lt;- glmnet(x = scale(xbf_e), y = bfat_e$grasacorp, alpha=0, lambda = exp(seq(-5, 5, 0.25))) plot(ridge_bodyfat, xvar = &#39;lambda&#39;, label=TRUE) Donde notamos que las variables con correlaciones altas se “encogen” juntas hacia valores similares conforme aumentamos la constante de penalización \\(\\lambda\\). Nótese que para regularización muy baja peso y abdomen por ejemplo, tienen signos opuestos y valores altos: esto es posible pues tienen correlación alta, de modo que la función de predicción está pobremente determinada: hay un espacio grande de pares de parámetros que dan predicciones similares, y esto resulta en coeficientes con varianza alta y predicciones inestables y ruidosas. Nótese, adicionalmente, que los coeficientes parecen tener más sentido en relación al problema con regularización. Regularización, en este tipo de problemas, es una de las componentes necesarias (pero no suficiente) para ir hacia interpretación del fenómeno que nos interesa. 5.3 Entrenamiento, Validación y Prueba El enfoque que vimos arriba, en donde dividemos la muestra en dos partes al azar, es la manera más fácil de seleccionar modelos. En general, el proceso es el siguiente: Una parte con los que ajustamos todos los modelos que nos interesa. Esta es la muestra de entrenamiento Una parte como muestra de prueba, con el que evaluamos el desempeño de cada modelo ajustado en la parte anterior. En este contexto, a esta muestra se le llama muestra de validación}. Posiblemente una muestra adicional independiente, que llamamos muestra de prueba, con la que hacemos una evaluación final del modelo seleccionado arriba. Es una buena idea apartar esta muestra si el proceso de validación incluye muchos métodos con varios parámetros afinados (como la \\(\\lambda\\) de regresión ridge). knitr::include_graphics(&quot;./imagenes/div_muestra.png&quot;) Cuando tenemos datos abundantes, este enfoque es el usual. Por ejemplo, podemos dividir la muestra en 50-25-25 por ciento. Ajustamos modelos con el primer 50%, evaluamos y seleccionamos con el segundo 25% y finalmente, si es necesario, evaluamos el modelo final seleccionado con la muestra final de 25%. La razón de este proceso es que así podemos ir y venir entre entrenamiento y validación, buscando mejores enfoques y modelos, y no ponemos en riesgo la estimación final del error. (Pregunta: ¿por qué probar agresivamente buscando mejorar el error de validación podría ponder en riesgo la estimación final del error del modelo seleccionado? ) 5.3.1 Validación cruzada En muchos casos, no queremos apartar una muestra de validación para seleccionar modelos, pues no tenemos muchos datos (al dividir la muestra obtendríamos un modelo relativamente malo en relación al que resulta de todos los datos). Un criterio para seleccionar la regularización adecuada es el de **validación cruzada*, que es un método computacional para producir una estimación interna (usando sólo muestra de entrenamiento) del error de predicción. En validación cruzada (con \\(k\\) vueltas), construimos al azar una partición, con tamaños similares, de la muestra de entrenamiento \\({\\mathcal L}=\\{ (x_i,y_i)\\}_{i=1}^n\\): \\[ {\\mathcal L}={\\mathcal L}_1\\cup {\\mathcal L}_2\\cup\\cdots\\cup {\\mathcal L}_k.\\] knitr::include_graphics(&quot;./imagenes/div_muestra_cv.png&quot;) Construimos \\(k\\) modelos distintos, digamos \\(\\hat{f}_j\\), usando solamente la muestra \\({\\mathcal L}-{\\mathcal L}_j\\). Este modelo lo evaluamos usando la parte que no usamos, \\({\\mathcal L}_j\\), para obtener una estimación honesta del error del modelo \\(\\hat{f}_k\\), a la que denotamos por \\(\\hat{e}_j\\). Notemos entonces que tenemos \\(k\\) estimaciones del error \\(\\hat{e}_1,\\ldots, \\hat{e}_k\\), una para cada uno de los modelos que construimos. La idea ahora es que Cada uno de los modelos \\(\\hat{f}_j\\) es similar al modelo ajustado con toda la muestra \\(\\hat{f}\\), de forma que podemos pensar que cada una de las estimaciones \\(\\hat{e}_j\\) es un estimador del error de \\(\\hat{f}\\). Dado el punto anterior, podemos construir una mejor estimación promediando las \\(k\\) estimaciones anteriores, para obtener: \\[\\widehat{cv} = \\frac{1}{k} \\sum_{j=1}^k \\hat{e}_j.\\] ¿Cómo escoger \\(k\\)? Usualmente se usan \\(k=5,10,20\\), y \\(k=10\\) es el más popular. La razón es que cuando \\(k\\) es muy chico, tendemos a evaluar modelos construidos con pocos datos (comparado al modelo con todos los datos de entrenamiento). Por otra parte, cuando \\(k\\) es grande el método puede ser muy costoso (por ejemplo, si \\(k=N\\), hay que entrenar un modelo para cada dato de entrada). Por ejemplo, el paquete glmnet incluye la función cv.glmnet, que hace los \\(k\\) ajustes para cada una de las lambdas: library(glmnet) set.seed(291) cv_mod_ridge &lt;- cv.glmnet(x = x_e, y=dat_e$y, alpha = 0, family=&#39;binomial&#39;, intercept = F, nfolds = 10, nlambda=50) plot(cv_mod_ridge) cv_mod_ridge$lambda.min ## [1] 0.2155714 cv_mod_ridge$lambda.1se ## [1] 7.666755 Nótese que la estimación del error de predicción por validación cruzada incluye un error de estimación (intervalos). Esto nos da dos opciones para escoger la lambda final: Escoger la que de el mínimo valor de error por validación cruzada Escoger la lambda más grande que no esté a más de 1 error estándar del mínimo. En la gráfica anterior se muestran las dos posibilidades. La razón del segundo criterio es tomar el modelo más simple que tenga error consistente con el mejor modelo. ¿Cómo se desempeña validación cruzada como estimación del error? cross_valid &lt;- data_frame(devianza_cv = cv_mod_ridge$cvm, modelo = attr(cv_mod_ridge$glmnet.fit$a0, &#39;names&#39;)[1:49]) devs &lt;- devianzas_prueba %&gt;% left_join(cross_valid) %&gt;% rename(devianza_prueba = devianza) %&gt;% gather(tipo, devianza, devianza_prueba, devianza_cv) ## Joining, by = &quot;modelo&quot; ggplot(devs, aes(x=log(lambda), y=devianza, colour=tipo)) + geom_point() ## Warning: Removed 1 rows containing missing values (geom_point). Vemos que la estimación en algunos casos no es tan buena, aún cuando todos los datos fueron usados. Pero el mínimo se encuentra en lugares muy similares. La razón es que validación cruzada en realidad considera perturbaciones del conjunto de entrenamiento, de forma que lo que intenta evaluar el error producido, para cada lambda, sobre distintas muestras de entrenamiento. En realidad nosotros queremos evaluar el error de predicción del modelo que ajustamos. Validación cruzada es más un estimador del error esperado de predicción sobre los modelos que ajustaríamos con distintas muestras de entrenamiento. El resultado es que: Usamos validación cruzada para escoger la complejidad adecuada de la familia de modelos que consideramos. Como estimación del error de predicción del modelo que ajustamos, validación cruzada es más seguro que usar el error de entrenamiento, que muchas veces puede estar fuertemente sesgado hacia abajo. Sin embargo, lo mejor en este caso es utilizar una muestra de prueba. Ejercicio Consideremos el ejemplo de reconocimiento de dígitos. library(readr) digitos_entrena &lt;- read_csv(&#39;datos/zip-train.csv&#39;) digitos_prueba &lt;- read_csv(&#39;datos/zip-test.csv&#39;) names(digitos_entrena)[1] &lt;- &#39;digito&#39; names(digitos_entrena)[2:257] &lt;- paste0(&#39;pixel_&#39;, 1:256) names(digitos_prueba)[1] &lt;- &#39;digito&#39; names(digitos_prueba)[2:257] &lt;- paste0(&#39;pixel_&#39;, 1:256) set.seed(2912) if(TRUE){ digitos_entrena_s &lt;- sample_n(digitos_entrena, size = 2000) } else { digitos_entrena_s &lt;- digitos_entrena } x_e &lt;- digitos_entrena_s %&gt;% select(-digito) %&gt;% as.matrix x_p &lt;- digitos_prueba %&gt;% select(-digito) %&gt;% as.matrix library(doMC) ## Loading required package: iterators ## Loading required package: parallel registerDoMC(cores=5) digitos_cv &lt;- cv.glmnet(x = x_e, y = factor(digitos_entrena_s$digito), family = &#39;multinomial&#39;, alpha = 0, parallel = TRUE, nfolds = 10, lambda = exp(seq(-12, 2, 1))) plot(digitos_cv) preds_prueba &lt;- predict(digitos_cv, newx = x_p, s = &#39;lambda.min&#39;)[,,1] # solo un grupo de coeficientes dim(preds_prueba) ## [1] 2007 10 preds_clase &lt;- apply(preds_prueba, 1, which.max) table(preds_clase, digitos_prueba$digito) ## ## preds_clase 0 1 2 3 4 5 6 7 8 9 ## 1 348 0 4 3 1 6 3 1 5 0 ## 2 0 252 0 0 1 0 0 0 0 3 ## 3 2 1 167 5 6 1 3 0 8 1 ## 4 2 2 8 140 0 11 0 1 6 0 ## 5 3 5 8 1 172 3 3 9 2 6 ## 6 0 0 0 12 1 126 3 2 8 1 ## 7 2 2 2 0 8 2 158 0 0 0 ## 8 0 0 1 1 1 3 0 131 0 2 ## 9 1 1 8 2 3 6 0 0 135 1 ## 10 1 1 0 2 7 2 0 3 2 163 mean(preds_clase -1 != digitos_prueba$digito) ## [1] 0.1071251 Este modelo mejora considerablemente al modelo sin regularización. Observación: Cuando vimos regresión multinomial, la última clase es uno menos la suma del resto de probabilidades de clase (\\((K-1)(p+1)\\) parámetros). La salida de glmnet, sin embargo, tiene coeficientes para todas las clases (\\(K(p+1)\\) parámetros). ¿Por qué en regresión ridge no está sobreparametrizado el modelo? 5.4 Regularización lasso Otra forma de regularización es el lasso, que en lugar de penalizar con la suma de cuadrados en los coeficientes, penaliza por la suma de su valor absoluto. En regresión lasso (lineal/logística), para \\(\\lambda&gt;0\\) fija minimizamos \\[D_{\\lambda}^2 (\\beta)=D(\\beta) + \\lambda\\sum_{i=1}^p |\\beta_j|\\], donde suponemos que las entradas están estandarizadas (centradas y escaladas por la desviación estándar). El problema de minimización de ridge y de lasso se pueden reescribir como problemas de restricción: En regresión lasso (lineal/logística), para \\(s&gt;0\\) fija minimizamos \\[D(\\beta), \\] sujeto a \\[\\sum_{i=1}^p |\\beta_j|&lt; s\\] donde suponemos que las entradas están estandarizadas (centradas y escaladas por la desviación estándar). En regresión ridge (lineal/logística), para \\(t&gt;0\\) fija minimizamos \\[D(\\beta), \\] sujeto a \\[\\sum_{i=1}^p \\beta_j^2 &lt; t\\] donde suponemos que las entradas están estandarizadas (centradas y escaladas por la desviación estándar). \\(s\\) y \\(t\\) chicas corresponden a valores de penalización \\(\\lambda\\) grandes. En un principio, puede parecer que ridge y lasso deben dar resultados muy similares, pues en ambos casos penalizamos por el tamaño de los coeficientes. Sin embargo, son distintos de una manera muy importante. En la siguiente gráfica regresentamos las curvas de nivel de \\(D(\\beta)\\). Recordemos que en mínimos cuadrados o regresión logística intentamos minimizar esta cantidad sin restricciones, y este mínimo se encuentra en el centro de estas curvas de nivel. Para el problema restringido, buscamos más bien la curva de nivel más baja que intersecta la restricción: knitr::include_graphics(&#39;./imagenes/ridge_lasso.png&#39;) Y obsérvese ahora que la solución de lasso puede hacer algunos coeficientes igual a 0. Es decir, En regresión ridge, los coeficientes se encogen gradualmente desde la solución no restringida hasta el origen. Ridge es un método de encogimiento de coeficientes. En regresión lasso, los coeficientes se encogen gradualmente, pero también se excluyen variables del modelo. Por eso lasso es un método de encogimiento y selección de variables. Regresión ridge es especialmente útil cuando tenemos varias variables de entrada fuertemente correlacionadas. Regresión ridge intenta encoger juntos coeficientes de variables correlacionadas para reducir varianza en las predicciones. Lasso encoge igualmente coeficientes para reducir varianza, pero también comparte similitudes con regresión de mejor subconjunto, en donde para cada número de variables \\(l\\) buscamos escoger las \\(l\\) variables que den el mejor modelo. Sin embargo, el enfoque de lasso es más escalable y puede calcularse de manera más simple. Descenso en gradiente no es apropiado para regresión lasso (ver documentación de glmnet para ver cómo se hace en este paquete). El problema es que los coeficientes nunca se hacen exactamente cero, pues la restricción no es diferenciable en el origen (coeficientes igual a cero). Ejemplo Consideramos el ejemplo de bodyfat: library(readr) dat_grasa &lt;- read_csv(file = &#39;datos/bodyfat.csv&#39;) head(dat_grasa) ## # A tibble: 6 x 14 ## grasacorp edad peso estatura cuello pecho abdomen cadera muslo ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12.3 23 154.25 67.75 36.2 93.1 85.2 94.5 59.0 ## 2 6.1 22 173.25 72.25 38.5 93.6 83.0 98.7 58.7 ## 3 25.3 22 154.00 66.25 34.0 95.8 87.9 99.2 59.6 ## 4 10.4 26 184.75 72.25 37.4 101.8 86.4 101.2 60.1 ## 5 28.7 24 184.25 71.25 34.4 97.3 100.0 101.9 63.2 ## 6 20.9 24 210.25 74.75 39.0 104.5 94.4 107.8 66.0 ## # ... with 5 more variables: rodilla &lt;dbl&gt;, tobillo &lt;dbl&gt;, biceps &lt;dbl&gt;, ## # antebrazo &lt;dbl&gt;, muñeca &lt;dbl&gt; nrow(dat_grasa) ## [1] 252 set.seed(127) dat_grasa$unif &lt;- runif(nrow(dat_grasa), 0, 1) dat_grasa &lt;- arrange(dat_grasa, unif) dat_grasa$id &lt;- 1:nrow(dat_grasa) dat_e &lt;- dat_grasa[1:150,] dat_p &lt;- dat_grasa[151:252,] x_e &lt;- dat_e %&gt;% select(-grasacorp, -id, -unif) %&gt;% as.matrix x_p &lt;- dat_p %&gt;% select(-grasacorp, -id, -unif) %&gt;% as.matrix mod_bodyfat &lt;- cv.glmnet(x = x_e, y = dat_e$grasacorp, alpha = 1) #alpha=1 para lasso plot(mod_bodyfat) coeficientes &lt;- predict(mod_bodyfat, s =&#39;lambda.1se&#39;, type=&#39;coefficients&#39;) coeficientes ## 14 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) -20.75924241 ## edad 0.05179279 ## peso . ## estatura -0.09936002 ## cuello . ## pecho . ## abdomen 0.58019360 ## cadera . ## muslo . ## rodilla . ## tobillo . ## biceps . ## antebrazo . ## muñeca -0.51756817 pred_prueba &lt;- predict(mod_bodyfat, newx = x_p, s =&#39;lambda.1se&#39;) sqrt(mean((pred_prueba-dat_p$grasacorp)^2)) ## [1] 4.374339 Comparado con regresión lineal: pred_prueba &lt;- predict(lm(grasacorp ~., data = dat_e %&gt;% select(-id, -unif)), newdata=dat_p) sqrt(mean((pred_prueba-dat_p$grasacorp)^2)) ## [1] 4.311924 5.5 Tarea Repite el ejercicio de spam (con todas las variables), y utiliza regresión ridge (glmnet). Escoge el parámetro de regularización con validación cruzada y recalcula la matriz de confusión. ¿Obtuviste ganancias en clasificación? Checa los nuevos coeficientes y compara con los que obtuviste usando regresión logística sin regularización. (Nota: los coeficientes que devuelve glmnet son no estandarizados, aún cuando el cálculo se hace estandarizando - si quieres obtener coeficientes estandarizados puedes estandarizar a mano antes de correr glmnet). "],
["extensiones-para-regresion-lineal-y-logistica.html", "Clase 6 Extensiones para regresión lineal y logística 6.1 Cómo hacer más flexible el modelo lineal 6.2 Transformación de entradas 6.3 Variables cualitativas 6.4 Interacciones 6.5 Categorización de variables 6.6 Splines", " Clase 6 Extensiones para regresión lineal y logística Los modelos lineales son modelos simples que tienen la ventaja de que es relativamente fácil entender cómo contribuyen las variables de entrada (simplemente describimos los coeficientes), y es relativamente fácil ajustarlos. Sin embargo, puede ser que sean pobres desde el punto de vista predictivo. Hay dos razones: Los coeficientes tienen varianza alta, de modo que las predicciones resultantes son inestables (por ejemplo, por pocos datos o variables de entradas correlacionadas). En este caso, vimos que con el enfoque de regularización ridge o lasso podemos mejorar la estabilidad, las predicciones, y obtener modelos más parsimoniosos. El modelo tiene sesgo alto, en el sentido de que la estructura lineal es deficiente para describir patrones claros e importantes en los datos. Este problema puede suceder cuando tenemos relaciones complejas entre las variables. Cuando hay relativamente pocas entradas y suficientes datos, puede ser posible ajustar estructuras más realistas y complejas. Aunque veremos otros métodos para atacar este problema más adelante, a veces extensiones simples del modelo lineal pueden resolver este problema. Igualmente, esperamos encontrar mejores predicciones con modelos más realistas. 6.1 Cómo hacer más flexible el modelo lineal Podemos construir modelos lineales más flexibles expandiendo el espacio de entradas con transformaciones y combinaciones de las variables originales de entrada. La idea básica es entonces transformar a nuevas entradas, antes de ajustar un modelo: \\[(x_1,...,x_p) \\to (b_1(x),...,b_M (x)).\\] donde típicamente \\(M\\) es mayor que \\(p\\). Entonces, en lugar de ajustar el modelo lineal en las \\(x_1,\\ldots, x_p\\), que es \\[ f(x) = \\beta_0 + \\sum_{i=1}^p \\beta_jx_j\\] ajustamos un modelo lineal en las entradas transformadas: \\[ f(x) = \\beta_0 + \\sum_{i=1}^M \\beta_jb_j(x).\\] Como cada \\(b_j\\) es una función que toma valores numéricos, podemos considerarla como una entrada derivada de las entradas originales. Ejemplo Si \\(x_1\\) es compras totales de un cliente de tarjeta de crédito, y \\(x_2\\) es el número de compras, podemos crear una entrada derivada \\(b_1(x_1,x_2)=x_1/x_2\\) que representa el tamaño promedio por compra. Podríamos entonces poner \\(b_2(x_1,x_2)=x_1\\), \\(b_3(x_1,x_2)=x_2\\), y ajustar un modelo lineal usando las entradas derivadas \\(b_1,b_2, b_3\\). Lo conveniente de este enfoque es que lo único que hacemos para hacer más flexible el modelo es transformar en primer lugar las variables de entrada (quizá produciendo más entradas que el número de variables originales). Después construimos un modelo lineal, y todo lo que hemos visto aplica sin cambios: el modelo sigue siendo lineal, pero el espacio de entradas es diferente (generalmente expandido). Veremos las siguientes técnicas: Incluir variables cualitativas (categóricas). Transformación de variables. Interacciones entre variables: incluir términos de la forma \\(x_1x_2\\) Regresión polinomial: incluír términos de la forma \\(x_1^2\\), \\(x_1^3\\), etcétera. Splines de regresión. 6.2 Transformación de entradas Una técnica útil para mejorar el sesgo de modelos de regresión consiste en incluir o sustituir valores transformados de las variables de entrada. Una de las más comunes es usar logaritmo para variables positivas: Ejemplo Consideramos predecir el quilataje de un diamante en función de su precio. library(ggplot2) library(dplyr) library(tidyr) set.seed(231) diamonds_muestra &lt;- sample_n(diamonds, 3000) ggplot(diamonds_muestra, aes(x=price, y=carat)) + geom_point() + geom_smooth(method = &#39;lm&#39;) Nótese que el modelo lineal está sesgado, y produce sobrestimaciones y subestimaciones para distintos valores de \\(x\\). Aunque podríamos utilizar un método más flexible para este modelo, una opción es transformar entrada y salida con logaritmo: diamonds_muestra &lt;- diamonds_muestra %&gt;% mutate(log_price = log(price), log_carat = log(carat)) ggplot(diamonds_muestra, aes(x=log_price, y=log_carat)) + geom_point() + geom_smooth(method = &#39;lm&#39;) Nota: si tenemos ceros en los datos podemos usar también \\(\\log(x+1)\\). Podemos graficar también en unidades originales: ggplot(diamonds_muestra, aes(x=price/1000, y=carat)) + geom_point() + geom_smooth(method = &#39;lm&#39;) + scale_x_log10(breaks=2^seq(-1,5,1)) + scale_y_log10(breaks=2^seq(-2,5,1)) Cuando una variable toma valores positivos y recorre varios órdenes de magnitud, puede ayudar transformar con logaritmo o raíz cuadrada (esto incluye transformar la variable respuesta). Menos común: variables que son proporciones \\(p\\) pueden transformarse mediante la transformación inversa de la logística (\\(x = \\log(\\frac{p}{1-p})\\).) 6.3 Variables cualitativas Muchas veces queremos usar variables cualitativas como entradas de nuestro modelo. Pero en la expresión \\[ f(x) = \\beta_0 + \\sum_{i=1}^p \\beta_jx_j,\\] todas las entradas son numéricas. Podemos usar un truco simple para incluir variables cualitativas Ejemplo Supongamos que queremos incluir la variable color: diamonds_muestra %&gt;% group_by(color) %&gt;% count ## # A tibble: 7 x 2 ## # Groups: color [7] ## color n ## &lt;ord&gt; &lt;int&gt; ## 1 D 383 ## 2 E 542 ## 3 F 533 ## 4 G 645 ## 5 H 471 ## 6 I 270 ## 7 J 156 ggplot(diamonds_muestra, aes(x=price, y=carat, colour=color, group=color)) + geom_point(alpha=0.5) + geom_smooth(method=&#39;lm&#39;, se=FALSE, size=1.5) + scale_y_log10(breaks=c(0.25,0.5,1,2))+ scale_x_log10(breaks=c(500,1000,2000,4000,8000)) Podemos incluir de manera simple esta variable creando variables dummy o indicadoras, que son variables que toman valores 0 o 1 dependiendo de cada clase: diamonds_muestra &lt;- diamonds_muestra %&gt;% mutate(color= as.character(color)) datos &lt;- diamonds_muestra[, c(&#39;log_carat&#39;, &#39;log_price&#39;, &#39;color&#39;)] head(datos) ## # A tibble: 6 x 3 ## log_carat log_price color ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.07696104 8.765771 H ## 2 0.26236426 8.950792 H ## 3 -0.23572233 7.948385 G ## 4 -1.17118298 6.733402 F ## 5 0.23901690 8.421123 J ## 6 0.03922071 8.811205 G x_e &lt;- model.matrix( ~ color, data = datos) head(x_e, 10) ## (Intercept) colorE colorF colorG colorH colorI colorJ ## 1 1 0 0 0 1 0 0 ## 2 1 0 0 0 1 0 0 ## 3 1 0 0 1 0 0 0 ## 4 1 0 1 0 0 0 0 ## 5 1 0 0 0 0 0 1 ## 6 1 0 0 1 0 0 0 ## 7 1 0 1 0 0 0 0 ## 8 1 1 0 0 0 0 0 ## 9 1 0 0 1 0 0 0 ## 10 1 0 1 0 0 0 0 Y ahora podemos hacer: datos_d &lt;- as.data.frame(x_e) datos_d$log_carat &lt;- datos$log_carat datos_d$log_price &lt;- datos$log_price datos_d$`(Intercept)` &lt;- NULL mod_1 &lt;- lm(log_carat ~ ., data = datos_d) summary(mod_1) ## ## Call: ## lm(formula = log_carat ~ ., data = datos_d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.73747 -0.08570 -0.00132 0.08607 0.78010 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.711054 0.020670 -227.923 &lt; 2e-16 *** ## colorE 0.001684 0.009212 0.183 0.854984 ## colorF 0.027917 0.009245 3.020 0.002551 ** ## colorG 0.032919 0.008904 3.697 0.000222 *** ## colorH 0.109352 0.009518 11.489 &lt; 2e-16 *** ## colorI 0.184680 0.010983 16.815 &lt; 2e-16 *** ## colorJ 0.259030 0.013155 19.690 &lt; 2e-16 *** ## log_price 0.546347 0.002538 215.272 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1379 on 2992 degrees of freedom ## Multiple R-squared: 0.9436, Adjusted R-squared: 0.9435 ## F-statistic: 7152 on 7 and 2992 DF, p-value: &lt; 2.2e-16 Nótese que si la variable categórica tiene \\(K\\) clases, solo creamos variables indicadores de las primeras \\(K-1\\) clases, pues la dummy de la última clase tiene información redundante: es decir, si para las primeras \\(K-1\\) clases las variables dummy son cero, entonces ya sabemos que se trata de la última clase \\(K\\), y no necesitamos incluir una indicadora para la última clase. Más fácilmente, la función lm hace la codificación dummy automáticamente. Por ejemplo, para el modelo logarítmico: lm(log_carat ~ log_price + color, data = diamonds_muestra) ## ## Call: ## lm(formula = log_carat ~ log_price + color, data = diamonds_muestra) ## ## Coefficients: ## (Intercept) log_price colorE colorF colorG ## -4.711054 0.546347 0.001684 0.027917 0.032919 ## colorH colorI colorJ ## 0.109352 0.184680 0.259030 Observaciones: - Nótese también que no hay coeficiente para una de las clases, por lo que discutimos arriba. También podemos pensar que el coeficiente de esta clase es 0, y así comparamos con las otras clases. - Cuando tenemos variables dummy, el intercept se interpreta con el nivel esperado cuando las variables cuantitativas valen cero, y la variable categórica toma la clase que se excluyó en la construcción de las indicadoras. Podemos incluir variables cualitativas usando este truco de codificación dummy (también llamado a veces one-hot encoding). Ojo: variables con muchas categorías pueden inducir varianza alta en el modelo (dependiendo del tamaño de los datos). En estos casos conviene usar regularización y quizá (si es razonable) usar categorizaciones más gruesas. 6.4 Interacciones En el modelo lineal, cada variable contribuye de la misma manera independientemente de los valores de las otras variables. Esta es un simplificación o aproximación útil, pero muchas veces puede producir sesgo demasiado grande en el modelo. Por ejemplo: consideremos los siguientes datos de la relación de mediciones de temperatura y ozono en la atmósfera: head(airquality) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 air &lt;- filter(airquality, !is.na(Ozone) &amp; !is.na(Wind) &amp; !is.na(Temp)) lm(Ozone ~Temp, data = air[1:80,]) ## ## Call: ## lm(formula = Ozone ~ Temp, data = air[1:80, ]) ## ## Coefficients: ## (Intercept) Temp ## -136.474 2.306 set.seed(9132) air &lt;- sample_n(air, 116) ggplot(air[1:50,], aes(x = Temp, y = Ozone)) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = FALSE) Y notamos un sesgo posible en nuestro modelo. Si coloreamos por velocidad del viento: cuantiles &lt;- quantile(air$Wind) ggplot(air[1:50,], aes(x = Temp, y = Ozone, colour= cut(Wind, cuantiles))) + geom_point() + geom_smooth(method = &#39;lm&#39;, se = FALSE) Nótese que parece ser que cuando los niveles de viento son altos, entonces hay una relación más fuerte entre temperatura y Ozono. Esto es una interacción de temperatura y viento. Podemos hacer los siguiente: incluír un factor adicional, el producto de temperatura con viento: air$temp_wind &lt;- air$Temp*air$Wind mod_0 &lt;- lm(Ozone ~ Temp, data = air[1:50,]) mod_1 &lt;- lm(Ozone ~ Temp + Wind, data = air[1:50,]) mod_2 &lt;- lm(Ozone ~ Temp + Wind + temp_wind, air[1:50,]) mod_2 ## ## Call: ## lm(formula = Ozone ~ Temp + Wind + temp_wind, data = air[1:50, ## ]) ## ## Coefficients: ## (Intercept) Temp Wind temp_wind ## -317.8272 4.8036 15.9498 -0.2311 pred_0 &lt;- predict(mod_0, newdata = air[51:116,]) pred_1 &lt;- predict(mod_1, newdata = air[51:116,]) pred_2 &lt;- predict(mod_2, newdata = air[51:116,]) mean(abs(pred_0-air[51:116,&#39;Ozone&#39;])) ## [1] 19.88217 mean(abs(pred_1-air[51:116,&#39;Ozone&#39;])) ## [1] 17.13767 mean(abs(pred_2-air[51:116,&#39;Ozone&#39;])) ## [1] 15.52405 Podemos interpretar el modelo con interacción de la siguiente forma: Si \\(Wind = 5\\), entonces la relación Temperatura Ozono es: \\[ Ozono = -290 + 4.5Temp + 14.6(5) - 0.2(Temp)(5) = -217 + 3.5Temp\\] Si \\(Wind=10\\), entonces la relación Temperatura Ozono es: \\[ Ozono = -290 + 4.5Temp + 14.6(15) - 0.2(Temp)(15) = -71 + 1.5Temp\\] Incluir interacciones en modelos lineales es buena idea para problemas con un número relativamente chico de variables (por ejemplo, \\(p &lt; 10\\)). En estos casos, conviene comenzar agregando interacciones entre variables que tengan efectos relativamente grandes en la predicción. No es tan buena estrategia para un número grande de variables: por ejemplo, para clasificación de dígitos, hay 256 entradas. Poner todas las interacciones añadiría más de 30 mil variables adicionales, y es difícil escoger algunas para incluir en el modelo a priori. Pueden escribirse interacciones en fórmulas de lm y los cálculos se hacen automáticamente: mod_3 &lt;- lm(Ozone ~ Temp + Wind + Temp:Wind, air[1:50,]) mod_3 ## ## Call: ## lm(formula = Ozone ~ Temp + Wind + Temp:Wind, data = air[1:50, ## ]) ## ## Coefficients: ## (Intercept) Temp Wind Temp:Wind ## -317.8272 4.8036 15.9498 -0.2311 Podemos incluir interacciones para pares de variables que son importantes en la predicción, o que por conocimiento del dominio sabemos que son factibles. Conviene usar regularización si necesitamos incluir varias interacciones. 6.5 Categorización de variables En categorización de variable, intentamos hacer un ajuste local en distintas partes del espacio de entrada. La idea es contruir cubetas, particionando el rango de una variable dada, y ajustar entonces un modelo usando la variable dummy indicadora de cada cubeta. dat_wage &lt;- ISLR::Wage ggplot(dat_wage, aes(x=age, y=wage)) + geom_point() Cuando la relación entre entradas y salida no es lineal, podemos obtener menor sesgo en nuestros modelos usando esta técnica. En este ejemplo, escogimos edades de corte aproximadamente separadas por 10 años, por ejemplo: #cuantiles_age &lt;- quantile(dat_wage$age, probs=seq(0,1,0.2)) #cuantiles_age dat_wage &lt;- dat_wage %&gt;% mutate(age_cut = cut(age, c(18, 25, 35, 45, 55, 65, 80), include.lowest=TRUE)) head(dat_wage) ## year age maritl race education region ## 1 2006 18 1. Never Married 1. White 1. &lt; HS Grad 2. Middle Atlantic ## 2 2004 24 1. Never Married 1. White 4. College Grad 2. Middle Atlantic ## 3 2003 45 2. Married 1. White 3. Some College 2. Middle Atlantic ## 4 2003 43 2. Married 3. Asian 4. College Grad 2. Middle Atlantic ## 5 2005 50 4. Divorced 1. White 2. HS Grad 2. Middle Atlantic ## 6 2008 54 2. Married 1. White 4. College Grad 2. Middle Atlantic ## jobclass health health_ins logwage wage age_cut ## 1 1. Industrial 1. &lt;=Good 2. No 4.318063 75.04315 [18,25] ## 2 2. Information 2. &gt;=Very Good 2. No 4.255273 70.47602 [18,25] ## 3 1. Industrial 1. &lt;=Good 1. Yes 4.875061 130.98218 (35,45] ## 4 2. Information 2. &gt;=Very Good 1. Yes 5.041393 154.68529 (35,45] ## 5 2. Information 1. &lt;=Good 1. Yes 4.318063 75.04315 (45,55] ## 6 2. Information 2. &gt;=Very Good 1. Yes 4.845098 127.11574 (45,55] mod_age &lt;- lm(wage ~ age_cut, data=dat_wage) mod_age ## ## Call: ## lm(formula = wage ~ age_cut, data = dat_wage) ## ## Coefficients: ## (Intercept) age_cut(25,35] age_cut(35,45] age_cut(45,55] ## 76.28 27.88 42.79 41.34 ## age_cut(55,65] age_cut(65,80] ## 42.73 26.27 dat_wage$pred_wage &lt;- predict(mod_age) ggplot(dat_wage) + geom_point(aes(x=age, y=wage)) + geom_line(aes(x=age, y=pred_wage), colour = &#39;red&#39;, size=1.1) Podemos escoger los puntos de corte en lugares que son razonables para el problema (rangos en los es razonable modelar como una constante). También podemos hacer cortes automáticos usando percentiles de los datos: por ejemplo, cortar en cuatro usando los percentiles 25%, 0.5% y 0.75%. Con más datos es posible incrementar el número de cortes. Nótese que cuando hacemos estas categorizaciones estamos incrementando el número de parámetros a estimar del modelo (si hacemos tres cortes, por ejemplo, aumentamos en 3 el número de parámetros). Las categorizaciones de variables son útiles cuando sabemos que hay efectos no lineales de la variable subyacente (por ejemplo, edad o nivel socioeconómico), y las categorías son suficientemente chicas para que el modelo localmente constante sea razonable. Muchas veces los splines son mejores opciones: 6.6 Splines En estos ejemplos, también es posible incluir términos cuadráticos para modelar la relación, por ejemplo: dat_wage$age_2 &lt;- dat_wage$age^2 mod_age &lt;- lm(wage ~ age + age_2, data=dat_wage) mod_age ## ## Call: ## lm(formula = wage ~ age + age_2, data = dat_wage) ## ## Coefficients: ## (Intercept) age age_2 ## -10.42522 5.29403 -0.05301 dat_wage$pred_wage &lt;- predict(mod_age) ggplot(dat_wage) + geom_point(aes(x=age, y=wage)) + geom_line(aes(x=age, y=pred_wage), colour = &#39;red&#39;, size=1.1) Estas dos técnicas para hacer más flexible el modelo lineal tienen algunas deficiencias: Muchas veces usar potencias de variables de entrada es una mala idea, pues fácilmente podemos encontrar problemas numéricos (potencias altas pueden dar valores muy chicos o muy grandes). La categorización de variables numéricas puede resultar en predictores con discontinuidades, lo cual no siempre es deseable (interpretación). Una alternativa es usar splines, que son familias de funciones con buenas propiedades que nos permiten hacer expansiones del espacio de entradas. No las veremos con detalle, pero aquí hay unos ejemplos: Por ejemplo, podemos usar B-spines, que construyen “chipotes” en distintos rangos de la variable de entrada (es como hacer categorización, pero con funciones de respuesta suaves): library(splines2) age &lt;- seq(18,80, 0.2) splines_age &lt;- bSpline(age, knots = c(25, 35, 45, 55, 65), degree = 3) matplot(x = age, y = splines_age, type = &#39;l&#39;) Observación: estos splines son como una versión suave de categorización de variables numéricas. En particular, los splines de grado 0 son justamente funciones que categorizan variables: splines_age &lt;- bSpline(age, knots = c(25, 35, 45, 55, 65), degree = 0) matplot(splines_age, type=&#39;l&#39;) Por ejemplo: si expandimos el espacio de entradas con estos splines y corremos el modelo: dat_wage &lt;- ISLR::Wage splines_age &lt;- bSpline(dat_wage$age, knots = c(25, 35, 45, 65), degree = 3) %&gt;% data.frame colnames(splines_age) &lt;- paste0(&#39;spline_&#39;, 1:6) dat_wage &lt;- bind_cols(dat_wage, splines_age) dat_sp &lt;- dat_wage %&gt;% dplyr::select(wage, contains(&#39;spline&#39;)) head(dat_sp) ## wage spline_1 spline_2 spline_3 spline_4 spline_5 ## 1 75.04315 0.0000000 0.000000000 0.00000000 0.0000000 0.00000000 ## 2 70.47602 0.4555974 0.474260292 0.06722689 0.0000000 0.00000000 ## 3 130.98218 0.0000000 0.000000000 0.33333333 0.5925926 0.07407407 ## 4 154.68529 0.0000000 0.001481481 0.44018519 0.5204074 0.03792593 ## 5 75.04315 0.0000000 0.000000000 0.14062500 0.6272321 0.22704082 ## 6 127.11574 0.0000000 0.000000000 0.05545833 0.5406104 0.37417611 ## spline_6 ## 1 0.000000000 ## 2 0.000000000 ## 3 0.000000000 ## 4 0.000000000 ## 5 0.005102041 ## 6 0.029755102 mod_age &lt;- lm(wage ~. , data=dat_sp) mod_age ## ## Call: ## lm(formula = wage ~ ., data = dat_sp) ## ## Coefficients: ## (Intercept) spline_1 spline_2 spline_3 spline_4 ## 65.044 1.464 27.176 54.472 52.121 ## spline_5 spline_6 ## 58.230 31.771 dat_wage$pred_wage &lt;- predict(mod_age) ggplot(dat_wage) + geom_point(aes(x=age, y=wage)) + geom_line(aes(x=age, y=pred_wage), colour = &#39;red&#39;, size=1.1) O podemos usar i-splines (b-splines integrados), por ejemplo: splines_age &lt;- iSpline(age, knots = c(25, 35, 45, 65), degree = 2) matplot(splines_age, type=&#39;l&#39;) dat_wage &lt;- ISLR::Wage splines_age &lt;- iSpline(dat_wage$age, knots = c(25, 35, 45, 65), degree = 2) %&gt;% data.frame colnames(splines_age) &lt;- paste0(&#39;spline_&#39;, 1:6) dat_wage &lt;- bind_cols(dat_wage, splines_age) dat_sp &lt;- dat_wage %&gt;% dplyr::select(wage, contains(&#39;spline&#39;)) head(dat_sp) ## wage spline_1 spline_2 spline_3 spline_4 spline_5 spline_6 ## 1 75.04315 0.0000000 0.00000000 0.0000000 0.00000000 0.000000000 0 ## 2 70.47602 0.5414872 0.06722689 0.0000000 0.00000000 0.000000000 0 ## 3 130.98218 1.0000000 1.00000000 0.6666667 0.07407407 0.000000000 0 ## 4 154.68529 1.0000000 0.99851852 0.5583333 0.03792593 0.000000000 0 ## 5 75.04315 1.0000000 1.00000000 0.8593750 0.23214286 0.005102041 0 ## 6 127.11574 1.0000000 1.00000000 0.9445417 0.40393122 0.029755102 0 mod_age &lt;- lm(wage ~. , data=dat_sp) mod_age ## ## Call: ## lm(formula = wage ~ ., data = dat_sp) ## ## Coefficients: ## (Intercept) spline_1 spline_2 spline_3 spline_4 ## 64.643 28.331 26.442 -2.510 7.600 ## spline_5 spline_6 ## -31.903 -5.441 dat_wage$pred_wage &lt;- predict(mod_age) ggplot(dat_wage) + geom_point(aes(x=age, y=wage)) + geom_line(aes(x=age, y=pred_wage), colour = &#39;red&#39;, size=1.1) 6.6.1 ¿Cuándo usar estas técnicas? Estas técnicas pueden mejorar considerablemente nuestros modelos lineales, pero a veces puede ser difícil descubrir exactamente que transformaciones pueden ser útiles, y muchas veces requiere conocimiento experto del problema que enfrentamos. En general, Es mejor usar regularización al hacer este tipo de trabajo, para protegernos de varianza alta cuando incluimos varias entradas derivadas. Es buena idea probar incluir interacciones entre variables que tienen efectos grandes en la predicción, o interacciones que creemos son importantes en nuestro problema (por ejemplo, temperatura y viento en nuestro ejemplo de arriba, o existencia de estacionamiento y tráfico vehicular como en nuestro ejemplo de predicción de ventas de una tienda). Gráficas como la de arriba (entrada vs respuesta) pueden ayudarnos a decidir si conviene categorizar alguna variable o añadir un efecto no lineal. Este es un trabajo que no es tan fácil, pero para problema con relativamente pocas variables es factible. En situaciones con muchas variables de entrada y muchos datos, existen mejores opciones. "],
["redes-neuronales-parte-1.html", "Clase 7 Redes neuronales (parte 1) 7.1 Introducción a redes neuronales 7.2 Interacciones en redes neuronales 7.3 Cálculo en redes: feed-forward. Notación 7.4 Feed forward 7.5 Backpropagation: cálculo del gradiente 7.6 Ajuste de parámetros (introducción) Tarea (para 25 de septiembre) Tarea (2 de octubre)", " Clase 7 Redes neuronales (parte 1) 7.1 Introducción a redes neuronales En la parte anterior, vimos cómo hacer más flexibles los métodos de regresión: la idea es construir entradas derivadas a partir de las variables originales, e incluirlas en el modelo de regresión. Este enfoque es bueno cuando tenemos relativamente pocas variables originales de entrada, y tenemos una idea de qué variables derivadas es buena idea incluir (por ejemplo, splines para una variable como edad, interacciones para variables importantes, etc). Sin embargo, si hay una gran cantidad de entradas, esta técnica puede ser prohibitiva en términos de cálculo y trabajo manual. Por ejemplo, si tenemos unas 100 entradas numéricas, al crear todas las interacciones \\(x_i x_j\\) y los cuadrados \\(x_i^2\\) terminamos con unas 5150 variables. Para el problema de dígitos (256 entradas o pixeles) terminaríamos con unas 32 mil entradas adicionales. Aún cuando es posible regularizar, en estos casos suena más conveniente construir entradas derivadas a partir de los datos. Para hacer esto, consideramos entradas \\(X_1, . . . , X_p\\), y supongamos que tenemos un problema de clasificación binaria, con \\(G = 1\\) o \\(G = 0\\). Aunque hay muchas maneras de construir entradas derivadas, una manera simple sería construir \\(m\\) nuevas entradas mediante: \\[a_k = h \\left ( \\theta_{k,0} + \\sum_{j=1}^p \\theta_{k,j}x_j \\right)\\] para \\(k=1,\\ldots, m\\), donde \\(h\\) es la función logística, y las \\(\\theta\\) son parámetros que seleccionaremos más tarde. Modelamos ahora la probabilidad de clase 1 con regresión logística -pero en lugar de usar las entradas originales X usamos las entradas derivadas \\(a_1, . . . , a_m\\): \\[p_1(x) = h \\left ( \\beta_0 + \\sum_{j=1}^m \\beta_ja_j \\right)\\] Podemos representar este esquema con una red dirigida (\\(m=3\\) variables derivadas): Observaciones: ¿Por qué usar \\(h\\) para las entradas derivadas \\(a_k\\)? En primer lugar, nótese que si no transformamos con alguna función no lineal \\(h\\), el modelo final \\(p_1\\) para la probabilidad condicional es el mismo que el de regresión logística (combinaciones lineales de combinaciones lineales son combinaciones lineales). Sin embargo, al transformar con \\(h\\), las \\(x_j\\) contribuyen de manera no lineal a las entradas derivadas. Las variables \\(a_k\\) que se pueden obtener son similares (para una variable de entrada) a los I-splines que vimos en la parte anterijor. Es posible demostrar que si se crean suficientes entradas derivadas (\\(m\\) es suficientemente grande), entonces la función \\(p_1(x)\\) puede aproximar cualquier función continua. La función \\(h\\) (que se llama función de activación no es especial: funciones continuas con forma similar a la sigmoide (logística) pueden usarse también (por ejemplo, arcotangente, o lineal rectificada). La idea es que cualquier función se puede aproximar mediante superposición de funciones tipo sigmoide (ver por ejemplo Cybenko 1989, Approximation by Superpositions of a Sigmoidal Function). ¿Cómo construyen entradas las redes neuronales? Comencemos por un ejemplo simple de clasificación binaria con una sola entrada \\(x\\). Supondremos que el modelo verdadero está dado por: h &lt;- function(x){ 1/(1 + exp(-x)) # es lo mismo que exp(x)/(1 + exp(x)) } x &lt;- seq(-2, 2, 0.1) p &lt;- h(2 - 3 * x^2) #probabilidad condicional de clase 1 (vs. 0) set.seed(2805721) x_1 &lt;- runif(30, -2, 2) g_1 &lt;- rbinom(30, 1, h(2 - 3 * x_1^2)) datos &lt;- data.frame(x_1, g_1) dat_p &lt;- data.frame(x, p) g &lt;- qplot(x, p, geom=&#39;line&#39;) g + geom_point(data = datos, aes(x = x_1, y = g_1), colour = &#39;red&#39;) donde adicionalmente graficamos 30 datos simulados. Recordamos que queremos ajustar la curva roja, que da la probabilidad condicional de clase. Podríamos ajustar un modelo de regresión logística expandiendo el espacio de entradas agregando \\(x^2\\), y obtendríamos un ajuste razonable. La idea aquí es que podemos crear entradas derivadas de forma automática. Suponamos entonces que pensamos crear dos entradas \\(a_1\\) y \\(a_2\\), funciones de \\(x_1\\), y luego predecir \\(g.1\\), la clase, en función de estas dos entradas. Por ejemplo, podríamos tomar: donde hacemos una regresión logística para predecir \\(G\\) mediante \\[p_1(a) = h(\\beta_0 + \\beta_1a_1+\\beta_2 a_2),\\] \\(a_1\\) y \\(a_2\\) están dadas por \\[a_1(x)=h(\\beta_{1,0} + \\beta_{1,1} x_1),\\] \\[a_2(x)=h(\\beta_{2,0} + \\beta_{2,1} x_1).\\] Por ejemplo, podríamos tomar a_1 &lt;- h( 1 + 2*x) # 2(x+1/2) a_2 &lt;- h(-1 + 2*x) # 2(x-1/2) # una es una versión desplazada de otra. Las funciones \\(a_1\\) y \\(a_2\\) dependen de \\(x\\) de la siguiente forma: dat_a &lt;- data.frame(x = x, a_1 = a_1, a_2 = a_2) dat_a_2 &lt;- dat_a %&gt;% gather(variable, valor, a_1:a_2) ggplot(dat_a_2, aes(x=x, y=valor, colour=variable, group=variable)) + geom_line() Si las escalamos y sumamos, obtenemos dat_a &lt;- data.frame(x=x, a_1=-4+12*a_1, a_2=-12*a_2, suma=-4+12*a_1-12*a_2) dat_a_2 &lt;- dat_a %&gt;% gather(variable, valor, a_1:suma) ggplot(dat_a_2, aes(x=x, y=valor, colour=variable, group=variable)) + geom_line() y finalmente, aplicando \\(h\\): dat_2 &lt;- data.frame(x, p2=h(-4 + 12*a_1 - 12*a_2)) ggplot(dat_2, aes(x=x, y=p2)) + geom_line()+ geom_line(data=dat_p, aes(x=x,y=p), col=&#39;red&#39;) +ylim(c(0,1))+ geom_point(data = datos, aes(x=x_1,y=g_1)) que da un ajuste razonable. Este es un ejemplo de cómo la mezcla de dos funciones logísticas puede replicar esta función con forma de chipote. ¿Cómo ajustar los parámetros? Para encontrar los mejores parámetros, minimizamos la devianza sobre los parámetros \\(\\beta_0,\\beta_1,\\beta_{1,0},\\beta_{1,1}, \\beta_{2,0},\\beta_{2,1}\\). Veremos más adelante que conviene hacer esto usando descenso o en gradiente o descenso en gradiente estocástico, pero por el momento usamos la función optim de R para minimizar la devianza. En primer lugar, creamos una función que para todas las entradas calcula los valores de salida. En esta función hacemos feed-forward de las entradas a través de la red para calcular la salida ## esta función calcula los valores de cada nodo en toda la red, ## para cada entrada feed_fow &lt;- function(beta, x){ a_1 &lt;- h(beta[1] + beta[2]*x) # calcula variable 1 de capa oculta a_2 &lt;- h(beta[3] + beta[4]*x) # calcula variable 2 de capa oculta p &lt;- h(beta[5]+beta[6]*a_1 + beta[7]*a_2) # calcula capa de salida p } Nótese que simplemente seguimos el diagrama mostrado arriba para hacer los cálculos, combinando linealmente las entradas en cada capa. Ahora definimos una función para calcular la devianza. Conviene crear una función que crea funciones, para obtener una función que sólo se evalúa en los parámetros para cada conjunto de datos de entrenamiento fijos: devianza_fun &lt;- function(x, y){ # esta función es una fábrica de funciones devianza &lt;- function(beta){ p &lt;- feed_fow(beta, x) - 2 * mean(y*log(p) + (1-y)*log(1-p)) } devianza } Por ejemplo: dev &lt;- devianza_fun(x_1, g_1) # crea función dev ## ahora dev toma solamente los 7 parámetros beta: dev(c(0,0,0,0,0,0,0)) ## [1] 1.386294 Finalmente, optimizamos la devianza. Para esto usaremos la función optim de R: set.seed(5) salida &lt;- optim(rnorm(7), dev, method=&#39;BFGS&#39;) # inicializar al azar punto inicial salida ## $par ## [1] -24.8192568 23.0201169 -8.4364869 -6.7633494 0.9849461 -14.0157655 ## [7] -14.3394673 ## ## $value ## [1] 0.654347 ## ## $counts ## function gradient ## 103 100 ## ## $convergence ## [1] 1 ## ## $message ## NULL beta &lt;- salida$par Y ahora podemos graficar con el vector \\(\\beta\\) encontrado: ## hacer feed forward con beta encontrados p_2 &lt;- feed_fow(beta, x) dat_2 &lt;- data.frame(x, p_2 = p_2) ggplot(dat_2, aes(x = x, y = p_2)) + geom_line()+ geom_line(data = dat_p, aes(x = x, y = p), col=&#39;red&#39;) +ylim(c(0,1))+ geom_point(data = datos, aes(x = x_1, y = g_1)) Los coeficientes estimados, que en este caso muchas veces se llaman pesos, son: beta ## [1] -24.8192568 23.0201169 -8.4364869 -6.7633494 0.9849461 -14.0157655 ## [7] -14.3394673 que parecen ser muy grandes. Igualmente, de la figura vemos que el ajuste no parece ser muy estable (esto se puede confirmar corriendo con distintos conjuntos de entrenamiento). Podemos entonces regularizar ligeramente la devianza para resolver este problema. En primer lugar, definimos la devianza regularizada (ridge): devianza_reg &lt;- function(x, y, lambda){ # esta función es una fábrica de funciones devianza &lt;- function(beta){ p &lt;- feed_fow(beta, x) # en esta regularizacion quitamos sesgos, pero puede hacerse también con sesgos. - 2 * mean(y*log(p) + (1-y)*log(1-p)) + lambda*sum(beta[-c(1,3,5)]^2) } devianza } dev_r &lt;- devianza_reg(x_1, g_1, 0.001) # crea función dev set.seed(5) salida &lt;- optim(rnorm(7), dev_r, method=&#39;BFGS&#39;) # inicializar al azar punto inicial salida ## $par ## [1] -4.826652 4.107146 -4.845864 -4.561488 1.067216 -5.236453 -5.195981 ## ## $value ## [1] 0.8322745 ## ## $counts ## function gradient ## 102 100 ## ## $convergence ## [1] 1 ## ## $message ## NULL beta &lt;- salida$par dev(beta) ## [1] 0.74018 p_2 &lt;- feed_fow(beta, x) dat_2 &lt;- data.frame(x, p_2 = p_2) ggplot(dat_2, aes(x = x, y = p_2)) + geom_line()+ geom_line(data = dat_p, aes(x = x, y = p), col=&#39;red&#39;) +ylim(c(0,1))+ geom_point(data = datos, aes(x = x_1, y = g_1)) y obtenemos un ajuste mucho más estable. Podemos también usar la función nnet del paquete nnet. Ojo: en nnet, el error es la devianza no está normalizada por número de casos y dividida entre dos: library(nnet) set.seed(12) nn &lt;- nnet(g_1 ~ x_1, data=datos, size = 2, decay=0.0, entropy = T) ## # weights: 7 ## initial value 19.318858 ## iter 10 value 11.967705 ## iter 20 value 10.251964 ## iter 30 value 9.647707 ## iter 40 value 9.573030 ## iter 50 value 9.569389 ## iter 60 value 9.555125 ## iter 70 value 9.546210 ## iter 80 value 9.544512 ## iter 90 value 9.539825 ## iter 100 value 9.535977 ## final value 9.535977 ## stopped after 100 iterations nn$wts ## [1] -51.274012 48.789640 8.764849 6.219901 -29.155181 -24.998108 ## [7] 30.125349 nn$value ## [1] 9.535977 2*nn$value/30 ## [1] 0.6357318 dev(nn$wts) ## [1] 0.6357318 qplot(x, predict(nn, newdata=data.frame(x_1 = x)), geom=&#39;line&#39;) 7.1.0.1 Ejercicio Un ejemplo más complejo. Utiliza los siguientes datos, y agrega si es necesario variables derivadas \\(a_3,a_4\\) en la capa oculta. h &lt;- function(x){ exp(x)/(1+exp(x)) } x &lt;- seq(-2,2,0.05) p &lt;- h(3 + x- 3*x^2 + 3*cos(4*x)) set.seed(280572) x.2 &lt;- runif(300, -2, 2) g.2 &lt;- rbinom(300, 1, h(3 + x.2- 3*x.2^2 + 3*cos(4*x.2))) datos &lt;- data.frame(x.2,g.2) dat.p &lt;- data.frame(x,p) g &lt;- qplot(x,p, geom=&#39;line&#39;, col=&#39;red&#39;) g + geom_jitter(data = datos, aes(x=x.2,y=g.2), col =&#39;black&#39;, position =position_jitter(height=0.05), alpha=0.4) 7.2 Interacciones en redes neuronales Es posible capturar interacciones con redes neuronales. Consideremos el siguiente ejemplo simple: p &lt;- function(x1, x2){ h(-5 + 10*x1 + 10*x2 - 30*x1*x2) } dat &lt;- expand.grid(x1 = seq(0, 1, 0.05), x2 = seq(0, 1, 0.05)) dat &lt;- dat %&gt;% mutate(p = p(x1, x2)) ggplot(dat, aes(x=x1, y=x2)) + geom_tile(aes(fill=p)) Esta función puede entenderse como un o exclusivo: la probabilidad es alta sólo cuando x1 y x2 tienen valores opuestos (x1 grande pero x2 chica y viceversa). No es posible modelar esta función mediante el modelo logístico (sin interacciones). Sin embargo, podemos incluir la interacción en el modelo logístico o intentar usar una red neuronal. Primero simulamos unos datos y probamos el modelo logístico con y sin interacciones: set.seed(322) n &lt;- 500 dat_ent &lt;- data_frame(x1=runif(n,0,1), x2 = runif(n, 0, 1)) %&gt;% mutate(p = p(x1, x2)) %&gt;% mutate(y = rbinom(n, 1, p)) mod_1 &lt;- glm(y ~ x1 + x2, data = dat_ent, family = &#39;binomial&#39;) mod_1 ## ## Call: glm(formula = y ~ x1 + x2, family = &quot;binomial&quot;, data = dat_ent) ## ## Coefficients: ## (Intercept) x1 x2 ## -0.01011 -1.47942 -1.19196 ## ## Degrees of Freedom: 499 Total (i.e. Null); 497 Residual ## Null Deviance: 529.4 ## Residual Deviance: 504.5 AIC: 510.5 table(predict(mod_1) &gt; 0.5, dat_ent$y) ## ## 0 1 ## FALSE 389 111 mod_2 &lt;- glm(y ~ x1 + x2 + x1:x2, data = dat_ent, family = &#39;binomial&#39;) mod_2 ## ## Call: glm(formula = y ~ x1 + x2 + x1:x2, family = &quot;binomial&quot;, data = dat_ent) ## ## Coefficients: ## (Intercept) x1 x2 x1:x2 ## -4.726 9.641 9.831 -32.466 ## ## Degrees of Freedom: 499 Total (i.e. Null); 496 Residual ## Null Deviance: 529.4 ## Residual Deviance: 305.6 AIC: 313.6 table(predict(mod_2) &gt; 0.5, dat_ent$y) ## ## 0 1 ## FALSE 374 60 ## TRUE 15 51 Observese la gran diferencia de devianza entre los dos modelos (en este caso, el sobreajuste no es un problema). Ahora consideramos qué red neuronal puede ser apropiada set.seed(11) nn &lt;- nnet(y ~ x1 + x2, data = dat_ent, size = 3, decay = 0.001, entropy = T, maxit = 500) ## # weights: 13 ## initial value 294.186925 ## iter 10 value 233.560013 ## iter 20 value 195.096851 ## iter 30 value 190.466423 ## iter 40 value 184.454612 ## iter 50 value 170.767082 ## iter 60 value 156.347417 ## iter 70 value 153.521658 ## iter 80 value 153.069566 ## iter 90 value 152.852374 ## iter 100 value 152.835812 ## iter 110 value 152.826924 ## iter 120 value 152.825819 ## final value 152.825815 ## converged #primera capa matrix(round(nn$wts[1:9], 1), 3,3, byrow=T) ## [,1] [,2] [,3] ## [1,] -2.2 3.0 -2.4 ## [2,] -8.2 5.9 8.7 ## [3,] -2.7 -1.6 3.6 #segunda capa round(nn$wts[10:13], 1) ## [1] -5.7 15.1 -8.6 19.8 #2*nn$value El cálculo de esta red es: feed_fow &lt;- function(beta, x){ a_1 &lt;- h(beta[1] + beta[2]*x[1] + beta[3]*x[2]) a_2 &lt;- h(beta[4] + beta[5]*x[1] + beta[6]*x[2]) a_3 &lt;- h(beta[7] + beta[8]*x[1] + beta[9]*x[2]) p &lt;- h(beta[10]+beta[11]*a_1 + beta[12]*a_2 + beta[13]*a_3) # calcula capa de salida p } Y vemos que esta red captura la interacción: feed_fow(nn$wts, c(0,0)) ## [1] 0.04946031 feed_fow(nn$wts, c(0,1)) ## [1] 0.9560235 feed_fow(nn$wts, c(1,0)) ## [1] 0.9830594 feed_fow(nn$wts, c(1,1)) ## [1] 0.004197137 dat &lt;- dat %&gt;% rowwise %&gt;% mutate(p_red = feed_fow(nn$wts, c(x1, x2))) ggplot(dat, aes(x=x1, y=x2)) + geom_tile(aes(fill=p_red)) Observación: ¿cómo funciona esta red? Consideremos la capa intermedia dat_entrada &lt;- data_frame(x_1=c(0,0,1,1), x_2=c(0,1,0,1)) a_1 &lt;- dat_entrada %&gt;% rowwise() %&gt;% mutate(a_1= h(sum(nn$wts[1:3]*c(1,x_1,x_2) ))) a_2 &lt;- dat_entrada %&gt;% rowwise() %&gt;% mutate(a_2= h(sum(nn$wts[4:6]*c(1,x_1,x_2) ))) a_3 &lt;- dat_entrada %&gt;% rowwise() %&gt;% mutate(a_3= h(sum(nn$wts[7:9]*c(1,x_1,x_2) ))) capa_intermedia &lt;- left_join(a_1, a_2) %&gt;% left_join(a_3) ## Joining, by = c(&quot;x_1&quot;, &quot;x_2&quot;) ## Joining, by = c(&quot;x_1&quot;, &quot;x_2&quot;) a_1 ## Source: local data frame [4 x 3] ## Groups: &lt;by row&gt; ## ## # A tibble: 4 x 3 ## x_1 x_2 a_1 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0.10233895 ## 2 0 1 0.01014846 ## 3 1 0 0.68556319 ## 4 1 1 0.16392998 a_3 ## Source: local data frame [4 x 3] ## Groups: &lt;by row&gt; ## ## # A tibble: 4 x 3 ## x_1 x_2 a_3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0.06285298 ## 2 0 1 0.70876776 ## 3 1 0 0.01302357 ## 4 1 1 0.32378386 a_2 ## Source: local data frame [4 x 3] ## Groups: &lt;by row&gt; ## ## # A tibble: 4 x 3 ## x_1 x_2 a_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0.0002839063 ## 2 0 1 0.6213605990 ## 3 1 0 0.0959812149 ## 4 1 1 0.9983727125 Y observamos que las unidades \\(a_1\\) y \\(a_3\\) tienen valor alto cuando las variables \\(x_1\\) y \\(x_2\\), correspondientemente, tienen valores altos. La unidad \\(a_2\\) responde cuando tanto como \\(x_1\\)y \\(x_2\\) tienen valores altos. Para la capa final, tenemos que: nn$wts[10:13] ## [1] -5.747250 15.138708 -8.628917 19.801144 capa_final &lt;- capa_intermedia %&gt;% rowwise() %&gt;% mutate(p= h(sum(nn$wts[10:13]*c(1,a_1,a_2,a_3) ))) %&gt;% mutate(p=round(p,2)) capa_final ## Source: local data frame [4 x 6] ## Groups: &lt;by row&gt; ## ## # A tibble: 4 x 6 ## x_1 x_2 a_1 a_2 a_3 p ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0.10233895 0.0002839063 0.06285298 0.05 ## 2 0 1 0.01014846 0.6213605990 0.70876776 0.96 ## 3 1 0 0.68556319 0.0959812149 0.01302357 0.98 ## 4 1 1 0.16392998 0.9983727125 0.32378386 0.00 7.3 Cálculo en redes: feed-forward. Ahora generalizamos lo que vimos arriba para definir la arquitectura básica de redes neuronales y cómo se hacen cálculos en las redes. A las variables originales les llamamos capa de entrada de la red, y a la variable de salida capa de salida. Puede haber más de una capa intermedia. A estas les llamamos capas ocultas. Cuando todas las conexiones posibles de cada capa a la siguiente están presente, decimos que la red es completamente conexa. Como vimos en el ejemplo de arriba, para hacer cálculos en la red empezamos con la primera capa, hacemos combinaciones lineales y aplicamos nuestra función no lineal \\(h\\). Una vez que calculamos la segunda capa, podemos calcular la siguiente de la misma forma: combinaciones lineales y aplicación de \\(h\\). Y así sucesivamente hasta que llegamos a la capa final. Notación Sea \\(L\\) el número total de capas. En primer lugar, para un cierto caso de entrada \\(x = (x_1,x_2,\\ldots, x_p)\\), denotamos por: \\(a^{(l)}_j\\) el valor que toma la unidad \\(j\\) de la capa \\(l\\), para \\(j=0,1,\\ldots, n_{l}\\), donde \\(n_l\\) es el número de unidades de la capa \\(l\\). Ponemos \\(a^{(l)}_0=1\\) para lidiar con los sesgos. En particular, ponemos \\(a^{(1)}_j = x_j\\), que son los valores de las entradas (primera capa) Para clasificación binaria, la última capa solo tiene un elemento, que es \\(p_1 = a^{(L)}\\). Para un problema de clasificación en \\(K&gt;2\\) clases, tenemos que la última capa es de tamaño \\(K\\): \\(p_1 = a^{(L)}_1, p_2 = a^{(L)}_2,\\ldots, p_K = a^{(L)}_K\\) Adicionalmente, escribimos \\(\\theta_{i,k}^{(l)}=\\) es el peso de entrada \\(a_{k}^{(l-1)}\\) de capa \\(l-1\\) en la entrada \\(a_{i}^{(l)}\\) de la capa \\(l\\). Los sesgos están dados por \\[\\theta_{i,0}^{(l)}\\] Ejemplo En nuestro ejemplo, tenemos que en la capa \\(l=3\\) hay dos unidades. Así que podemos calcular los valores \\(a^{(3)}_1\\) y \\(a^{(3)}_2\\). Están dados por \\[a_1^{(3)} = h(\\theta_{1,0}^{(3)} + \\theta_{1,1}^{(3)} a_1^{(2)}+ \\theta_{1,2}^{(3)}a_2^{(2)}+ \\theta_{1,3}^{(3)} a_3^{(2)})\\] \\[a_2^{(3)} = h(\\theta_{2,0}^{(3)} + \\theta_{2,1}^{(3)} a_1^{(2)}+ \\theta_{2,2}^{(3)}a_2^{(2)}+ \\theta_{2,3}^{(3)} a_3^{(2)})\\] Como se ilustra en la siguiente gráfica: Para visualizar las ordenadas (que también se llaman sesgos en este contexto), ponemos \\(a_0^2=1\\). Ejemplo Consideremos propagar con los siguientes pesos para capa 3 y valores de la capa 2 (en gris están los sesgos): Que en nuestra notación escribimos como \\[a^{(2)}_0 = 1, a^{(2)}_1 = -2, a^{(2)}_2 = 5\\] y los pesos son, para la primera unidad: \\[\\theta^{(3)}_{1,0} = 3, \\,\\,\\, \\theta^{(3)}_{1,1} = 1,\\,\\,\\,\\theta^{(3)}_{1,2} = -1\\] y para la segunda unidad \\[\\theta^{(3)}_{2,0} = 1, \\,\\,\\, \\theta^{(3)}_{2,1} = 2,\\,\\,\\,\\theta^{(3)}_{2,2} = 0.5\\] Y ahora queremos calcular los valores que toman las unidades de la capa 3, que son \\(a^{(3)}_1\\) y \\(a^{(3)}_2\\)$ Para hacer feed forward a la siguiente capa, hacemos entonces \\[a^{(3)}_1 = h(3 + a^{(2)}_1 - a^{(2)}_2),\\] \\[a^{(3)}_2 = h(1 + 2a^{(2)}_1 + 0.5a^{(2)}_2),\\] Ponemos los pesos y valores de la capa 2 (incluyendo sesgo): a_2 &lt;- c(1,-2,5) # ponemos un 1 al principio para el sesgo theta_2_1 = c(3,1,-1) theta_2_2 = c(1,2,0.5) y calculamos a_3 &lt;- c(1, h(sum(theta_2_1*a_2)),h(sum(theta_2_2*a_2))) # ponemos un 1 al principio a_3 ## [1] 1.00000000 0.01798621 0.37754067 7.4 Feed forward Para calcular los valores de salida de una red a partir de pesos y datos de entrada, usamos el algoritmo feed-forward, calculando capa por capa. Cálculo en redes: Feed-forward Para la primera capa, escribimos las variables de entrada: \\[a^{(1)}_j = x_j, j=1\\ldots,n_1\\] Para la primera capa oculta, o la segunda capa \\[a^{(2)}_j = h\\left( \\theta_{j,0}^{(2)}+ \\sum_{k=1}^{n_1} \\theta_{j,k}^{(2)} a^{(1)}_k \\right), j=1\\ldots,n_2\\] para la \\(l\\)-ésima capa: \\[a^{(l)}_j = h\\left( \\theta_{j,0}^{(l)}+ \\sum_{k=1}^{n_{l-1}} \\theta_{j,k}^{(l)} a^{(l-1)}_k \\right), j=1\\ldots,n_{l}\\] y así sucesivamente. Para la capa final o capa de salida (para problema binario), suponiendo que tenemos \\(L\\) capas (\\(L-2\\) capas ocultas): \\[p_1 = h\\left( \\theta_{1,0}^{(L)}+ \\sum_{k=1}^{n_{L-1}} \\theta_{1,k}^{(L)} a^{(L-1)}_k \\right).\\] Nótese que entonces: Cada capa se caracteriza por el conjunto de parámetros \\(\\Theta^{(l)}\\), que es una matriz de \\(n_l\\times n_{l-1}\\). La red completa entonces se caracteriza por: La estructura elegida (número de capas ocultas y número de nodos en cada capa oculta). Las matrices de pesos en cada capa \\(\\Theta^{(1)},\\Theta^{(2)},\\ldots, \\Theta^{(L)}\\) Adicionalmente, escribimos en forma vectorial: \\[a^{(l)} = (a^{(l)}_0, a^{(l)}_1, a^{(l)}_2, \\ldots, a^{(l)}_{n_l})^t\\] Para calcular la salidas, igual que hicimos, antes, propagaremos hacia adelante los valores de las variables de entrada usando los pesos. Agregando entradas adicionales en cada capa \\(a_0^{(l)}\\), \\(l=1,2,\\ldots, L-1\\), donde \\(a_0^{l}=1\\), y agregando a \\(\\Theta^{(l)}\\) una columna con las ordenadas al origen (o sesgos) podemos escribir: Feed-forward(matricial) Capa 1 (vector de entradas) \\[ a^{(1)} = x\\] Capa 2 \\[ a^{(2)} = h(\\Theta^{(1)}a^{(1)})\\] Capa \\(l\\) (oculta) \\[ a^{(l)} = h(\\Theta^{(l)}a^{(l-1)})\\] Capa de salida: \\[a^{(L)}= p = h(\\Theta^{(L)}a^{(L-1)})\\] donde \\(h\\) se aplica componente a componente sobre los vectores correspondientes. Nótese que feed-foward consiste principalmente de mutliplicaciones de matrices con algunas aplicaciones de \\(h\\) 7.5 Backpropagation: cálculo del gradiente Más adelante, para ajustar los pesos y sesgos de las redes (valores \\(\\theta\\)), utilizaremos descenso en gradiente y otros algoritmos derivados del gradiente (descenso estocástico). En esta parte entonces veremos cómo calcular estos gradientes con el algoritmo de back-propagation, que es una aplicación de la regla de la cadena para derivar. Back-propagation resulta en una fórmula recursiva donde propagamos errores de la red como gradientes desde el final de red (capa de salida) hasta el principio, capa por capa. Recordamos la devianza (con regularización ridge) es \\[D = -\\frac{2}{n}\\sum_{i=1}^n y_i\\log(p_1(x_i)) +(1-y_i)\\log(1-p_1(x_i)) + \\lambda \\sum_{l=2}^{L} \\sum_{k=1}^{n_{l-1}} \\sum_{j=1}^{n_l}(\\theta_{j,k}^{(l)})^2.\\] Queremos entonces calcular las derivadas de la devianza con respecto a cada parámetro \\(\\theta_{j,k}^{(l)}\\). Esto nos proporciona el gradiente para nuestro algoritmo de descenso. Consideramos aquí el problema de clasificación binaria con devianza como función de pérdida, y sin regularización. La parte de la parcial que corresponde al término de regularización es fácil de agregar al final. Recordamos también nuestra notación para la función logística (o sigmoide): \\[h(z)=\\frac{1}{1+e^{-z}}.\\] Necesitaremos su derivada, que está dada por (cálculala): \\[h&#39;(z) = h(z)(1-h(z))\\] 7.5.1 Cálculo para un caso de entrenamiento Como hicimos en regresión logística, primero simplificamos el problema y consideramos calcular las parciales para un solo caso de entrenamiento \\((x,y)\\): \\[ D= -\\left ( y\\log (p_1(x)) + (1-y)\\log (1-p_1(x))\\right) . \\] Después sumaremos sobre toda la muestra de entrenamiento. Entonces queremos calcular \\[\\frac{\\partial D}{\\partial \\theta_{j,k}^{(l)}}\\] Y escribiremos, con la notación de arriba, \\[a^{(l)}_j = h(z^{(l)}_j)\\] donde \\[z^{(l)} = \\Theta^{l} a^{(l-1)},\\] que coordenada a coordenada se escribe como \\[z^{(l)}_j = \\sum_{k=0}^{n_{l-1}} \\theta_{j,k}^{(l)} a^{(l-1)}_k\\] Paso 1: Derivar respecto a capa \\(l\\) Como los valores de cada capa determinan los valores de salida y la devianza, podemos escribir (recordemos que \\(a_0^{(l)}=1\\) es constante): \\[D=D(a_0^{(l)},a_1^{(l)},a_2^{(l)},\\ldots, a_{n_{l}}^{(l)})=D(a_1^{(l)},a_2^{(l)},\\ldots, a_{n_{l}}^{(l)})\\] Así que por la regla de la cadena para varias variables: \\[\\frac{\\partial D}{\\partial \\theta_{j,k}^{(l)}} = \\sum_{t=1}^{n_{l}} \\frac{\\partial D}{\\partial a_t^{l}}\\frac{\\partial a_t^{(l)}} {\\partial \\theta_{j,k}^{(l)} }\\] Pero si vemos dónde aparece \\(\\theta_{j,k}^{(l)}\\) en la gráfica de la red: \\[ \\cdots a^{(l-1)}_k \\xrightarrow{\\theta_{j,k}^{(l)}} a^{(l)}_j \\cdots \\rightarrow D\\] Entonces podemos concluir que \\(\\frac{\\partial a_t^{(l)}}{\\partial \\theta_{j,k}^{(l)}} =0\\) cuando \\(t\\neq j\\) (pues no dependen de \\(\\theta_{j,k}^{(l)}\\)), de lo que se concluye que, para toda \\(j=1,2,\\ldots, n_{l+1}, k=0,1,\\ldots, n_{l}\\) \\[\\begin{equation} \\frac{\\partial D}{\\partial \\theta_{j,k}^{(l)}} = \\frac{\\partial D}{\\partial a_j^{(l)}}\\frac{\\partial a_j^{(l)}}{\\partial \\theta_{j,k}^{(l)} } , \\tag{7.1} \\end{equation}\\] y como \\[a_j^{(l)} = h(z_j^{(l)}) = h\\left (\\sum_{k=0}^{n_{l-1}} \\theta_{j,k}^{(l)} a^{(l-1)}_k \\right )\\] Tenemos por la regla de la cadena que \\[\\begin{equation} \\frac{\\partial a_j^{(l)}}{\\partial \\theta_{j,k}^{(l)} } = h&#39;(z_j^{(l)})a_k^{(l-1)}. \\end{equation}\\] Esta última expresión podemos calcularla pues sólo requiere la derivada de \\(h\\) y los valores de los nodos obtenidos en la pasada de feed-forward. Paso 2: Derivar con respecto a capa \\(l+1\\) Así que sólo nos queda calcular las parciales (\\(j = 1,\\ldots, n_l\\)) \\[\\frac{\\partial D}{\\partial a_j^{(l)}}\\] Para obtener una fórmula recursiva para esta cantidad (hacia atrás), aplicamos otra vez regla de la cadena, pero con respecto a la capa \\(l+1\\) (ojo: queremos obtener una fórmula recursiva!): \\[\\frac{\\partial D}{\\partial a_j^{(l)}}= \\sum_{s=1}^{n_l} \\frac{\\partial D}{\\partial a_s^{(l+1)}}\\frac{\\partial a_s^{(l+1)}}{\\partial a_j^{(l)}},\\] que se puede entender a partir de este diagrama: Nótese que la suma empieza en \\(s=1\\), no en \\(s=0\\), pues \\(a_0^{(l+1)}\\) no depende de \\(a_k^{(l)}\\). En este caso los elementos de la suma no se anulan necesariamente. Primero consideramos la derivada de: \\[\\frac{\\partial a_s^{(l+1)}}{\\partial a_j^{(l)}}=h&#39;(z_s^{(l+1)})\\theta_{s,j}^{(l+1)},\\] de modo que \\[\\frac{\\partial D}{\\partial a_j^{(l)}}= \\sum_{s=1}^{n_l} \\frac{\\partial D}{\\partial a_s^{(l+1)}} h&#39;(z_s^{(l+1)})\\theta_{s,j}^{(l+1)}.\\] Denotaremos \\[\\delta_s^{ (l+1)}=\\frac{\\partial D}{\\partial a_s^{(l+1)}} h&#39;(z_s^{(l+1)})\\] de manera que la ecuación anterior es \\[\\begin{equation} \\frac{\\partial D}{\\partial a_j^{(l)}} = \\sum_{s=1}^{n_{l+1}} \\delta_s^{(l+1)}\\theta_{s,j}^{(l+1)}. \\tag{7.2} \\end{equation}\\] Observación Nótese que \\(\\delta_s^{(l)} =\\frac{\\partial D}{\\partial z_s^{(l+1)}}\\), que nos dice a dónde tenemos que mover la entrada derivada \\(z_s^{(l+1)}\\) para reducir el error \\(D\\). Como \\(z_s^{(l+1)}\\) es una entrada derivada que depende de parámetros \\(\\theta\\), esta cantidad también nos ayudará a entender cómo debemos cambiar los parámetros \\(\\theta\\) para disminuir el error. Paso 3: Construir fórmula recursiva para \\(\\delta\\) Lo único que nos falta calcular entonces son las \\(\\delta_s^{(l)}\\). Tenemos que si \\(l=2,\\ldots,L-1\\), entonces podemos escribir (usando (7.2)) como fórmula recursiva: \\[\\begin{equation} \\delta_j^{(l)} = \\frac{\\partial D}{\\partial a_j^{l}} h&#39;(z_j^{(l)}) = \\left (\\sum_{s=1}^{n_l} \\delta_s^{(l+1)} \\theta_{s,j}^{(l+1)}\\right ) h&#39;(z_j^{(l)}), \\tag{7.3} \\end{equation}\\] para \\(j=1,2,\\ldots, n_{l}\\). y para la última capa, tenemos que (demostrar!) \\[\\delta_1^{(L)}=p - y.\\] Finalmente, usando (7.1), obtenemos \\[\\frac{\\partial D}{\\partial \\theta_{j,k}^{(l)}} = \\delta_j^{(l)}a_k^{(l-1)},\\] y con esto ya podemos hacer backpropagation para calcular el gradiente sobre cada caso de entrenamiento, y solo resta acumular para obtener el gradiente sobre la muestra de entrenamiento. Muchas veces es útil escribir una versión vectorizada (importante para implementar): Paso 4: Versión matricial Ahora podemos escribir estas ecuaciones en forma vectorial. En primer lugar, \\[\\delta^{(L)}=p-y.\\] Y además se puede ver de la ecuación (7.3) que (\\(\\Theta_{*}^{(l+1)}\\) denota la matriz de pesos sin la columna correspondiente al sesgo): \\[\\begin{equation} \\delta^{(l)}=\\left( \\Theta_{*}^{(l+1)} \\right)^t\\delta^{(l+1)} \\circ h&#39;(z^{(l)}) \\tag{7.4} \\end{equation}\\] donde \\(\\circ\\) denota el producto componente a componente. Ahora todo ya está calculado. Lo interesante es que las \\(\\delta^{(l)}\\) se calculan de manera recursiva. 7.5.2 Algoritmo de backpropagation Backpropagation Para problema de clasificación con regularización $0 $. Para \\(i=1,\\ldots, N\\), tomamos el dato de entrenamiento \\((x^{(i)}, y^{(i)})\\) y hacemos: Ponemos \\(a^{(1)}=x^{(i)}\\) (vector de entradas, incluyendo 1). Calculamos \\(a^{(2)},a^{(3)},\\ldots, a^{(L)}\\) usando feed forward para la entrada \\(x^{(i)}\\) Calculamos \\(\\delta^{(L)}=a^{ (L)}-y^{(i)}\\), y luego \\(\\delta^{(L-1)},\\ldots, \\delta^{(2)}\\) según la recursión (7.3). Acumulamos \\(\\Delta_{j,k}^{(l)}=\\Delta_{j,k}^{(l)} + \\delta_j^{(l)}a_k^{(l-1)}\\). Finalmente, ponemos, si \\(k\\neq 0\\), \\[D_{j,k}^{(l)} = \\frac{2}{N}\\Delta_{j,k}^{(l)} + 2\\lambda\\theta_{j,k}^{(l)}\\] y si \\(k=0\\), \\[D_{j,k}^{(l)} = \\frac{2}{N}\\Delta_{j,k}^{(l)} .\\] Entonces: \\[D_{j,k}^{(l)} =\\frac{\\partial D}{\\partial \\theta_{j,k}^{(l)}}.\\] Nótese que back-propagation consiste principalmente de mutliplicaciones de matrices con algunas aplicaciones de \\(h\\) y acumulaciones, igual que feed-forward. 7.6 Ajuste de parámetros (introducción) Consideramos la versión con regularización ridge (también llamada L2) de la devianza de entrenamiento como nuestro función objetivo: Ajuste de redes neuronales Para un problema de clasificación binaria con \\(y_i=0\\) o \\(y_i=1\\), ajustamos los pesos \\(\\Theta^{(1)},\\Theta^{(2)},\\ldots, \\Theta^{(L)}\\) de la red minimizando la devianza (penalizada) sobre la muestra de entrenamiento: \\[D = -\\frac{2}{n}\\sum_{i=1}^n y_i\\log(p_1(x_i)) +(1-y_i)\\log(1-p_1(x_i)) + \\lambda \\sum_{l=2}^{L} \\sum_{k=1}^{n_{l-1}} \\sum_{j=1}^{n_l}(\\theta_{j,k}^{(l)})^2.\\] Este problema en general no es convexo y puede tener múltiples mínimos. Veremos el proceso de ajuste, selección de arquitectura, etc. más adelante. Por el momento hacemos unas observaciones acerca de este problema de minimización: Hay varios algoritmos para minimizar esta devianza, algunos avanzados incluyendo información de segundo orden (como Newton), pero actualmente las técnicas más populares, para redes grandes, están derivadas de descenso en gradiente. Más específicamente, una variación, que es descenso estocástico. Que el algoritmo depende principalmente de multiplicaciones de matrices y acumulaciones implica que puede escalarse de diversas maneras. Una es paralelizando sobre la muestra de entrenamiento (y acumular acumulados al final), pero quizá la más importante actualmente es la de multiplicaciones de matrices. Para redes neuronales, el gradiente se calcula con un algoritmo que se llama back-propagation, que es una aplicación de la regla de la cadena para propagar errores desde la capa de salida a lo largo de todas las capas para ajustar los pesos y sesgos. En estos problemas no buscamos el mínimo global, sino un mínimo local de buen desempeño. Puede haber múltiples mínimos, puntos silla, regiones relativamente planas, precipicios (curvatura alta). Todo esto dificulta el entrenamiento de redes neuronales grandes. Para redes grandes, ni siquiera esperamos a alcanzar un mínimo local, sino que nos detenemos prematuramente cuando obtenemos el mejor desempeño posible. Nótese que la simetría implica que podemos obtener la misma red cambiando pesos entre neuronas y las conexiones correspondientes. Esto implica que necesariamente hay varios mínimos. Para este problema, no tiene sentido comenzar las iteraciones con todos los pesos igual a cero, pues las unidades de la red son simétricas: no hay nada que diferencie una de otra si todos los pesos son iguales. Esto quiere decir que si iteramos, ¡todas las neuronas van a aprender lo mismo! Es importante no comenzar valores de los pesos grandes, pues las funciones logísticas pueden quedar en regiones planas donde la minimización es lenta, o podemos tener gradientes demasiado grandes y produzcan inestabilidad en el cálculo del gradiente. Generalmente los pesos se inicializan al azar con variables independientes gaussianas o uniformes centradas en cero, y con varianza chica (por ejemplo \\(U(-0.5,0.5)\\)). Una recomendación es usar \\(U(-1/\\sqrt(m), 1/\\sqrt(m))\\) donde \\(m\\) es el número de entradas. En general, hay que experimentar con este parámetro. El proceso para ajustar una red es entonces: Definir número de capas ocultas, número de neuronas por cada capa, y un valor del parámetro de regularización. Estandarizar las entradas. Seleccionar parámetros al azar para \\(\\Theta^{(2)},\\Theta^{(3)},\\ldots, \\Theta^{(L)}\\). Se toman, por ejemplo, normales con media 0 y varianza chica. Correr un algoritmo de minimización de la devianza mostrada arriba. Verificar convergencia del algoritmo a un mínimo local (o el algoritmo no está mejorando). Predecir usando el modelo ajustado. Finalmente, podemos probar distintas arquitecturas y valores del parámetros de regularización, para afinar estos parámetros según validación cruzada o una muestra de validación. 7.6.1 Ejemplo Consideramos una arquitectura de dos capas para el problema de diabetes if(Sys.info()[&#39;nodename&#39;] == &#39;vainilla.local&#39;){ # esto es por mi instalación particular de tensorflow - típicamente # no es necesario que corras esta línea. #Sys.setenv(TENSORFLOW_PYTHON=&quot;/usr/local/bin/python&quot;) } library(keras) ## ## Attaching package: &#39;keras&#39; ## The following objects are masked from &#39;package:igraph&#39;: ## ## %&lt;-%, normalize Escalamos y preparamos los datos: library(readr) library(tidyr) library(dplyr) diabetes_ent &lt;- MASS::Pima.tr diabetes_pr &lt;- MASS::Pima.te set.seed(293) x_ent &lt;- diabetes_ent %&gt;% select(-type) %&gt;% as.matrix x_ent_s &lt;- scale(x_ent) x_valid &lt;- diabetes_pr %&gt;% select(-type) %&gt;% as.matrix x_valid_s &lt;- x_valid %&gt;% scale(center = attr(x_ent_s, &#39;scaled:center&#39;), scale = attr(x_ent_s, &#39;scaled:scale&#39;)) y_ent &lt;- as.numeric(diabetes_ent$type == &#39;Yes&#39;) y_valid &lt;- as.numeric(diabetes_pr$type == &#39;Yes&#39;) Para definir la arquitectura de dos capas con: 10 unidades en cada capa función de activación sigmoide, regularización L2 (ridge), salida logística (\\(p_1\\)), escribimos: set.seed(9232) modelo_tc &lt;- keras_model_sequential() # no es necesario asignar a nuevo objeto, modelo_tc es modificado al agregar capas modelo_tc %&gt;% layer_dense(units = 10, activation = &#39;sigmoid&#39;, kernel_regularizer = regularizer_l2(l = 1e-4), kernel_initializer = initializer_random_uniform(minval = -0.5, maxval = 0.5), input_shape=7) %&gt;% layer_dense(units = 10, activation = &#39;sigmoid&#39;, kernel_regularizer = regularizer_l2(l = 1e-4), kernel_initializer = initializer_random_uniform(minval = -0.5, maxval = 0.5)) %&gt;% layer_dense(units = 1, activation = &#39;sigmoid&#39;, kernel_regularizer = regularizer_l2(l = 1e-4), kernel_initializer = initializer_random_uniform(minval = -0.5, maxval = 0.5) ) Ahora difinimos la función de pérdida (devianza es equivalente a entropía cruzada binaria), y pedimos registrar porcentaje de correctos (accuracy) y compilamos en tensorflow: modelo_tc %&gt;% compile( loss = &#39;binary_crossentropy&#39;, optimizer = optimizer_sgd(lr = 0.5), metrics = c(&#39;accuracy&#39;,&#39;binary_crossentropy&#39;)) Iteramos con descenso en gradiente y monitoreamos el error de validación. Hacemos 100 iteraciones de descenso en gradiente (épocas=100) iteraciones &lt;- modelo_tc %&gt;% fit( x_ent_s, y_ent, #batch size mismo que nrow(x_ent_s) es descenso en grad. epochs = 500, batch_size = nrow(x_ent_s), verbose = 0, validation_data = list(x_valid_s, y_valid) ) score &lt;- modelo_tc %&gt;% evaluate(x_valid_s, y_valid) score ## $loss ## [1] 0.4317647 ## ## $acc ## [1] 0.7891566 ## ## $binary_crossentropy ## [1] 0.4273301 tab_confusion &lt;- table(modelo_tc %&gt;% predict_classes(x_valid_s),y_valid) tab_confusion ## y_valid ## 0 1 ## 0 194 41 ## 1 29 68 prop.table(tab_confusion, 2) ## y_valid ## 0 1 ## 0 0.8699552 0.3761468 ## 1 0.1300448 0.6238532 Es importante monitorear las curvas de aprendizaje (entrenamiento y validación) para diagnosticar mejoras: df_iteraciones &lt;- as.data.frame(iteraciones) ggplot(df_iteraciones, aes(x=epoch, y=value, colour=data, group=data)) + geom_line() + geom_point() + facet_wrap(~metric, ncol=1, scales = &#39;free&#39;) Ejercicio Corre el ejemplo anterior con distintos parámetros de tasa de aprendizaje, número de unidades en las capas de intermedia y regularización (cambia arriba verbose=1 para monitorear al correr). 7.6.2 Hiperparámetros: búsqueda manual En búsqueda manual intentamos ajustar los parámetros haciendo experimentos sucesivos, monitoreando el error de entrenamiento y de validación. Los principios básicos que tenemos que entender son los de sesgo (rigidez) y varianza (flexibilidad) de los modelos y cómo se comportan estas cantidades cuando cambiamos parámetros, aunque es también importante experiencia e intuición. No hay una receta para hacer este proceso y garantizar llegar a una buena solución. Sin embargo, una guías para el proceso son las siguientes: Comenzamos poniendo valores usuales para los parámetros (que sabemos de ejemplos similares, o valores como 0.0001 para regularización, 0.1 para tasa de aprendizaje, etc.) Corremos algunas iteraciones y observamos en primer lugar que la tasa de aprendizaje sea apropiada (es el parámetro más importante!): queremos poner valores tan altos como sea posible sin que haya oscilaciones grandes del error de entrenamiento. Muchas veces, con una tasa demasiado alta, rápidamente llegamos a una región mala con gradientes bajos donde no podemos escapar (por ejemplo, con unidades saturadas) y parece que el modelo no aprende. Una tasa baja da convergencia muy lenta. Otra razón por la que la red puede no aprender es por gradientes se anulan en unidades saturadas. Si nos movemos a regiones donde valores de la salida de una capa son muy positivos o negativos, podemos saturar las unidades sigmoides (que son planas para valores muy positivos o negativos). Este se puede componer poniendo valores más chicos para la inicialización de pesos, o reducir la tasa de aprendizaje. Para redes de muchas capas, los gradientes también pueden explotar (valores muy grandes cuando hacemos backprop sobre la red) y llevarnos a regiones malas de saturación. Si el error de entrenamiento es muy alto y similar al de validación, el modelo quizá no tiene capacidad de aprender por sesgo (rigidez). Podemos incrementar el número de unidades, disminuir el valor de regularización (penalización L2 ), poner menos capas. También puede ser que la tasa de aprendizaje sea demasiado baja, y nos estamos quedando “atorados” en una región relativamente plana donde los gradientes son chicos. Si el error de entrenamiento es bajo, pero el de validación es alto, entonces quizá el modelo es demasiado flexible y está sobreajustando. Podemos regularizar más incrementando la penalización L2, incrementar el número de unidades por capa o de capas. Para redes neuronales grandes y problemas con ruido bajo, una estrategia es intentar obtener un error lo más chico posible para entrenamiento (la red aprende y/o memoriza), y después se afina para que la generalización (error de validación) sea buena. Podemos pensar que el error de validación tiene dos partes: el error de entrenamiento y el margen que hay entre error de entrenamiento y validación. Para reducir el error de validación podemos intentar reducir el error de entrenamiento y/o reducir el margen. Generalmente hay que ir balanceando flexibilidad con rigidez a través de varios parámetros para terminar con un buen ajuste. Ejercicio Haz algunos experimentos con las guías de arriba para el ejemplo de spam. Usa el script scripts/ejercicio-spam-hiperparametros.R 7.6.3 Hiperparámetros: búsqueda en grid Los hiperparámetros de un modelo son los que no se aprenden por el algoritmo principal. Por ejemplo, en regresión regularizada, los coeficientes son parámetros (se aprenden por descenso en gradiente), pero el parámetro de regularización \\(\\lambda\\) es un hiperparámetro. En redes neuronales tenemos más hiperparámetros: por ejemplo el número de capas, el número de unidades por capa, y el parámetro de regularización. La primera técnica para poner los hiperparámetros es búsqueda en grid. En este método escogemos los valores de cada hiperparámetro y ajustamos modelos para todas las posibles combinaciones. hiperparams &lt;- expand.grid(lambda = 10^seq(-9,-1, 1), n_capa = c(5, 10, 20, 50), lr = c(0.1, 0.5, 0.9), n_iter = 1000, init_pesos = c(0.5), stringsAsFactors = FALSE) hiperparams$corrida &lt;- 1:nrow(hiperparams) head(hiperparams) ## lambda n_capa lr n_iter init_pesos corrida ## 1 1e-09 5 0.1 1000 0.5 1 ## 2 1e-08 5 0.1 1000 0.5 2 ## 3 1e-07 5 0.1 1000 0.5 3 ## 4 1e-06 5 0.1 1000 0.5 4 ## 5 1e-05 5 0.1 1000 0.5 5 ## 6 1e-04 5 0.1 1000 0.5 6 nrow(hiperparams) ## [1] 108 correr_modelo &lt;- function(params, x_ent_s, y_ent, x_valid_s, y_valid){ modelo_tc &lt;- keras_model_sequential() u &lt;- params[[&#39;init_pesos&#39;]] modelo_tc %&gt;% layer_dense(units = params[[&#39;n_capa&#39;]], activation = &#39;sigmoid&#39;, kernel_regularizer = regularizer_l2(l = params[[&#39;lambda&#39;]]), kernel_initializer = initializer_random_uniform(minval = -u, maxval = u), input_shape=7) %&gt;% # layer_dense(units = params[[&#39;n_capa&#39;]], activation = &#39;sigmoid&#39;, # kernel_regularizer = regularizer_l2(l = params[[&#39;lambda&#39;]]), # kernel_initializer = initializer_random_uniform(minval = -u, # maxval = u)) %&gt;% layer_dense(units = 1, activation = &#39;sigmoid&#39;, kernel_regularizer = regularizer_l2(l = params[[&#39;lambda&#39;]]), kernel_initializer = initializer_random_uniform(minval = -u, maxval = u)) modelo_tc %&gt;% compile( loss = &#39;binary_crossentropy&#39;, optimizer = optimizer_sgd(lr =params[[&#39;lr&#39;]]), metrics = c(&#39;accuracy&#39;, &#39;binary_crossentropy&#39;) ) history &lt;- modelo_tc %&gt;% fit( x_ent_s, y_ent, epochs = params[[&#39;n_iter&#39;]], batch_size = nrow(x_ent_s), verbose = 0 ) score &lt;- modelo_tc %&gt;% evaluate(x_valid_s, y_valid) print(score) score } set.seed(34321) nombres &lt;- names(hiperparams) if(!usar_cache) { res &lt;- lapply(1:nrow(hiperparams), function(i){ params &lt;- as.vector(hiperparams[i,]) #names(params) &lt;- nombres #print(params$corrida) salida &lt;- correr_modelo(params, x_ent_s, y_ent, x_valid_s, y_valid) salida }) hiperparams$binary_crossentropy &lt;- sapply(res, function(item){ item$binary_crossentropy }) hiperparams$loss &lt;- sapply(res, function(item){ item$loss }) hiperparams$acc &lt;- sapply(res, function(item){ item$acc }) saveRDS(hiperparams, file = &#39;./cache_obj/diabetes-grid.rds&#39;) } else { hiperparams &lt;- readRDS(file = &#39;./cache_obj/diabetes-grid.rds&#39;) } Ordenamos del mejor modelo al peor según la pérdida: arrange(hiperparams, binary_crossentropy) %&gt;% head(10) ## lambda n_capa lr n_iter init_pesos corrida binary_crossentropy ## 1 1e-09 10 0.1 1000 0.5 10 0.4288144 ## 2 1e-08 5 0.1 1000 0.5 2 0.4298817 ## 3 1e-08 20 0.1 1000 0.5 20 0.4303028 ## 4 1e-08 10 0.1 1000 0.5 11 0.4303874 ## 5 1e-07 10 0.1 1000 0.5 12 0.4305910 ## 6 1e-06 20 0.1 1000 0.5 22 0.4312452 ## 7 1e-05 10 0.1 1000 0.5 14 0.4312888 ## 8 1e-07 20 0.1 1000 0.5 21 0.4320093 ## 9 1e-07 50 0.1 1000 0.5 30 0.4320917 ## 10 1e-06 50 0.1 1000 0.5 31 0.4321587 ## loss acc ## 1 0.4288144 0.7981928 ## 2 0.4298819 0.8012048 ## 3 0.4303031 0.8012048 ## 4 0.4303876 0.8012048 ## 5 0.4305927 0.8042169 ## 6 0.4312678 0.8012048 ## 7 0.4314596 0.8012048 ## 8 0.4320115 0.8042169 ## 9 0.4320959 0.8012048 ## 10 0.4321982 0.7981928 Y podemos estudiar la dependencia de la pérdida según distintos parámetros (ojo: los resultados son ruidosos por la muestra de validación relativamente chica y por el proceso de ajuste. Por ejemplo, los pesos aleatorios al arranque). ggplot(hiperparams, aes(x = lambda, y = binary_crossentropy, group=n_capa, colour=factor(n_capa))) + geom_line() + geom_point() + facet_wrap(~lr, ncol = 2) + scale_x_log10() ggplot(filter(hiperparams, lr==0.1, n_capa &gt; 10, lambda &lt; 1e-4), aes(x = lambda, y = binary_crossentropy, group=n_capa, colour=factor(n_capa))) + geom_line() + geom_point() + facet_wrap(~lr, ncol = 2) + scale_x_log10() Por ejemplo: lambda mayor a 0.001 es demasiado grande para cualquiera de estos modelo. la tasa de aprendizaje parece ser mejor alrededor de 0.1 para estos modelos - esto puede ser consecuencia de la regularización por pararnos antes de sobreajuste. Nótese por ejemplo que desperdiciamos iteraciones cuando la regularización es alta, y el rango de número de unidades que probamos tampoco parece producir muchas diferencias 7.6.4 Hiperparámetros: búsqueda aleatoria En el ejemplo anterior pudimos ver que muchas de las iteraciones son iteraciones desperdiciadas. Por ejemplo, el mal desempeño de un parámetro dado produce que no importen los valores de los otros parámetros. Sin embargo, corremos todas las combinaciones de los otros parámetros, las cuales todas se desempeñan mal. Especialmente cuando tenemos muchos parámetros, es más hacer eficiente hacer búsqueda aleatoria. Para hacer esto simulamos al azar valores de los parámetros a partir de una distribución de valores que queremos probar. Por ejemplo, para el número de unidades podríamos usar runif(1, 20, 200) ## [1] 30.75713 y para la regularización (donde queremos probar varios órdenes de magnitud) podríamos usar exp(runif(1, -8,-1)) ## [1] 0.0006036878 n_pars &lt;- 100 set.seed(913) if(!usar_cache){ hiperparams &lt;- data_frame(lambda = 10^(runif(n_pars, -10, -1)), n_capa = sample(c(2, 5, 10, 20, 50), n_pars, replace = T), lr = runif(n_pars, 0.01, 0.9), n_iter = 1000, init_pesos = runif(n_pars, 0.2,0.7)) hiperparams$corrida &lt;- 1:nrow(hiperparams) res_aleatorio &lt;- lapply(1:nrow(hiperparams), function(i){ params &lt;- as.vector(hiperparams[i,]) salida &lt;- correr_modelo(params, x_ent_s, y_ent, x_valid_s, y_valid) salida }) hiperparams$loss &lt;- sapply(res_aleatorio, function(item){ item$loss}) hiperparams$binary_crossentropy &lt;- sapply(res_aleatorio, function(item){ item$binary_crossentropy}) hiperparams$acc &lt;- sapply(res_aleatorio, function(item){ item$acc}) saveRDS(hiperparams, file = &#39;./cache_obj/diabetes-aleatorio.rds&#39;) } else { hiperparams &lt;- readRDS(file = &#39;./cache_obj/diabetes-aleatorio.rds&#39;) } arrange(hiperparams, binary_crossentropy) ## # A tibble: 100 x 9 ## lambda n_capa lr n_iter init_pesos corrida loss ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1.557467e-08 5 0.39449976 1000 0.3677915 19 0.4278859 ## 2 1.778047e-09 5 0.24924917 1000 0.5504638 2 0.4296006 ## 3 4.801019e-04 10 0.17805261 1000 0.3146846 67 0.4382751 ## 4 4.089486e-10 20 0.32910360 1000 0.4952716 37 0.4316516 ## 5 6.355566e-07 50 0.09925637 1000 0.6199004 69 0.4319281 ## 6 3.474533e-07 10 0.31243237 1000 0.4618183 7 0.4322901 ## 7 3.135937e-10 20 0.13151933 1000 0.2840034 55 0.4323465 ## 8 6.467834e-05 20 0.12685442 1000 0.5140470 23 0.4340249 ## 9 2.284078e-05 20 0.24400867 1000 0.5521355 68 0.4332621 ## 10 1.391978e-08 10 0.08317381 1000 0.5076250 60 0.4326821 ## # ... with 90 more rows, and 2 more variables: binary_crossentropy &lt;dbl&gt;, ## # acc &lt;dbl&gt; hiperparams$lr_grupo &lt;- cut(hiperparams$lr, breaks=c(0,0.1,0.25,0.5, 0.75,1)) hiperparams$init_grupo &lt;- cut(hiperparams$init_pesos, breaks=c(0.2,0.5,0.8)) ggplot(hiperparams, aes(x = lambda, y = binary_crossentropy, colour=lr_grupo, size = n_capa)) + geom_point(alpha = 0.75) + scale_x_log10() + facet_wrap(~init_grupo, ncol=1) 7.6.4.1 Nota (entrenamiento en keras) Al entrenar con keras, nótese que El valor de loss (pérdida de entrenamiento) incluye regularización (ridge, sin unidades dropout), pero el valor de val_loss no los incluye. El valor de loss se calcula como el promedio de pérdidas sobre los minilotes. El valor de val_loss se calcula con los parámetros obtenidos al final de la época. Por tanto, loss tiende a ser menor que lo que obtendríamos evaluando la pérdida de entrenamiento al final de la época, y puede ser en ocasiones más grande que val_loss. Si queremos ver valores totalmente comparables, podemos obtener el score del modelo, al final del entrenamiento, tanto para entrenamiento como prueba. Adicionalmente, diferencias en las muestras de entrenamiento y validación pueden producir también valores de validación ligaramente menores de entrenamiento. Este efecto puede ser más grande si se trata de muestras relativamente chicas de entrenamiento/prueba. Tarea (para 25 de septiembre) Instalar (keras para R)[https://keras.rstudio.com] (versión CPU). Suscribirse a kaggle (pueden ser equipos de 2 máximo, entonces les conviene suscribirse como un equipo). Hacer el ejercicio de arriba 7.1.0.1 Tarea (2 de octubre) Considera la siguiente red para un problema de clasificación binaria: Supón que los sesgos son 0 para la unidad \\(a_1\\), 0 para la unidad \\(a_2\\) y -0.5 para la unidad \\(p\\). Escribe cada \\(\\theta_{i,j}^{(l)}\\) según la notación de clase e identifica su valor. Por ejemplo, tenemos que \\(\\theta^{(2)}_{1,0} = 0\\) (tienes que escribir 7 valores como este). Supón que tenemos un caso (observación) con \\((x, p)=(1, 0)\\). ¿Qué es \\(a_1^{(1)}\\)? Haz forward feed para calcular los valores de \\(a_1^{(2)}\\), \\(a_1^{(2)}\\) y \\(p=a_1^{(3)}\\). Calcula la devianza para el caso \\((x, p)=(1, 0)\\) Según el cálculo que hiciste en 3, intuitivamente, ¿qué conviene hacer con los dos últimos pesos de la última capa para reducir la devianza? ¿Incrementarlos o disminuirlos? Error en la última capa (3): Calcula \\(\\delta^{(3)}_1\\) Calcula con backpropagation la derivada \\(\\frac{\\partial D}{\\partial \\theta^{(3)}_{1,1}}\\). El resultado coincide con tu intuición del inciso 4? Puedes intentar calcular directamente esta derivada también, con el método que quieras. Error en capa 2: Calcula \\(\\delta^{(2)}_1\\) y \\(\\delta^{(2)}_2\\), según la fórmula (7.3). Utiliza el inciso anterior para calcular \\(\\frac{\\partial D}{\\partial \\theta_{1,0}^{(2)}}\\), \\(\\frac{\\partial D}{\\partial \\theta_{1,1}^{(2)}}\\), \\(\\frac{\\partial D}{\\partial \\theta_{2,0}^{(2)}}\\) y \\(\\frac{\\partial D}{\\partial \\theta_{2,1}^{(2)}}\\) . ¿Puedes explicar los signos que obtuviste para estas derivadas (tip: tienes qué ver también que sucede en la siguiente capa)? "],
["redes-neuronales-parte-2.html", "Clase 8 Redes neuronales (parte 2) 8.1 Descenso estocástico 8.2 Algoritmo de descenso estocástico 8.3 ¿Por qué usar descenso estocástico por minilotes? 8.4 Escogiendo la tasa de aprendizaje 8.5 Mejoras al algoritmo de descenso estocástico. 8.6 Ajuste de redes con descenso estocástico 8.7 Activaciones relu 8.8 Dropout para regularización", " Clase 8 Redes neuronales (parte 2) En esta parte veremos aspectos más modernos de redes neuronales (incluyendo aprendizaje profundo). Estoy incluye métodos de ajuste, regularización, y definición de activaciones. 8.1 Descenso estocástico El algoritmo más popular para ajustar redes grandes es descenso estocástico, que es una modificación de nuestro algoritmo de descenso en gradiente. Antes de presentar las razones para usarlo, veremos cómo funciona para problemas con regresión lineal o logística. En descenso estocástico, el cálculo del gradiente se hace sobre una submuestra relativamente chica de la muestra de entrenamiento. En este contexto, a esta submuestra se le llama un minilote. En cada iteración, nos movemos en la dirección de descenso de ese minilote. La muestra de entrenamiento se divide entonces (al azar) en minilotes, y recorremos todos los minilotes haciendo una actualización de nuestros parámetros en cada minilote. Un recorrido sobre todos los minilotes se llama una época (las iteraciones se entienden sobre los minilotes). Antes de escribir el algoritmo mostramos una implementación para regresión logística. Usamos las mismas funciones para calcular devianza y gradiente. library(dplyr) library(tidyr) library(ggplot2) h &lt;- function(x){1/(1+exp(-x))} # la devianza es la misma devianza_calc &lt;- function(x, y){ dev_fun &lt;- function(beta){ p_beta &lt;- h(as.matrix(cbind(1, x)) %*% beta) -2*mean(y*log(p_beta) + (1-y)*log(1-p_beta)) } dev_fun } # el cálculo del gradiente es el mismo, pero x_ent y y_ent serán diferentes grad_calc &lt;- function(x_ent, y_ent){ salida_grad &lt;- function(beta){ p_beta &lt;- h(as.matrix(cbind(1, x_ent)) %*% beta) e &lt;- y_ent - p_beta grad_out &lt;- -2*as.numeric(t(cbind(1,x_ent)) %*% e)/nrow(x_ent) names(grad_out) &lt;- c(&#39;Intercept&#39;, colnames(x_ent)) grad_out } salida_grad } Y comparamos los dos algoritmos: descenso &lt;- function(n, z_0, eta, h_deriv){ z &lt;- matrix(0,n, length(z_0)) z[1, ] &lt;- z_0 for(i in 1:(n-1)){ z[i+1, ] &lt;- z[i, ] - eta * h_deriv(z[i, ]) } z } # esta implementación es solo para este ejemplo: descenso_estocástico &lt;- function(n_epocas, z_0, eta, minilotes){ #minilotes es una lista m &lt;- length(minilotes) z &lt;- matrix(0, m*n_epocas, length(z_0)) z[1, ] &lt;- z_0 for(i in 1:(m*n_epocas-1)){ k &lt;- i %% m + 1 if(i %% m == 0){ #comenzar nueva época y reordenar minilotes al azar minilotes &lt;- minilotes[sample(1:m, m)] } h_deriv &lt;- grad_calc(minilotes[[k]]$x, minilotes[[k]]$y) z[i+1, ] &lt;- z[i, ] - eta * h_deriv(z[i, ]) } z } Usaremos el ejemplo simulado de regresión para hacer algunos experimentos: p_1 &lt;- function(x){ ifelse(x &lt; 30, 0.9, 0.9 - 0.007 * (x - 15)) } set.seed(143) sim_datos &lt;- function(n){ x &lt;- pmin(rexp(n, 1/30), 100) probs &lt;- p_1(x) g &lt;- rbinom(length(x), 1, probs) # con dos variables de ruido: dat &lt;- data_frame(x_1 = (x - mean(x))/sd(x), x_2 = rnorm(length(x),0,1), x_3 = rnorm(length(x),0,1), p_1 = probs, g ) dat %&gt;% select(x_1, x_2, x_3, g) } dat_ent &lt;- sim_datos(100) dat_valid &lt;- sim_datos(1000) glm(g ~ x_1 + x_2+ x_3 , data = dat_ent, family = &#39;binomial&#39;) %&gt;% coef ## (Intercept) x_1 x_2 x_3 ## 1.8082362 -0.7439627 0.2172971 0.3711973 Hacemos descenso en gradiente: iteraciones_descenso &lt;- descenso(300, rep(0,4), 0.8, h_deriv = grad_calc(x_ent = as.matrix(dat_ent[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop =FALSE]), y_ent=dat_ent$g)) %&gt;% data.frame %&gt;% rename(beta_1 = X2, beta_2 = X3) ggplot(iteraciones_descenso, aes(x=beta_1, y=beta_2)) + geom_point() Y ahora hacemos descenso estocástico. Vamos a hacer minilotes de tamaño 5: dat_ent$minilote &lt;- rep(1:10, each=5) split_ml &lt;- split(dat_ent %&gt;% sample_n(nrow(dat_ent)), dat_ent$minilote) minilotes &lt;- lapply(split_ml, function(dat_ml){ list(x = as.matrix(dat_ml[, c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop=FALSE]), y = dat_ml$g) }) length(minilotes) ## [1] 10 Ahora iteramos. Nótese cómo descenso en gradiente tiene un patrón aleatorio de avance hacia el mínimo, y una vez que llega a una región oscila alrededor de este mínimo. iter_estocastico &lt;- descenso_estocástico(30, rep(0, 4), 0.1, minilotes) %&gt;% data.frame %&gt;% rename(beta_0 = X1, beta_1 = X2, beta_2 = X3) ggplot(iteraciones_descenso, aes(x=beta_1, y=beta_2)) + geom_path() + geom_point() + geom_path(data = iter_estocastico, colour =&#39;red&#39;, alpha=0.5) + geom_point(data = iter_estocastico, colour =&#39;red&#39;, alpha=0.5) Podemos ver cómo se ve la devianza de entrenamiento: dev_ent &lt;- devianza_calc(x = as.matrix(dat_ent[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop =FALSE]), y=dat_ent$g) dev_valid &lt;- devianza_calc(x = as.matrix(dat_valid[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop =FALSE]), y=dat_valid$g) dat_dev &lt;- data_frame(iteracion = 1:nrow(iteraciones_descenso)) %&gt;% mutate(descenso = apply(iteraciones_descenso, 1, dev_ent), descenso_estocastico = apply(iter_estocastico, 1, dev_ent)) %&gt;% gather(algoritmo, dev_ent, -iteracion) %&gt;% mutate(tipo =&#39;entrenamiento&#39;) dat_dev_valid &lt;- data_frame(iteracion = 1:nrow(iteraciones_descenso)) %&gt;% mutate(descenso = apply(iteraciones_descenso, 1, dev_valid), descenso_estocastico = apply(iter_estocastico, 1, dev_valid)) %&gt;% gather(algoritmo, dev_ent, -iteracion) %&gt;% mutate(tipo =&#39;validación&#39;) dat_dev &lt;- bind_rows(dat_dev, dat_dev_valid) ggplot(filter(dat_dev, tipo==&#39;entrenamiento&#39;), aes(x=iteracion, y=dev_ent, colour=algoritmo)) + geom_line() + geom_point() + facet_wrap(~tipo) y vemos que descenso estocástico también converge a una buena solución. 8.2 Algoritmo de descenso estocástico Descenso estocástico. Separamos al azar los datos de entrenamiento en \\(n\\) minilotes de tamaño \\(m\\). Para épocas \\(e =1,2,\\ldots, n_e\\) Calcular el gradiente sobre el minilote y hacer actualización, sucesivamente para cada uno de los minilotes \\(k=1,2,\\ldots, n/m\\): \\[\\beta_{i+1} = \\beta_{i} - \\eta\\sum_{j=1}^m \\nabla D^{(k)}_j (\\beta_i)\\] donde \\(D^{(k)}_j (\\beta_i)\\) es la devianza para el \\(j\\)-ésimo caso del minilote \\(k\\). Repetir para la siguiente época (opcional: reordenar antes al azar los minibatches, para evitar ciclos). 8.3 ¿Por qué usar descenso estocástico por minilotes? Las propiedades importantes de descenso estocástico son: Muchas veces no es necesario usar todos los datos para encontrar una buena dirección de descenso. Podemos ver la dirección de descenso en gradiente como un valor esperado sobre la muestra de entrenamiento (pues la pérdida es un promedio sobre el conjunto de entrenamiento). Una submuestra (minilote) puede ser suficiente para estimar ese valor esperado, con costo menor de cómputo. Adicionalmente, quizá no es tan buena idea intentar estimar el gradiente con la mejor precisión pues es solamente una dirección de descenso local (así que quizá no da la mejor decisión de a dónde moverse en cada punto). Es mejor hacer iteraciones más rápidas con direcciones estimadas. Desde este punto de vista, calcular el gradiente completo para descenso en gradiente es computacionalmente ineficiente. Si el conjunto de entrenamiento es masivo, descenso en gradiente no es factible. ¿Cuál es el mejor tamaño de minilote? Por un lado, minilotes más grandes nos dan mejores eficiencias en paralelización (multiplicación de matrices), especialmente en GPUs. Por otro lado, con minilotes más grandes puede ser que hagamos trabajo de más, por las razones expuestas en los incisos anteriores, y tengamos menos iteraciones en el mismo tiempo. El mejor punto está entre minilotes demasiado chicos (no aprovechamos paralelismo) o demasiado grande (hacemos demasiado trabajo por iteración).Tamaño t 4.La propiedad más importante de descenso estocástico en minilotes es entonces que su convergencia no depende del tamaño del conjunto de entrenamiento, es decir, el tiempo de iteración para descenso estocástico no crece con el número de casos totales. Podemos tener obtener buenos ajustes incluso con tamaños muy grandes de conjuntos de entrenamiento (por ejemplo, antes de procesar todos los datos de entrenamiento). Descenso estocástico escala bien en este sentido: el factor limitante es el tamaño de minilote y el número de iteraciones. Es importante permutar al azar los datos antes de hacer los minibatches, pues órdenes naturales en los datos pueden afectar la convergencia. Se ha observado también que permutar los minibatches en cada iteración típicamente acelera la convergencia (si se pueden tener los datos en memoria). Ejemplo En el ejemplo anterior nota que las direcciones de descenso de descenso estocástico son muy razonables (punto 1). Nota también que obtenemos una buena aproximación a la solución con menos cómputo (punto 2 - mismo número de iteraciones, pero cada iteración con un minilote). ggplot(filter(dat_dev, iteracion &gt;= 1), aes(x=iteracion, y=dev_ent, colour=algoritmo)) + geom_line() + geom_point(size=0.5)+ facet_wrap(~tipo, ncol=1) 8.4 Escogiendo la tasa de aprendizaje Para escoger la tasa, monitoreamos las curvas de error de entrenamiento y de validación. Si la tasa es muy grande, habrá oscilaciones grandes y muchas veces incrementos grandes en la función objectivo (error de entrenamiento). Algunas oscilaciones suaves no tienen problema -es la naturaleza estocástica del algoritmo. Si la tasa es muy baja, el aprendizaje es lento y podemos quedarnos en un valor demasiado alto. Conviene monitorear las primeras iteraciones y escoger una tasa más alta que la mejor que tengamos acutalmente, pero no tan alta que cause inestabilidad. Una gráfica como la siguiente es útil. En este ejemplo, incluso podríamos detenernos antes para evitar el sobreajuste de la última parte de las iteraciones: ggplot(filter(dat_dev, algoritmo==&#39;descenso_estocastico&#39;), aes(x=iteracion, y=dev_ent, colour=tipo)) + geom_line() + geom_point() Por ejemplo: tasa demasiado alta: iter_estocastico &lt;- descenso_estocástico(20, rep(0,4), 0.95, minilotes) %&gt;% data.frame %&gt;% rename(beta_0 = X1, beta_1 = X2) dev_ent &lt;- devianza_calc(x = as.matrix(dat_ent[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop =FALSE]), y=dat_ent$g) dev_valid &lt;- devianza_calc(x = as.matrix(dat_valid[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop =FALSE]), y=dat_valid$g) dat_dev &lt;- data_frame(iteracion = 1:nrow(iter_estocastico)) %&gt;% mutate(entrena = apply(iter_estocastico, 1, dev_ent), validacion = apply(iter_estocastico, 1, dev_valid)) %&gt;% gather(tipo, devianza, entrena:validacion) ggplot(dat_dev, aes(x=iteracion, y=devianza, colour=tipo)) + geom_line() + geom_point() Tasa demasiado chica ( o hacer más iteraciones): iter_estocastico &lt;- descenso_estocástico(20, rep(0,4), 0.01, minilotes) %&gt;% data.frame %&gt;% rename(beta_0 = X1, beta_1 = X2) dev_ent &lt;- devianza_calc(x = as.matrix(dat_ent[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop =FALSE]), y=dat_ent$g) dev_valid &lt;- devianza_calc(x = as.matrix(dat_valid[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop =FALSE]), y=dat_valid$g) dat_dev &lt;- data_frame(iteracion = 1:nrow(iter_estocastico)) %&gt;% mutate(entrena = apply(iter_estocastico, 1, dev_ent), validacion = apply(iter_estocastico, 1, dev_valid)) %&gt;% gather(tipo, devianza, entrena:validacion) ggplot(dat_dev, aes(x=iteracion, y=devianza, colour=tipo)) + geom_line() Para redes neuronales, es importante explorar distintas tasas de aprendizaje, aún cuando no parezca haber oscilaciones grandes o convergencia muy lenta. En algunos casos, si la tasa es demasiado grande, puede ser que el algoritmo llegue a lugares con gradientes cercanos a cero (por ejemplo, por activaciones demasiado grandes) y tenga dificultad para moverse. 8.5 Mejoras al algoritmo de descenso estocástico. 8.5.1 Decaimiento de tasa de aprendizaje Hay muchos algoritmos derivados de descenso estocástico. La primera mejora consiste en reducir gradualmente la tasa de aprendizaje para aprender rápido al principio, pero filtrar el ruido de la estimación de minilotes más adelante en las iteraciones y permitir que el algoritmo se asiente en un mínimo. descenso_estocástico &lt;- function(n_epocas, z_0, eta, minilotes, decaimiento = 0.0){ #minilotes es una lista m &lt;- length(minilotes) z &lt;- matrix(0, m*n_epocas, length(z_0)) z[1, ] &lt;- z_0 for(i in 1:(m*n_epocas-1)){ k &lt;- i %% m + 1 if(i %% m == 0){ #comenzar nueva época y reordenar minilotes al azar minilotes &lt;- minilotes[sample(1:m, m)] } h_deriv &lt;- grad_calc(minilotes[[k]]$x, minilotes[[k]]$y) z[i+1, ] &lt;- z[i, ] - eta * h_deriv(z[i, ]) eta &lt;- eta*(1/(1+decaimiento*i)) } z } Y ahora vemos qué pasa con decaimiento: iter_estocastico &lt;- descenso_estocástico(20, c(0,0, 0, 0), 0.3, minilotes, decaimiento = 0.0002) %&gt;% data.frame %&gt;% rename(beta_0 = X1, beta_1 = X2, beta_2 = X3, beta_3 = X4) dev_ent &lt;- devianza_calc(x = as.matrix(dat_ent[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop =FALSE]), y=dat_ent$g) dev_valid &lt;- devianza_calc(x = as.matrix(dat_valid[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop =FALSE]), y=dat_valid$g) dat_dev &lt;- data_frame(iteracion = 1:nrow(iter_estocastico)) %&gt;% mutate(entrena = apply(iter_estocastico, 1, dev_ent), validacion = apply(iter_estocastico, 1, dev_valid)) %&gt;% gather(tipo, devianza, entrena:validacion) ggplot(filter(dat_dev, iteracion&gt;1), aes(x=iteracion, y=devianza, colour=tipo)) + geom_line() + geom_point() ggplot(iteraciones_descenso, aes(x=beta_1, y=beta_2)) + geom_path() + geom_point() + geom_path(data = iter_estocastico, colour =&#39;red&#39;, alpha=0.5) + geom_point(data = iter_estocastico, colour =&#39;red&#39;, alpha=0.5) La tasa de aprendizaje es uno de los parámetros en redes neuronales más importantes de afinar. Generalmente se empieza con una tasa de aprendizaje con un valor bajo (0.01, o. 0.1), pero es necesario experimentar. Un valor muy alto puede provocar oscilaciones muy fuertes en la pérdida Un valor alto también puede provocar que el algoritmo se detenga en lugar con función pérdida alta (sobreajusta rápidamente). Un valor demasiado bajo produce convergencia lenta. 8.5.2 Momento También es posible utilizar una idea adicional que acelera la convergencia. La idea es que muchas veces la aleatoriedad del algoritmo puede producir iteraciones en direcciones que no son tan buenas (pues la estimación del gradiente es mala). Esto es parte del algoritmo. Sin embargo, si en varias iteraciones hemos observado movimientos en direcciones consistentes, quizá deberíamos movernos en esas direcciones consistentes, y reducir el peso de la dirección del minilote (que nos puede llevar en una dirección mala). El resultado es un suavizamiento de las curvas de aprendizaje. Esto es similar al movimiento de una canica en una superficie: la dirección de su movimiento está dada en parte por la dirección de descenso (el gradiente) y en parte la velocidad actual de la canica. La canica se mueve en un promedio de estas dos direcciones Descenso estocástico con momento Separamos al azar los datos de entrenamiento en \\(n\\) minilotes de tamaño \\(m\\). Para épocas \\(e =1,2,\\ldots, n_e\\) Calcular el gradiente sobre el minilote y hacer actualización, sucesivamente para cada uno de los minilotes \\(k=1,2,\\ldots, n/m\\): \\[\\beta_{i+1} = \\beta_{i} + v,\\] \\[v= \\alpha v - \\eta\\sum_{j=1}^m \\nabla D^{(k)}_j\\] donde \\(D^{(k)}_j (\\beta_i)\\) es la devianza para el \\(j\\)-ésimo caso del minilote \\(k\\). A \\(v\\) se llama la velocidad Repetir para la siguiente época descenso_estocástico &lt;- function(n_epocas, z_0, eta, minilotes, momento = 0.0, decaimiento = 0.0){ #minilotes es una lista m &lt;- length(minilotes) z &lt;- matrix(0, m*n_epocas, length(z_0)) z[1, ] &lt;- z_0 v &lt;- 0 for(i in 1:(m*n_epocas-1)){ k &lt;- i %% m + 1 if(i %% m == 0){ #comenzar nueva época y reordenar minilotes al azar minilotes &lt;- minilotes[sample(1:m, m)] v &lt;- 0 } h_deriv &lt;- grad_calc(minilotes[[k]]$x, minilotes[[k]]$y) z[i+1, ] &lt;- z[i, ] + v v &lt;- momento*v - eta * h_deriv(z[i, ]) eta &lt;- eta*(1/(1+decaimiento*i)) } z } Y ahora vemos que usando momento el algoritmo es más parecido a descenso en gradiente usual (pues tenemos cierta memoria de direcciones anteriores de descenso): set.seed(231) iter_estocastico &lt;- descenso_estocástico(20, c(0,0, 0, 0), 0.2, minilotes, momento = 0.7, decaimiento = 0.001) %&gt;% data.frame %&gt;% rename(beta_0 = X1, beta_1 = X2, beta_2=X3, beta_3=X4) dev_ent &lt;- devianza_calc(x = as.matrix(dat_ent[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop =FALSE]), y=dat_ent$g) dev_valid &lt;- devianza_calc(x = as.matrix(dat_valid[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;), drop =FALSE]), y=dat_valid$g) dat_dev &lt;- data_frame(iteracion = 1:nrow(iter_estocastico)) %&gt;% mutate(entrena = apply(iter_estocastico, 1, dev_ent), validacion = apply(iter_estocastico, 1, dev_valid)) %&gt;% gather(tipo, devianza, entrena:validacion) ggplot(filter(dat_dev, iteracion &gt; 1), aes(x=iteracion, y=devianza, colour=tipo)) + geom_line() + geom_point() ggplot(iteraciones_descenso, aes(x=beta_1, y=beta_2)) + geom_path() + geom_point() + geom_path(data = iter_estocastico, colour =&#39;red&#39;, alpha=0.5) + geom_point(data = iter_estocastico, colour =&#39;red&#39;, alpha=0.5) Nótese cómo llegamos más rápido a una buena solución (comparado con el ejemplo sin momento). Adicionalmente, error de entrenamiento y validación lucen más suaves, producto de promediar velocidades a lo largo de iteraciones. Valores típicos para momento son 0,0.5,0.9 o 0.99. 8.5.3 Otras variaciones Otras variaciones incluyen usar una tasa adaptativa de aprendizaje por cada parámetro (algoritmos adagrad, rmsprop, adam y adamax), o actualizaciones un poco diferentes (nesterov). Los más comunes son descenso estocástico, descenso estocástico con momento, rmsprop y adam (Capítulo 8 del Deep Learning Book, (Goodfellow, Bengio, and Courville 2016)). 8.6 Ajuste de redes con descenso estocástico if(Sys.info()[&#39;nodename&#39;] == &#39;vainilla.local&#39;){ # esto es por mi instalación particular de tensorflow - típicamente # no es necesario que corras esta línea. #Sys.setenv(TENSORFLOW_PYTHON=&quot;/usr/local/bin/python&quot;) } library(keras) set.seed(21321) x_ent &lt;- as.matrix(dat_ent[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;)]) x_valid &lt;- as.matrix(dat_valid[,c(&#39;x_1&#39;,&#39;x_2&#39;,&#39;x_3&#39;)]) y_ent &lt;- dat_ent$g y_valid &lt;- dat_valid$g Empezamos con regresión logística (sin capas ocultas), que se escribe y ajusta como sigue: modelo &lt;- keras_model_sequential() modelo %&gt;% layer_dense(units = 1, activation = &#39;sigmoid&#39;, input_shape = c(3)) modelo %&gt;% compile(loss = &#39;binary_crossentropy&#39;, optimizer = optimizer_sgd(lr = 0.2, momentum = 0, decay = 0), metrics = c(&#39;accuracy&#39;) ) history &lt;- modelo %&gt;% fit(x_ent, y_ent, epochs = 50, batch_size = 64, verbose = 0, validation_data = list(x_valid, y_valid)) Podemos ver el progreso del algoritmo por época aprendizaje &lt;- as.data.frame(history) ggplot(aprendizaje, aes(x=epoch, y=value, colour=data, group=data)) + facet_wrap(~metric, ncol = 1) + geom_line() + geom_point(size = 0.5) Ver los pesos: get_weights(modelo) ## [[1]] ## [,1] ## [1,] -0.6408466 ## [2,] 0.1310949 ## [3,] 0.3389925 ## ## [[2]] ## [1] 1.669 Y verificamos que concuerda con la salida de glm: mod_logistico &lt;- glm(g ~ x_1 + x_2+ x_3, data = dat_ent, family = &#39;binomial&#39;) coef(mod_logistico) ## (Intercept) x_1 x_2 x_3 ## 1.8082362 -0.7439627 0.2172971 0.3711973 0.5*mod_logistico$deviance/nrow(dat_ent) ## [1] 0.3925183 Ejemplo Ahora hacemos algunos ejemplos para redes totalmente conexas. Usaremos los datos de reconocimiento de dígitos. library(readr) digitos_entrena &lt;- read_csv(&#39;./datos/zip-train.csv&#39;) digitos_prueba &lt;- read_csv(&#39;./datos/zip-test.csv&#39;) names(digitos_entrena)[1] &lt;- &#39;digito&#39; names(digitos_entrena)[2:257] &lt;- paste0(&#39;pixel_&#39;, 1:256) names(digitos_prueba)[1] &lt;- &#39;digito&#39; names(digitos_prueba)[2:257] &lt;- paste0(&#39;pixel_&#39;, 1:256) dim(digitos_entrena) ## [1] 7291 257 table(digitos_entrena$digito) ## ## 0 1 2 3 4 5 6 7 8 9 ## 1194 1005 731 658 652 556 664 645 542 644 Ponemos el rango entre [0,2] (pixeles positivos) x_train &lt;- digitos_entrena %&gt;% select(contains(&#39;pixel&#39;)) %&gt;% as.matrix + 1 x_train &lt;- x_train x_test &lt;- digitos_prueba %&gt;% select(contains(&#39;pixel&#39;)) %&gt;% as.matrix + 1 x_test &lt;- x_test Usamos codificación dummy: #dim(x_train) &lt;- c(nrow(x_train), 16, 16, 1) #dim(x_test) &lt;- c(nrow(x_test), 16, 16, 1) y_train &lt;- to_categorical(digitos_entrena$digito) y_test &lt;- to_categorical(digitos_prueba$digito) head(y_train) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 0 0 0 0 0 0 1 0 0 0 ## [2,] 0 0 0 0 0 1 0 0 0 0 ## [3,] 0 0 0 0 1 0 0 0 0 0 ## [4,] 0 0 0 0 0 0 0 1 0 0 ## [5,] 0 0 0 1 0 0 0 0 0 0 ## [6,] 0 0 0 0 0 0 1 0 0 0 Y definimos un modelo con 2 capas de 200 unidades cada una y regularización L2. Nótese que usamos softmax en la última capa, que es la función (ver parte de regresión multinomial) cuya salida \\(k\\) está dada por \\[p_k = \\frac{exp(z_k)}{\\sum_j exp(z_j)}\\] donde \\(z=(z_1,\\ldots, z_K)\\) (estas son las combinaciones lineales de las unidades de la capa anterior). modelo_tc &lt;- keras_model_sequential() modelo_tc %&gt;% layer_dense(units = 200, activation = &#39;sigmoid&#39;, kernel_regularizer = regularizer_l2(l = 1e-6), input_shape=256) %&gt;% layer_dense(units = 200, activation = &#39;sigmoid&#39;, kernel_regularizer = regularizer_l2(l = 1e-6)) %&gt;% layer_dense(units = 10, activation = &#39;softmax&#39;, kernel_regularizer = regularizer_l2(l = 1e-6)) modelo_tc %&gt;% compile( loss = &#39;categorical_crossentropy&#39;, optimizer = optimizer_sgd(lr = 0.5, momentum = 0.0, decay = 1e-6), metrics = c(&#39;accuracy&#39; ,&#39;categorical_crossentropy&#39;) ) history &lt;- modelo_tc %&gt;% fit( x_train, y_train, epochs = 100, batch_size = 256, verbose = 0, validation_data = list(x_test, y_test) ) score &lt;- modelo_tc %&gt;% evaluate(x_test, y_test) score ## $loss ## [1] 0.2828375 ## ## $acc ## [1] 0.9327354 ## ## $categorical_crossentropy ## [1] 0.2817869 Podemos también intentar con el ejemplo de spam: library(readr) library(tidyr) library(dplyr) spam_entrena &lt;- read_csv(&#39;./datos/spam-entrena.csv&#39;) #%&gt;% sample_n(2000) spam_prueba &lt;- read_csv(&#39;./datos/spam-prueba.csv&#39;) set.seed(293) x_ent &lt;- spam_entrena %&gt;% select(-X1, -spam) %&gt;% as.matrix x_ent_s &lt;- scale(x_ent) x_valid &lt;- spam_prueba %&gt;% select(-X1, -spam) %&gt;% as.matrix x_valid_s &lt;- x_valid %&gt;% scale(center = attr(x_ent_s, &#39;scaled:center&#39;), scale = attr(x_ent_s, &#39;scaled:scale&#39;)) y_ent &lt;- spam_entrena$spam y_valid &lt;- spam_prueba$spam En este caso, intentemos una capa oculta: modelo_tc &lt;- keras_model_sequential() modelo_tc %&gt;% layer_dense(units = 200, activation = &#39;sigmoid&#39;, kernel_regularizer = regularizer_l2(l = 1e-5), input_shape=57) %&gt;% layer_dense(units = 1, activation = &#39;sigmoid&#39;) modelo_tc %&gt;% compile( loss = &#39;binary_crossentropy&#39;, optimizer = optimizer_sgd(lr = 0.5, momentum = 0.5), metrics = c(&#39;accuracy&#39;, &#39;binary_crossentropy&#39;) ) history &lt;- modelo_tc %&gt;% fit( x_ent_s, y_ent, epochs = 200, batch_size = 256, verbose = 0, validation_data = list(x_valid_s, y_valid) ) score &lt;- modelo_tc %&gt;% evaluate(x_valid_s, y_valid) tab_confusion &lt;- table(modelo_tc %&gt;% predict_classes(x_valid_s),y_valid) tab_confusion ## y_valid ## 0 1 ## 0 894 57 ## 1 33 550 prop.table(tab_confusion, 2) ## y_valid ## 0 1 ## 0 0.96440129 0.09390445 ## 1 0.03559871 0.90609555 8.7 Activaciones relu Recientemente se ha descubierto (en gran parte empíricamente) que hay una unidad más conveniente para las activaciones de las unidades, en lugar de la función sigmoide Activaciones lineales rectificadas (relu) La función relu es \\[\\begin{equation} h(z) = \\begin{cases} z &amp;\\, z&gt;0\\\\ 0 &amp;\\, z&lt;=0 \\end{cases} \\end{equation}\\] Estas generalmente sustituyen a las unidades sigmoidales en capas ocultas h_relu &lt;- function(z) ifelse(z &gt; 0, z, 0) h_logistica &lt;- function(z) 4/(1+exp(-z)) #mult por 4 para comparar más fácilmente curve(h_relu, -5,5) curve(h_logistica, add=T, col=&#39;red&#39;) La razón del exito de estas activaciones no está del todo clara, aunque generalmente se cita el hecho de que una unidad saturada (valores de entrada muy positivos o muy negativos) es problemática en optimización, y las unidades tienen menos ese problema pues no se saturan para valores positivos. Pregunta: ¿cómo cambiaría el algoritmo de feed-forward con estas unidades? ¿el de back-prop? Ejemplo Veamos el mismo modelo de dos capas de arriba, pero con activaciones relu: modelo_tc &lt;- keras_model_sequential() modelo_tc %&gt;% layer_dense(units = 200, activation = &#39;relu&#39;, kernel_regularizer = regularizer_l2(l = 1e-3), input_shape=256) %&gt;% layer_dense(units = 200, activation = &#39;relu&#39;, kernel_regularizer = regularizer_l2(l = 1e-3)) %&gt;% layer_dense(units = 10, activation = &#39;softmax&#39;, kernel_regularizer = regularizer_l2(l = 1e-3)) modelo_tc %&gt;% compile( loss = &#39;categorical_crossentropy&#39;, optimizer = optimizer_sgd(lr = 0.3, momentum = 0.0, decay = 0), metrics = c(&#39;accuracy&#39;, &#39;categorical_crossentropy&#39;) ) history &lt;- modelo_tc %&gt;% fit( x_train, y_train, epochs = 200, batch_size = 256, verbose = 0, validation_data = list(x_test, y_test) ) score &lt;- modelo_tc %&gt;% evaluate(x_test, y_test) score ## $loss ## [1] 0.2871302 ## ## $acc ## [1] 0.941704 ## ## $categorical_crossentropy ## [1] 0.2110756 8.8 Dropout para regularización Un método más nuevo y exitoso para regularizar es el dropout. Consiste en perturbar la red en cada pasada de entrenamiento de minibatch (feed-forward y backprop), eliminando al azar algunas de las unidades de cada capa. El objeto es que al introducir ruido en el proceso de entrenamiento evitamos sobreajuste, pues en cada paso de la iteración estamos limitando el número de unidades que la red puede usar para ajustar las respuestas. Dropout entonces busca una reducción en el sobreajuste que sea más provechosa que el consecuente aumento en el sesgo. Dropout En cada iteración (minibatch), seleccionamos con cierta probablidad \\(p\\) eliminar cada una de las unidades (independientemente en cada capa, y posiblemente con distintas \\(p\\) en cada capa), es decir, hacemos su salida igual a 0. Hacemos forward-feed y back-propagation poniendo en 0 las unidades eliminadas. Escalar pesos: para predecir (prueba), usamos todas las unidades. Si una unidad tiene peso \\(\\theta\\) en una capa después de entrenar, y la probablidad de que esa capa no se haya hecho 0 es \\(1-p\\), entonces usamos \\((1-p)\\theta\\) como peso para hacer predicciones. Si hacemos dropout de la capa de entrada, generalmente se usan valores chicos alrededor de \\(0.2\\). En capas intermedias se usan generalmente valores más grandes alrededor de \\(0.5\\). Podemos hacer dropout de la capa de entrada. En este caso, estamos evitando que el modelo dependa fuertemente de variables individuales. Por ejemplo, en procesamiento de imágenes, no queremos que por sobreajuste algunas predicciones estén ligadas fuertemente a un solo pixel (aún cuando en entrenamiento puede ser que un pixel separe bien los casos que nos interesa clasificar). Ejemplo: dropout y regularización Consideremos el problema de separar 9 y 3 del resto de dígitos zip. Queremos comparar el desempeño de una red sin y con dropout (tanto de entradas como de capa oculta) y entender parcialmente cómo se comportan los pesos aprendidos: set.seed(29123) entrena_3 &lt;- digitos_entrena %&gt;% sample_n(nrow(digitos_entrena)) %&gt;% sample_n(3000) x_train_3 &lt;- entrena_3 %&gt;% select(-digito) %&gt;% as.matrix + 1 y_train_3 &lt;- (entrena_3$digito %in% c(3,9)) %&gt;% as.numeric set.seed(12) modelo_sin_reg &lt;- keras_model_sequential() modelo_sin_reg %&gt;% layer_dense(units = 30, activation = &#39;relu&#39;, input_shape = 256) %&gt;% layer_dense(units = 1, activation = &#39;sigmoid&#39;) set.seed(12) modelo_dropout &lt;- keras_model_sequential() modelo_dropout %&gt;% layer_reshape(input_shape=256, target_shape=256) %&gt;% layer_dropout(0.3) %&gt;% layer_dense(units = 30, activation = &#39;relu&#39;, input_shape = 256) %&gt;% layer_dropout(0.3) %&gt;% layer_dense(units = 1, activation = &#39;sigmoid&#39;) El modelo sin regularización sobreajusta (nótese que el error de validación comienza a crecer considerablemente muy pronto, hay un margen grande entre entrenamiento y validación, y la pérdida de entrenamiento es cercana a 0): modelo_sin_reg %&gt;% compile(loss = &#39;binary_crossentropy&#39;, optimizer = optimizer_sgd(lr = 0.3), metrics = c(&#39;accuracy&#39;) ) history_1 &lt;- modelo_sin_reg %&gt;% fit(x_train_3/2, y_train_3, verbose=0, epochs = 500, batch_size = 256, validation_split = 0.2 ) hist_1 &lt;- as.data.frame(history_1) ggplot(hist_1, aes(x=epoch, y=value, colour=data)) + geom_line() + facet_wrap(~metric, scales = &#39;free&#39;, ncol=1) Y parecen ruidosas las unidades que aprendió en la capa oculta (algunas no aprendieron o aprendieron cosas irrelevantes). En la siguiente imagen, cada pixel es un peso. Cada imagen agrupa los pesos de una unidad, y ordenamos los pesos según la variable de entrada (pixel) al que se multiplican. graf_pesos &lt;- function(pesos, mostrar_facets=FALSE){ pesos_df &lt;- as_tibble(pesos) %&gt;% mutate(pixel = 1:256) %&gt;% mutate(x=(pixel -1) %% 16, y = (pixel-1)%/% 16) %&gt;% gather(unidad, valor, -pixel,-x,-y) %&gt;% mutate(unidad=as.integer(unidad)) gplot &lt;- ggplot(pesos_df, aes(x=x,y=-y, fill=valor)) + geom_tile() + facet_wrap(~unidad)+scale_fill_gradient2(low = &quot;black&quot;, mid=&#39;gray80&#39;, high = &quot;white&quot;) + coord_fixed() if(!mostrar_facets){ gplot &lt;- gplot + theme(strip.background = element_blank(), strip.text = element_blank()) } gplot } pesos &lt;- get_weights(modelo_sin_reg)[[1]] colnames(pesos) &lt;- 1:ncol(pesos) graf_pesos(pesos) Ahora ajustamos el modelo con dropout: modelo_dropout %&gt;% compile(loss = &#39;binary_crossentropy&#39;, optimizer = optimizer_sgd(lr = 0.5), metrics = c(&#39;accuracy&#39;) ) history_2 &lt;- modelo_dropout %&gt;% fit(x_train_3/2, y_train_3, verbose=1, epochs = 500, batch_size = 256, validation_split = 0.5 ) hist_2 &lt;- as.data.frame(history_2) ggplot(hist_2, aes(x=epoch, y=value, colour=data)) + geom_line() + facet_wrap(~metric, scales = &#39;free&#39;) El desempeño es mejor, y parecen ser más útiles los patrones que aprendió el capa oculta: pesos &lt;- get_weights(modelo_dropout)[[1]] colnames(pesos) &lt;- 1:ncol(pesos) graf_pesos(pesos) get_weights(modelo_dropout)[[3]] ## [,1] ## [1,] -1.5522770 ## [2,] -1.3059361 ## [3,] 1.1885204 ## [4,] -1.4361888 ## [5,] -1.4261484 ## [6,] 0.1157310 ## [7,] 1.3153723 ## [8,] -1.6062913 ## [9,] -1.1689290 ## [10,] -1.2372445 ## [11,] -0.3671513 ## [12,] 1.4014351 ## [13,] 0.3783767 ## [14,] 0.1039644 ## [15,] -0.9265355 ## [16,] 0.3366016 ## [17,] 0.1431678 ## [18,] -1.7270532 ## [19,] -1.8995001 ## [20,] -1.6508746 ## [21,] -1.1958852 ## [22,] -1.6964142 ## [23,] 0.6303982 ## [24,] -1.4776405 ## [25,] -1.1463586 ## [26,] 1.3802754 ## [27,] -0.8566877 ## [28,] 0.6562567 ## [29,] -1.6447566 ## [30,] 1.2081616 ¿Cuáles de estas unidades tienen peso positivo y negativo en la capa final? pesos_capa_f &lt;- get_weights(modelo_dropout)[[3]] graf_pesos(pesos[, pesos_capa_f &gt; 0.5], mostrar_facets = TRUE) graf_pesos(pesos[, pesos_capa_f &lt; -1.5], mostrar_facets = TRUE) Comentarios adicionales Algunas maneras en que podemos pensar en la regularización de dropout: Dropout busca que cada unidad calcule algo importante por sí sola, y dependa menos de otras unidades para hacer algo útil. Algunas unidades y pesos pueden acoplarse fuertemente (y de manera compleja) para hacer las predicciones. Si estas unidades aprendieron ese acoplamento demasiado fuerte para el conjunto de entrenamiento, entonces puede ser nuevos datos, con perturbaciones, puedan producir predicciones malas (mala generalización). Con dropout buscamos que la unidades capturen información útil en general, no necesariamente en acoplamiento fuerte con otras unidades. Podemos pensar que en cada pasada de minibatch, escogemos una arquitectura diferente, y entrenamos. El resultado final será entonces es un tipo de promedio de todas esas arquitecturas que probamos. Este promedio reduce varianza de las salidas de las unidades. El paso de escalamiento es importante para el funcionamiento correcto del método. La idea intuitiva es que el peso de una unidad es 0 con probabilidad \\(p\\) y \\(\\theta\\) con probabilidad \\(1-p\\). Tomamos el valor esperado como peso para la red completa, que es \\(p0+(1-p)\\theta\\). Ver (Srivastava et al. 2014) Ejemplo Experimenta en este ejemplo con distintos valores de dropout, y verifica intuitivamente sus efectos de regularización (ve las curvas de aprendizaje). modelo_tc &lt;- keras_model_sequential() modelo_tc %&gt;% layer_reshape(input_shape=256, target_shape=256) %&gt;% layer_dropout(rate=0.2) %&gt;% layer_dense(units = 200, activation = &#39;relu&#39;) %&gt;% layer_dropout(rate = 0.5) %&gt;% layer_dense(units = 200, activation = &#39;relu&#39;) %&gt;% layer_dropout(rate = 0.5) %&gt;% layer_dense(units = 10, activation = &#39;softmax&#39;, kernel_regularizer = regularizer_l2(l = 1e-4)) modelo_tc %&gt;% compile( loss = &#39;categorical_crossentropy&#39;, optimizer = optimizer_sgd(lr = 0.3, momentum = 0.5, decay = 0.0001), metrics = c(&#39;accuracy&#39;, &#39;categorical_crossentropy&#39;) ) history &lt;- modelo_tc %&gt;% fit( x_train, y_train, epochs = 100, batch_size = 256, validation_data = list(x_test, y_test) ) score &lt;- modelo_tc %&gt;% evaluate(x_test, y_test) score ## $loss ## [1] 0.2113736 ## ## $acc ## [1] 0.9521674 ## ## $categorical_crossentropy ## [1] 0.2072624 References "],
["redes-convolucionales.html", "Clase 9 Redes convolucionales 9.1 Filtros convolucionales 9.2 Filtros convolucionales para redes neuronales 9.3 Capas de agregación (pooling) 9.4 Ejemplo (arquitectura LeNet):", " Clase 9 Redes convolucionales Las redes convolucionales son un tipo de arquitectura de red que utiliza ciertos supuestos acerca de los pesos, en contraste a las redes totalmente conexas donde los pesos pueden tomar cualquier valor. Esos supuestos están adaptados para explotar la estructura señales, por ejemplo: sonido o imágenes. En estos dos casos, se trata de entradas que tienen una estructura adicional de proximidad (es decir, hay un concepto de pixeles cercanos y lejanos, igual de tiempos cercanos o lejanos). Las redes convolucionales son la arquitectura más exitosa para tratar con este tipo de problemas con estructura espacial o temporal. Hay tres consecuencias básicos que producen el uso de convoluciones, que explicamos primero intuitivamente: Conexiones ralas: existen unidades que solo están conectadas a una fracción relativamente chica de las unidades de la capa anterior (en lugar de todas, como en redes totalmente conexas). Por ejemplo: una unidad que busca detectar una forma en una esquina de una imagen no necesita estar conectada a pixeles de otras partes de la imagen. Parámetros compartidos: diferentes unidades tienen pesos compartidos. Por ejemplo: una unidad que quiere detectar el sonido de cierto animal al principio de la grabación puede utilizar los mismos pesos aplicados a otra parte de la grabación. Podemos “mover” el detector (con los mismos pesos) a lo largo de la grabación para ver en dónde detecta el sonido que nos interesa. Equivarianza: Una translación de una entrada (en tiempo o espacio), produce una traslación equivalente en la salida. Por ejemplo, Si una unidad asociada a la esquina superior derecha de una imagen detecta un número, entonces habrá otra unidad que puede detectar el número en la esquina inferior. 9.1 Filtros convolucionales Filtros en una dimensión Comenzamos por considerar filtros para una serie de tiempo. Un filtro es una transformación de una señal que pretende extraer ciertas características y suprimir otras. Por ejemplo, consideramos la siguiente serie, y promedios móviles centrados de longitud 5. Los promedios móviles filtran las componentes de frecuencia alta (variaciones en tiempos cortos), y nos dejan con la variación de menor frecuencia: library(ggplot2) library(dplyr) library(tidyr) library(RcppRoll) h &lt;- function(x){ifelse(x&gt;0,x,0)} datos &lt;- data_frame(t = 1:length(BJsales), serie = as.numeric(BJsales) + rnorm(length(BJsales), 0, 10)) %&gt;% mutate(promedio_mov = roll_mean(serie, 5, align=&#39;center&#39;, fill = NA)) ggplot(filter(datos, t &lt; 100), aes(x=t, y=serie)) + geom_line() + geom_line(aes(y=promedio_mov), colour=&#39;red&#39;, size=1.2) Podemos escribir este filtro de la siguiente manera: si \\(x_t\\) representa la serie original, y \\(y_t\\) la serie filtrada, entonces \\[ y_t = \\frac{1}{5}(x_{t-2} + x_{t-1} + x_t + x_{t+1}+x_{t+2})\\] Podemos escribir esta operación poniendo \\[f =\\frac{1}{5} (\\ldots, 0,0,1,1,1,1,1,0,0,\\ldots)\\] donde \\(f_s=1/5\\) para \\(s=-2,-1,0,1,2\\) y cero en otro caso. Entonces \\[y_t = \\cdots + x_{t-2}f_{-2} + x_{t-1}f_{-1} + x_{t}f_{0} +x_{t+1}f_{1} +x_{t+2}f_{2}\\] Que también se puede escribir como \\[\\begin{equation} y_t = \\sum_{s=-\\infty}^{\\infty} x_s f_{t-s} \\end{equation}\\] Nótese que estamos moviendo el filtro \\(f\\) a lo largo de la serie (tiempo) y aplicándolo cada vez. Este es un ejemplo de filtro convolucional: es una vector \\(f\\) que se aplica a la serie \\(x\\) como en la ecuación anterior para obtener una serie transformada (filtrada) \\(y\\). Otro ejemplo son las primeras diferencias, que toma valores altos cuando la serie crece y bajos cuando decrece (extrae los incrementos) datos &lt;- datos %&gt;% mutate(dif = promedio_mov - lag(promedio_mov)) ggplot(datos, aes(x=t, y=dif)) + geom_line() + geom_abline(slope=0, intercept=0) ## Warning: Removed 5 rows containing missing values (geom_path). ¿Cuál es el filtro \\(f\\) en este caso? Filtros convolucionales en dos dimensiones En dos dimensiones, nuestro filtro es una matriz \\(f_{i,j}\\), que se aplica a una matriz \\(x_{i,j}\\) (podemos pensar que es una imagen) alrededor de cada posible pixel, para obtener la matriz (imagen) filtrada \\(y_{i,j}\\) dada por \\[\\begin{equation} y_{a,b} = \\sum_{s,t=-\\infty}^{\\infty} x_{s,t} f_{s-a,t-b} \\end{equation}\\] A la matriz \\(f\\) se le llama matriz convolucional, kernel o máscara del filtro Por ejemplo, consideremos el filtro de 3x3 filtro_difuminar &lt;- matrix(rep(1/9,9), 3,3, byrow=T) filtro_difuminar ## [,1] [,2] [,3] ## [1,] 0.1111111 0.1111111 0.1111111 ## [2,] 0.1111111 0.1111111 0.1111111 ## [3,] 0.1111111 0.1111111 0.1111111 El centro de este filtro se sobrepone sobre la cada pixel de la imagen \\(x\\), se multiplican los valores de la imagen por los del filtro y se suma para obtener el nuevo pixel de la imagen \\(y\\). Por ejemplo, si tenemos la imagen ¿Qué efecto tiene este filtro? Este filtro promedia los pixeles de un parche de 3x3 de la imagen, o suaviza la imagen. Es el análogo en 2 dimensiones del filtro de promedios móviles que vimos arriba. library(imager) estatua &lt;- load.image(&#39;imagenes/escultura.jpg&#39;) %&gt;% grayscale plot(estatua) estatua_mat &lt;- as.array(estatua) dim(estatua_mat) ## [1] 174 240 1 1 estatua_dif &lt;- array(0, c(dim(estatua)[1]-1, dim(estatua)[2]-1, 1, 1)) # Ojo: esta manera es muy lenta: si necesitas convoluciones a mano busca # paquetes apropiados for(i in 2:dim(estatua_dif)[1]){ for(j in 2:dim(estatua_dif)[2]){ estatua_dif[i,j,1,1] &lt;- sum(filtro_difuminar*estatua[(i-1):(i+1),(j-1):(j+1),1,1]) } } plot(as.cimg(estatua_dif), axes=FALSE) Podemos intentar otro filtro, que detecta bordes de arriba hacia abajo (es decir, cambios de intensidad que van de bajos a altos conforme bajamos en la imagen): filtro_borde &lt;- (matrix(c(-1,-1,-1,0,0,0,1,1,1), 3,3, byrow=T)) filtro_borde ## [,1] [,2] [,3] ## [1,] -1 -1 -1 ## [2,] 0 0 0 ## [3,] 1 1 1 estatua_filtrada &lt;- array(0, c(dim(estatua_dif)[1]-1, dim(estatua_dif)[2]-1, 1, 1)) for(i in 2:dim(estatua_filtrada)[1]){ for(j in 2:dim(estatua_filtrada)[2]){ estatua_filtrada[i,j,1,1] &lt;- sum(t(filtro_borde)*estatua_dif[(i-1):(i+1),(j-1):(j+1),1,1]) } } plot(as.cimg(estatua_filtrada)) Este filtro toma valores altos cuando hay un gradiente de intensidad de arriba hacia abajo. ¿Cómo harías un filtro que detecta curvas? Considera el siguiente ejemplo, en donde construimos un detector de diagonales: library(keras) mnist &lt;- dataset_mnist() digito &lt;- t(mnist$train$x[10,,]) plot(as.cimg(digito)) filtro_diag &lt;- matrix(rep(-1,25), 5, 5) diag(filtro_diag) &lt;- 2 for(i in 1:4){ filtro_diag[i, i+1] &lt;- 1 filtro_diag[i+1, i] &lt;- 1 } filtro_diag_1 &lt;- filtro_diag[, 5:1] filtro_diag_1 ## [,1] [,2] [,3] [,4] [,5] ## [1,] -1 -1 -1 1 2 ## [2,] -1 -1 1 2 1 ## [3,] -1 1 2 1 -1 ## [4,] 1 2 1 -1 -1 ## [5,] 2 1 -1 -1 -1 digito_f &lt;- array(0, c(dim(digito)[1]-2, dim(digito)[2]-2, 1, 1)) for(i in 3:dim(digito_f)[1]){ for(j in 3:dim(digito_f)[2]){ digito_f[i,j,1,1] &lt;- sum((filtro_diag_1)*digito[(i-2):(i+2),(j-2):(j+2)]) } } plot(as.cimg(digito_f)) 9.2 Filtros convolucionales para redes neuronales En redes neuronales, la idea es que que qeremos aprender estos filtros a partir de los datos. La imagen filtrada nos da las entradas de la siguiente capa. Entonces, supongamos que un filtro de 3x3 está dado por ciertos pesos \\[ f = \\left[ {\\begin{array}{ccccc} \\theta_{1,1} &amp; \\theta_{1,2} &amp; \\theta_{1,3} \\\\ \\theta_{2,1} &amp; \\theta_{2,2} &amp; \\theta_{2,3} \\\\ \\theta_{3,1} &amp; \\theta_{3,2} &amp; \\theta_{3,3} \\\\ \\end{array} } \\right] \\] Este filtro lo aplicaremos a cada parche de la imagen de entrada. Empezamos aplicando el filtro sobre la parte superior izquierda de la imagen para calcular la primera unidad de salida \\(a_1\\) knitr::include_graphics(&#39;./imagenes/conv_1.png&#39;) Ahora nos movemos un pixel a la derecha y aplicamos el filtro para obtener la unidad \\(a_2\\). Podemos poner las unidades en el orden de la imagen para entender mejor las unidades: knitr::include_graphics(&#39;./imagenes/conv_2.png&#39;) Al aplicar el filtro a lo largo de toda la imagen, obtenemos 9 unidades de salida: knitr::include_graphics(&#39;./imagenes/conv_3.png&#39;) Finalmente, podemos agregar más parámetros para otros filtros: knitr::include_graphics(&#39;./imagenes/conv_4.png&#39;) 9.3 Capas de agregación (pooling) En procesamiento de imágenes y redes convolucionales también se utilizan capas de pooling. Estas se encargan de resumir pixeles adyacentes. Una de las más populares es el max pooling, donde en cada parche de la imagen tomamos el máximo. knitr::include_graphics(&#39;./imagenes/pooling_1.png&#39;) Hay dos razones para usar estas agregaciones: Obtener invarianza a translaciones adicional (en un parche de la imagen, solo importa si alguno de las unidades agregadas está activa para que el max-pooling esté activo) Reduce el tamaño de la imagen (o de una capa de convolución) y en consecuencia tenemos menos parámetros que tratar en las siguientes capas 9.4 Ejemplo (arquitectura LeNet): Las capas de pooling generalmente se aplican después de las convoluciones, y hacia al final usamos capas totalmente conexas. Estas últimas capas se encargan de combinar la información de las capas de convolución anteriores, que detectan patrones simples, para obtener unidades que se encargan de detectar patrones más complejos. knitr::include_graphics(&#39;./imagenes/lenet_1.png&#39;) ## [1] 7291 257 ## ## 0 1 2 3 4 5 6 7 8 9 ## 1194 1005 731 658 652 556 664 645 542 644 Ponemos el rango entre [0,1] (pixeles positivos) y usamos codificación dummy x_train &lt;- digitos_entrena %&gt;% select(contains(&#39;pixel&#39;)) %&gt;% as.matrix + 1 x_train &lt;- x_train/2 dim(x_train) &lt;- c(nrow(x_train), 16, 16, 1) x_test &lt;- digitos_prueba %&gt;% select(contains(&#39;pixel&#39;)) %&gt;% as.matrix + 1 x_test &lt;- x_test/2 dim(x_test) &lt;- c(nrow(x_test), 16, 16, 1) y_train &lt;- to_categorical(digitos_entrena$digito, 10) y_test &lt;- to_categorical(digitos_prueba$digito, 10) Para fines de interpretación, agregaremos regularización ridge además de dropout (puedes obtener buen desempeño usando solamente dropout): usar_cache &lt;- TRUE if(!usar_cache){ set.seed(213) model_2 &lt;- keras_model_sequential() model_2 %&gt;% layer_conv_2d(filters = 8, kernel_size = c(5,5), activation = &#39;relu&#39;, input_shape = c(16,16,1), padding =&#39;same&#39;, kernel_regularizer = regularizer_l2(0.01) ) %&gt;% layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;% layer_dropout(rate = 0.25) %&gt;% layer_conv_2d(filters = 12, kernel_size = c(3,3), activation = &#39;relu&#39;, kernel_regularizer = regularizer_l2(0.01)) %&gt;% layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;% layer_dropout(rate = 0.25) %&gt;% layer_flatten() %&gt;% layer_dense(units = 100, activation = &#39;relu&#39;) %&gt;% layer_dropout(rate = 0.5) %&gt;% layer_dense(units = 10, activation = &#39;softmax&#39;) model_2 %&gt;% compile( loss = &#39;categorical_crossentropy&#39;, optimizer = optimizer_sgd(lr = 0.05, momentum = 0.5), metrics = c(&#39;accuracy&#39;,&#39;categorical_crossentropy&#39;) ) history &lt;- model_2 %&gt;% fit( x_train, y_train, epochs = 200, batch_size = 256, validation_data = list(x_test, y_test) ) model_serialized &lt;- serialize_model(model_2) saveRDS(model_serialized, file= &#39;cache_obj/red_conv_ser.rds&#39;) } else { model_serialized &lt;- readRDS(file = &#39;cache_obj/red_conv_ser.rds&#39;) model_2 &lt;- unserialize_model(model_serialized) } score &lt;- model_2 %&gt;% evaluate(x_test, y_test) score ## $loss ## [1] 0.2057457 ## ## $acc ## [1] 0.959143 ## ## $categorical_crossentropy ## [1] 0.1473516 score_entrena &lt;- model_2 %&gt;% evaluate(x_train, y_train) score_entrena ## $loss ## [1] 0.1045686 ## ## $acc ## [1] 0.9877932 ## ## $categorical_crossentropy ## [1] 0.04617448 Y ahora graficamos los filtros aprendidos en la primera capa: library(scales) ## ## Attaching package: &#39;scales&#39; ## The following object is masked from &#39;package:readr&#39;: ## ## col_factor wts &lt;- get_weights(model_2) capa_1 &lt;- wts[[1]] capa_list &lt;- lapply(1:8, function(i){ data_frame(val = as.numeric(t(capa_1[,,1,i])), pixel = 1:25, unidad=i) }) %&gt;% bind_rows %&gt;% mutate(y = (pixel-1) %% 5, x = (pixel-1) %/% 5) %&gt;% group_by(unidad) %&gt;% mutate(val = (val-mean(val))/sd(val)) capa_list ## # A tibble: 200 x 5 ## # Groups: unidad [8] ## val pixel unidad y x ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.4653404 1 1 0 0 ## 2 -0.9698185 2 1 1 0 ## 3 1.0867707 3 1 2 0 ## 4 1.5496123 4 1 3 0 ## 5 0.6970253 5 1 4 0 ## 6 -1.6848534 6 1 0 1 ## 7 -1.4587731 7 1 1 1 ## 8 0.8354561 8 1 2 1 ## 9 1.3157807 9 1 3 1 ## 10 0.1034943 10 1 4 1 ## # ... with 190 more rows ggplot(capa_list, aes(x=x, y=-y)) + geom_raster(aes(fill=val), interpolate=FALSE) + facet_wrap(~unidad, ncol=4) + coord_equal()+scale_fill_gradient2(low = &quot;black&quot;, mid=&#39;gray50&#39;,high = &quot;white&quot;) Podemos ver las activaciones de la primera capa para algunos dígitos (después de pooling): capa &lt;- keras_model_sequential() capa %&gt;% layer_conv_2d(filters = 8, kernel_size = c(5,5), activation = &#39;relu&#39;, input_shape = c(16,16,1), padding=&#39;same&#39;,weights = wts[1:2]) %&gt;% layer_max_pooling_2d() probas &lt;- predict_proba(capa, x_train[1:50,,,,drop=FALSE]) graf_activaciones &lt;- function(probas, ind){ probas_ind &lt;- probas[ind,,,] unidades_df &lt;- lapply(1:dim(probas_ind)[3], function(i){ mat &lt;- t(probas_ind[,,i]) data_frame(val = as.numeric(mat), pixel = 1:(8*8), unidad=i) %&gt;% mutate(y = (pixel-1) %% 8, x = (pixel-1) %/% 8) %&gt;% group_by(unidad) %&gt;% mutate(val=(val-mean(val))/sd(val)) }) dat &lt;- bind_rows(unidades_df) ggplot(dat, aes(x=x, y=-y, fill=val)) + geom_tile() + facet_wrap(~unidad) + scale_fill_gradient2(low = &quot;black&quot;, mid=&#39;gray20&#39;,high = &quot;white&quot;) + coord_equal() } graf_activaciones(probas, 4) graf_activaciones(probas, 5) graf_activaciones(probas, 15) graf_activaciones(probas, 8) graf_activaciones(probas, 33) #image((x_train[4,1:16,16:1,1])) Y los filtros aprendidos en la segunda capa: capa_2 &lt;- wts[[3]] out &lt;- list() for(j in 1:8){ out_temp &lt;- list() for(i in 1:12){ dat_lay &lt;- data_frame(val = as.numeric(capa_2[,,j,i]), pixel = 1:9, unidad=i, otra=j) %&gt;% mutate(y = (pixel-1) %% 3, x = (pixel-1) %/% 3) out_temp[[i]] &lt;- dat_lay } out[[j]] &lt;- bind_rows(out_temp) } capa_out &lt;- bind_rows(out) ggplot(capa_out, aes(x=x, y=-y)) + geom_tile(aes(fill=val)) + facet_grid(otra~unidad) + coord_equal()+scale_fill_gradient2(low = &quot;black&quot;, mid=&#39;gray50&#39;,high = &quot;white&quot;) + theme(strip.background = element_blank(), strip.text = element_blank()) "],
["diagnostico-y-mejora-de-modelos.html", "Clase 10 Diagnóstico y mejora de modelos 10.1 Aspectos generales 10.2 ¿Qué hacer cuando el desempeño no es satisfactorio? 10.3 Pipeline de procesamiento 10.4 Diagnósticos: sesgo y varianza 10.5 Refinando el pipeline 10.6 Consiguiendo más datos 10.7 Usar datos adicionales 10.8 Examen de modelo y Análisis de errores", " Clase 10 Diagnóstico y mejora de modelos 10.1 Aspectos generales Al comenzar un proyecto de machine learning, las primeras consideraciones deben ser: Establecer métricas de error apropiadas para el problema, y cuál es el máximo valor de este error requerido para nuestra aplicación. Construir un pipeline lo antes posible que vaya de datos hasta medición de calidad de los modelos. Este pipeline deberá, al menos, incluir cálculos de entradas, medición de desempeño de los modelos y cálculos de otros diagnósticos (como error de entrenamiento, convergencia de algoritmos, etc.) En general, es difícil preveer exactamente qué va a funcionar para un problema particular, y los diagnósticos que veremos requieren de haber ajustado modelos. Nuestra primera recomendación para ir hacia un modelo de mejor desempeño es: Es mejor y más rápido comenzar rápido, aún con un modelo simple, con entradas {} (no muy refinadas), y con los datos que tenemos a mano. De esta forma podemos aprender más rápido. Demasiado tiempo pensando, discutiendo, o diseñando qué algoritmo deberíamos usar, cómo deberíamos construir las entradas, etc. es muchas veces tiempo perdido. Con el pipeline establecido, si el resultado no es satisfactorio, entonces tenemos que tomar decisiones para mejorar. 10.2 ¿Qué hacer cuando el desempeño no es satisfactorio? Supongamos que tenemos un clasificador construido con regresión logística regularizada, y que cuando lo aplicamos a nuestra muestra de prueba el desempeño es malo. ¿Qué hacer? Algunas opciones: Conseguir más datos de entrenamiento. Reducir el número de entradas por algún método (eliminación manual, componentes principales, etc.) Construir más entradas utilizando distintos enfoques o fuentes de datos. Incluir variables derivadas adicionales e interacciones. Intentar construir una red neuronal para predecir (otro método). Aumentar la regularización. Disminuir la regularización. Correr más tiempo el algoritmo de ajuste. ¿Con cuál empezar? Cada una de estas estrategias intenta arreglar distintos problemas. En lugar de intentar al azar distintas cosas, que consumen tiempo y dinero y no necesariamente nos van a llevar a mejoras, a continuación veremos diagnósticos y recetas que nos sugieren la mejor manera de usar nuestro tiempo para mejorar nuestros modelos. Usaremos el siguiente ejemplo para ilustrar los conceptos: Ejemplo Nos interesa hacer una predicción de polaridad de críticas o comentarios de pelíıculas: buscamos clasificar una reseña como positiva o negativa dependiendo de su contenido. Tenemos dos grupos de reseñas separadas en positivas y negativas (estos datos fueron etiquetados por una persona). Cada reseña está un archivo de texto, y tenemos 1000 de cada tipo: negativos &lt;- list.files(&#39;./datos/sentiment/neg&#39;, full.names = TRUE) positivos &lt;- list.files(&#39;./datos/sentiment/pos&#39;, full.names = TRUE) head(negativos) ## [1] &quot;./datos/sentiment/neg/cv000_29416.txt&quot; ## [2] &quot;./datos/sentiment/neg/cv001_19502.txt&quot; ## [3] &quot;./datos/sentiment/neg/cv002_17424.txt&quot; ## [4] &quot;./datos/sentiment/neg/cv003_12683.txt&quot; ## [5] &quot;./datos/sentiment/neg/cv004_12641.txt&quot; ## [6] &quot;./datos/sentiment/neg/cv005_29357.txt&quot; head(positivos) ## [1] &quot;./datos/sentiment/pos/cv000_29590.txt&quot; ## [2] &quot;./datos/sentiment/pos/cv001_18431.txt&quot; ## [3] &quot;./datos/sentiment/pos/cv002_15918.txt&quot; ## [4] &quot;./datos/sentiment/pos/cv003_11664.txt&quot; ## [5] &quot;./datos/sentiment/pos/cv004_11636.txt&quot; ## [6] &quot;./datos/sentiment/pos/cv005_29443.txt&quot; length(negativos) ## [1] 1000 length(positivos) ## [1] 1000 read_file(negativos[1]) [1] “plot : two teen couples go to a church party , drink and then drive . get into an accident . of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . ’s the deal ? the movie and &quot; sorta &quot; find out . . . : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . is what makes this review an even harder one to write , since i generally applaud films which attempt to break the mold , mess with your head and such ( lost highway &amp; memento ) , but there are good and bad ways of making all types of films , and these folks just didn’t snag this one correctly . seem to have taken this pretty neat concept , but executed it terribly . what are the problems with the movie ? , its main problem is that it’s simply too jumbled . starts off &quot; normal &quot; but then downshifts into this &quot; fantasy &quot; world in which you , as an audience member , have no idea what’s going on . are dreams , there are characters coming back from the dead , there are others who look like the dead , there are strange apparitions , there are disappearances , there are a looooot of chase scenes , there are tons of weird things that happen , and most of it is simply not explained . i personally don’t mind trying to unravel a film every now and then , but when all it does is give me the same clue over and over again , i get kind of fed up after a while , which is this film’s biggest problem . ’s obviously got this big secret to hide , but it seems to want to hide it completely until its final five minutes . do they make things entertaining , thrilling or even engaging , in the meantime ? really . sad part is that the arrow and i both dig on flicks like this , so we actually figured most of it out by the half-way point , so all of the strangeness after that did start to make a little bit of sense , but it still didn’t the make the film all that more entertaining . guess the bottom line with movies like this is that you should always make sure that the audience is &quot; into it &quot; even before they are given the secret password to enter your world of understanding . mean , showing melissa sagemiller running away from visions for about 20 minutes throughout the movie is just plain lazy ! ! , we get it . . . there people chasing her and we don’t know who they are . we really need to see it over and over again ? about giving us different scenes offering further insight into all of the strangeness going down in the movie ? , the studio took this film away from its director and chopped it up themselves , and it shows . might’ve been a pretty decent teen mind-fuck movie in here somewhere , but i guess &quot; the suits &quot; decided that turning it into a music video with little edge , would make more sense . actors are pretty good for the most part , although wes bentley just seemed to be playing the exact same character that he did in american beauty , only in a new neighborhood . my biggest kudos go out to sagemiller , who holds her own throughout the entire film , and actually has you feeling her character’s unraveling . , the film doesn’t stick because it doesn’t entertain , it’s confusing , it rarely excites and it feels pretty redundant for most of its runtime , despite a pretty cool ending and explanation to all of the craziness that came before it . , and by the way , this is not a horror or teen slasher flick . . . it’s packaged to look that way because someone is apparently assuming that the genre is still hot with the kids . also wrapped production two years ago and has been sitting on the shelves ever since . . . . skip ! ’s joblo coming from ? nightmare of elm street 3 ( 7/10 ) - blair witch 2 ( 7/10 ) - the crow ( 9/10 ) - the crow : salvation ( 4/10 ) - lost highway ( 10/10 ) - memento ( 10/10 ) - the others ( 9/10 ) - stir of echoes ( 8/10 ) ” read_file(positivos[1]) [1] “films adapted from comic books have had plenty of success , whether they’re about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there’s never really been a comic book like from hell before . starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid ‘80s with a 12-part series called the watchmen . say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd . book ( or &quot; graphic novel , &quot; if you will ) is over 500 pages long and includes nearly 30 more that consist of nothing but footnotes . other words , don’t dismiss this film because of its source . you can get past the whole comic book thing , you might find another stumbling block in from hell’s directors , albert and allen hughes . the hughes brothers to direct this seems almost as ludicrous as casting carrot top in , well , anything , but riddle me this : who better to direct a film that’s set in the ghetto and features really violent street crime than the mad geniuses behind menace ii society ? ghetto in question is , of course , whitechapel in 1888 london’s east end . ’s a filthy , sooty place where the whores ( called &quot; unfortunates &quot; ) are starting to get a little nervous about this mysterious psychopath who has been carving through their profession with surgical precision . the first stiff turns up , copper peter godley ( robbie coltrane , the world is not enough ) calls in inspector frederick abberline ( johnny depp , blow ) to crack the case . , a widower , has prophetic dreams he unsuccessfully tries to quell with copious amounts of absinthe and opium . arriving in whitechapel , he befriends an unfortunate named mary kelly ( heather graham , say it isn’t so ) and proceeds to investigate the horribly gruesome crimes that even the police surgeon can’t stomach . don’t think anyone needs to be briefed on jack the ripper , so i won’t go into the particulars here , other than to say moore and campbell have a unique and interesting theory about both the identity of the killer and the reasons he chooses to slay . the comic , they don’t bother cloaking the identity of the ripper , but screenwriters terry hayes ( vertical limit ) and rafael yglesias ( les mis ? rables ) do a good job of keeping him hidden from viewers until the very end . ’s funny to watch the locals blindly point the finger of blame at jews and indians because , after all , an englishman could never be capable of committing such ghastly acts . from hell’s ending had me whistling the stonecutters song from the simpsons for days ( &quot; who holds back the electric car/who made steve guttenberg a star ? &quot; ) . ’t worry - it’ll all make sense when you see it . onto from hell’s appearance : it’s certainly dark and bleak enough , and it’s surprising to see how much more it looks like a tim burton film than planet of the apes did ( at times , it seems like sleepy hollow 2 ) . print i saw wasn’t completely finished ( both color and music had not been finalized , so no comments about marilyn manson ) , but cinematographer peter deming ( don’t say a word ) ably captures the dreariness of victorian-era london and helped make the flashy killing scenes remind me of the crazy flashbacks in twin peaks , even though the violence in the film pales in comparison to that in the black-and-white comic . winner martin childs’ ( shakespeare in love ) production design turns the original prague surroundings into one creepy place . the acting in from hell is solid , with the dreamy depp turning in a typically strong performance and deftly handling a british accent . holm ( joe gould’s secret ) and richardson ( 102 dalmatians ) log in great supporting roles , but the big surprise here is graham . cringed the first time she opened her mouth , imagining her attempt at an irish accent , but it actually wasn’t half bad . film , however , is all good . 2 : 00 - r for strong violence/gore , sexuality , language and drug content ” Consideremos primero la métrica de error, que depende de nuestra aplicación. En este caso, quisiéramos hacer dar una calificación a cada película basada en el % de reseñas positivas que tiene. Supongamos que se ha decidido que necesitamos al menos una tasa de correctos de 90% para que el score sea confiable (cómo calcularías algo así?). Ahora necesitamos construir un pipeline para obtener las primeras predicciones. Tenemos que pensar qué entradas podríamos construir. 10.3 Pipeline de procesamiento Empezamos por construir funciones para leer datos (ver script). Construimos un data frame: source(&#39;./scripts/funciones_sentiment.R&#39;) df &lt;- prep_df(&#39;./datos/sentiment/&#39;) %&gt;% unnest(texto) nrow(df) [1] 2000 df$texto[1] [1] “Review films adapted from comic books have had plenty of success , whether they’re about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there’s never really been a comic book like from hell before . for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid ‘80s with a 12-part series called the watchmen . to say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd . the book ( or &quot; graphic novel , &quot; if you will ) is over 500 pages long and includes nearly 30 more that consist of nothing but footnotes . in other words , don’t dismiss this film because of its source . if you can get past the whole comic book thing , you might find another stumbling block in from hell’s directors , albert and allen hughes . getting the hughes brothers to direct this seems almost as ludicrous as casting carrot top in , well , anything , but riddle me this : who better to direct a film that’s set in the ghetto and features really violent street crime than the mad geniuses behind menace ii society ? the ghetto in question is , of course , whitechapel in 1888 london’s east end . it’s a filthy , sooty place where the whores ( called &quot; unfortunates &quot; ) are starting to get a little nervous about this mysterious psychopath who has been carving through their profession with surgical precision . when the first stiff turns up , copper peter godley ( robbie coltrane , the world is not enough ) calls in inspector frederick abberline ( johnny depp , blow ) to crack the case . abberline , a widower , has prophetic dreams he unsuccessfully tries to quell with copious amounts of absinthe and opium . upon arriving in whitechapel , he befriends an unfortunate named mary kelly ( heather graham , say it isn’t so ) and proceeds to investigate the horribly gruesome crimes that even the police surgeon can’t stomach . i don’t think anyone needs to be briefed on jack the ripper , so i won’t go into the particulars here , other than to say moore and campbell have a unique and interesting theory about both the identity of the killer and the reasons he chooses to slay . in the comic , they don’t bother cloaking the identity of the ripper , but screenwriters terry hayes ( vertical limit ) and rafael yglesias ( les mis ? rables ) do a good job of keeping him hidden from viewers until the very end . it’s funny to watch the locals blindly point the finger of blame at jews and indians because , after all , an englishman could never be capable of committing such ghastly acts . and from hell’s ending had me whistling the stonecutters song from the simpsons for days ( &quot; who holds back the electric car/who made steve guttenberg a star ? &quot; ) . don’t worry - it’ll all make sense when you see it . now onto from hell’s appearance : it’s certainly dark and bleak enough , and it’s surprising to see how much more it looks like a tim burton film than planet of the apes did ( at times , it seems like sleepy hollow 2 ) . the print i saw wasn’t completely finished ( both color and music had not been finalized , so no comments about marilyn manson ) , but cinematographer peter deming ( don’t say a word ) ably captures the dreariness of victorian-era london and helped make the flashy killing scenes remind me of the crazy flashbacks in twin peaks , even though the violence in the film pales in comparison to that in the black-and-white comic . oscar winner martin childs’ ( shakespeare in love ) production design turns the original prague surroundings into one creepy place . even the acting in from hell is solid , with the dreamy depp turning in a typically strong performance and deftly handling a british accent . ians holm ( joe gould’s secret ) and richardson ( 102 dalmatians ) log in great supporting roles , but the big surprise here is graham . i cringed the first time she opened her mouth , imagining her attempt at an irish accent , but it actually wasn’t half bad . the film , however , is all good . 2 : 00 - r for strong violence/gore , sexuality , language and drug content” Ahora separamos una muestra de prueba (y una de entrenamiento más chica para simular después el proceso de recoger más datos): set.seed(94512) df$muestra &lt;- sample(c(&#39;entrena&#39;, &#39;prueba&#39;), 2000, prob = c(0.8, 0.2), replace = TRUE) table(df$muestra) ## ## entrena prueba ## 1575 425 df_ent &lt;- df %&gt;% filter(muestra == &#39;entrena&#39;) df_pr &lt;- df %&gt;% filter(muestra == &#39;prueba&#39;) df_ent &lt;- sample_n(df_ent, nrow(df_ent)) #permutamos al azar df_ent_grande &lt;- df_ent df_ent &lt;- df_ent %&gt;% sample_n(700) Intentemos algo simple para empezar: consideramos qué palabras contiene cada reseña, e intentamos clasificar en base esas palabras. Así que en primer lugar dividimos cada texto en tokens (pueden ser palabras, o sucesiones de caracteres o de palabras de tamaño fijo (n-gramas), oraciones, etc.). En este caso, usamos el paquete tidytext. La función unnest_tokens elimina signos de puntuación, convierte todo a minúsculas, y separa las palabras: Vamos a calcular los tokens y ordernarlos por frecuencia. Empezamos calculando nuestro vocabulario. Supongamos que usamos las 50 palabras más comunes, y usamos poca regularización: vocabulario &lt;- calc_vocabulario(df_ent, 50) head(vocabulario) ## # A tibble: 6 x 2 ## palabra frec ## &lt;chr&gt; &lt;int&gt; ## 1 a 12904 ## 2 about 1228 ## 3 all 1464 ## 4 an 2000 ## 5 and 12173 ## 6 are 2359 tail(vocabulario) ## # A tibble: 6 x 2 ## palabra frec ## &lt;chr&gt; &lt;int&gt; ## 1 what 1006 ## 2 when 1091 ## 3 which 1153 ## 4 who 1870 ## 5 with 3705 ## 6 you 1565 Todas las etapas de preprocesamiento deben hacerse en función de los datos de entrenamiento. En este ejemplo, podríamos cometer el error de usar todos los datos para calcular el vocabulario. Nuestras entradas aquí no se ven muy buenas: los términos más comunes son en su mayoría palabras sin significado, de modo que no esperamos un desempeño muy bueno. En este momento no nos preocupamos mucho por eso, queremos correr los primeros modelos. library(glmnet) mod_x &lt;- correr_modelo(df_ent, df_pr, vocabulario, lambda = 1e-1) ## [1] &quot;Error entrenamiento: 0.31&quot; ## [1] &quot;Error prueba: 0.36&quot; ## [1] &quot;Devianza entrena:1.148&quot; ## [1] &quot;Devianza prueba:1.271&quot; 10.4 Diagnósticos: sesgo y varianza Y notamos que El error de entrenamiento no es satisfactorio: está muy por arriba de nuestro objetivo (10%) Hay algo de brecha entre entrenamiento y prueba, de modo que disminuir varianza puede ayudar. ¿Qué hacer? Nuestro clasificador ni siquiera puede clasificar bien la muestra de entrenamiento, lo que implica que nuestro modelo tiene sesgo alto. Controlar la varianza no nos va a ayudar a resolver nuestro problema en este punto. Podemos intentar un modelo más flexible. Error de entrenamiento demasiado alto indica que necesitamos probar con modelos más flexibles (disminuir el sesgo). Para disminuir el sesgo podemos: Expander el vocabulario (agregar más entradas) Crear nuevas entradas a partir de los datos (más informativas) Usar un método más flexible (como redes neuronales) Regularizar menos Cosas que no van a funcionar (puede bajar un poco el error de validación, pero el error de entrenamiento es muy alto): Conseguir más datos de entrenamiento (el error de entrenamiento va a subir, y el de validación va a quedar muy arriba, aunque disminuya) Regularizar más (misma razón) Usar un vocabulario más chico, eliminar entradas (misma razón) Por ejemplo, si juntáramos más datos de entrenamiento (con el costo que esto implica), obtendríamos: mod_x &lt;- correr_modelo(df_ent_grande, df_pr, vocabulario, lambda = 1e-1) ## Joining, by = &quot;palabra&quot; ## Joining, by = &quot;palabra&quot; ## [1] &quot;Error entrenamiento: 0.31&quot; ## [1] &quot;Error prueba: 0.35&quot; ## [1] &quot;Devianza entrena:1.187&quot; ## [1] &quot;Devianza prueba:1.246&quot; Vemos que aunque bajó ligeramente el error de prueba, el error es demasiado alto. Esta estrategia no funcionó con este modelo, y hubiéramos perdido tiempo y dinero (por duplicar el tamaño de muestra) sin obtener mejoras apreciables. Observación: el error de entrenamiento subió. ¿Puedes explicar eso? Esto sucede porque típicamente el error para cada caso individual de la muestra original sube, pues la optimización se hace sobre más casos. Es más difícil ajustar los datos de entrenamiento cuando tenemos más datos. En lugar de eso, podemos comenzar quitando regularización, por ejemplo mod_x &lt;- correr_modelo(df_ent, df_pr, vocabulario, lambda =1e-10) ## Joining, by = &quot;palabra&quot; ## Joining, by = &quot;palabra&quot; ## [1] &quot;Error entrenamiento: 0.29&quot; ## [1] &quot;Error prueba: 0.37&quot; ## [1] &quot;Devianza entrena:1.099&quot; ## [1] &quot;Devianza prueba:1.32&quot; Y notamos que reducimos un poco el sesgo. Por el momento, seguiremos intentando reducir sesgo. Podemos ahora incluir más variables vocabulario &lt;- calc_vocabulario(df_ent, 3000) mod_x &lt;- correr_modelo(df_ent, df_pr, vocabulario, lambda=1e-10) ## Joining, by = &quot;palabra&quot; ## Joining, by = &quot;palabra&quot; ## [1] &quot;Error entrenamiento: 0&quot; ## [1] &quot;Error prueba: 0.38&quot; ## [1] &quot;Devianza entrena:0&quot; ## [1] &quot;Devianza prueba:7.66&quot; El sesgo ya no parece ser un problema: Ahora tenemos un problema de varianza. Una brecha grande entre entrenamiento y validación muchas veces indica sobreajuste (el problema es varianza). Podemos regularizar más: mod_x &lt;- correr_modelo(df_ent, df_pr, vocabulario, lambda=1e-6) ## Joining, by = &quot;palabra&quot; ## Joining, by = &quot;palabra&quot; ## [1] &quot;Error entrenamiento: 0&quot; ## [1] &quot;Error prueba: 0.3&quot; ## [1] &quot;Devianza entrena:0&quot; ## [1] &quot;Devianza prueba:3.203&quot; mod_x &lt;- correr_modelo(df_ent, df_pr, vocabulario, lambda=0.05) ## Joining, by = &quot;palabra&quot; ## Joining, by = &quot;palabra&quot; ## [1] &quot;Error entrenamiento: 0&quot; ## [1] &quot;Error prueba: 0.18&quot; ## [1] &quot;Devianza entrena:0.072&quot; ## [1] &quot;Devianza prueba:0.764&quot; 10.5 Refinando el pipeline Refinar el pipeline para producir mejores entradas, o corridas más rápidas, generalmente es una buena inversión de tiempo (aunque es mejor no hacerlo prematuramente). El error de entrenamiento es satisfactorio todavía, y nos estamos acercando a nuestro objetivo (intenta regularizar más para verificar que el problema ahora es sesgo). En este punto, podemos intentar reducir varianza (reducir error de prueba con algún incremento en error de entrenamiento). Buscar más casos de entrenamiento: si son baratos, esto podría ayudar (aumentar al doble o 10 veces más). Redefinir entradas más informativas, para reducir el número de variables pero al mismo tiempo no aumentar el sesgo. Intentaremos por el momento el segundo camino (reducción de varianza). Podemos intentar tres cosas: Eliminar los términos que son demasiado frecuentes (son palabras no informativas, como the, a, he, she, etc.). Esto podría reducir varianza sin afectar mucho el sesgo. Usar raíces de palabras en lugar de palabras (por ejemplo, transfomar defect, defects, defective -&gt; defect y boring,bored, bore -&gt; bore, etc.). De esta manera, controlamos la proliferación de entradas que indican lo mismo y aumentan varianza - y quizá el sesgo no aumente mucho. Intentar usar bigramas - esto reduce el sesgo, pero quizá la varianza no aumente mucho. data(&quot;stop_words&quot;) head(stop_words) ## # A tibble: 6 x 2 ## word lexicon ## &lt;chr&gt; &lt;chr&gt; ## 1 a SMART ## 2 a&#39;s SMART ## 3 able SMART ## 4 about SMART ## 5 above SMART ## 6 according SMART head(calc_vocabulario(df_ent, 100)) ## # A tibble: 6 x 2 ## palabra frec ## &lt;chr&gt; &lt;int&gt; ## 1 a 12904 ## 2 about 1228 ## 3 after 569 ## 4 all 1464 ## 5 also 704 ## 6 an 2000 head(calc_vocabulario(df_ent, 100, remove_stop = TRUE)) ## # A tibble: 6 x 2 ## palabra frec ## &lt;chr&gt; &lt;int&gt; ## 1 2 179 ## 2 acting 224 ## 3 action 418 ## 4 actor 165 ## 5 actors 256 ## 6 american 193 vocabulario &lt;- calc_vocabulario(df_ent, 2000, remove_stop = TRUE) head(vocabulario %&gt;% arrange(desc(frec)),20) ## # A tibble: 20 x 2 ## palabra frec ## &lt;chr&gt; &lt;int&gt; ## 1 film 2991 ## 2 movie 1844 ## 3 time 797 ## 4 review 788 ## 5 story 749 ## 6 character 639 ## 7 characters 631 ## 8 life 527 ## 9 films 515 ## 10 plot 490 ## 11 bad 484 ## 12 people 484 ## 13 scene 482 ## 14 movies 455 ## 15 scenes 443 ## 16 action 418 ## 17 director 413 ## 18 love 393 ## 19 real 329 ## 20 world 323 tail(vocabulario %&gt;% arrange(desc(frec)),20) ## # A tibble: 20 x 2 ## palabra frec ## &lt;chr&gt; &lt;int&gt; ## 1 shock 18 ## 2 sir 18 ## 3 sleep 18 ## 4 sole 18 ## 5 spot 18 ## 6 stays 18 ## 7 stereotypical 18 ## 8 strip 18 ## 9 supergirl 18 ## 10 taylor 18 ## 11 threat 18 ## 12 thrillers 18 ## 13 tradition 18 ## 14 tree 18 ## 15 trial 18 ## 16 trio 18 ## 17 triumph 18 ## 18 visit 18 ## 19 warning 18 ## 20 werewolf 18 Este vocabulario parece que puede ser más útil. Vamos a tener que ajustar la regularización de nuevo (y también el número de entradas). Nota: este proceso también lo podemos hacer con cv.glmnet de manera más rápida. require(doMC) ## Loading required package: doMC ## Loading required package: iterators ## Loading required package: parallel registerDoMC(cores=4) if(!usar_cache){ mod_x &lt;- correr_modelo_cv(df_ent, df_pr, vocabulario, lambda = exp(seq(-10,5,0.1))) saveRDS(mod_x, file = &#39;./cache_obj/mod_sentiment_1.rds&#39;) } else { mod_x &lt;- readRDS(&#39;./cache_obj/mod_sentiment_1.rds&#39;) describir_modelo_cv(mod_x) } ## [1] &quot;Lambda min: 0.201896517994655&quot; ## [1] &quot;Error entrenamiento: 0&quot; ## [1] &quot;Error prueba: 0.21&quot; ## [1] &quot;Devianza entrena:0.261&quot; ## [1] &quot;Devianza prueba:0.879&quot; #mod_x &lt;- correr_modelo(df_ent, df_pr, vocabulario, lambda =1) #mod_x &lt;- correr_modelo(df_ent, df_pr, vocabulario, lambda =0.1) #mod_x &lt;- correr_modelo(df_ent, df_pr, vocabulario, lambda =0.01) No estamos mejorando. Podemos intentar con un número diferente de entradas: vocabulario &lt;- calc_vocabulario(df_ent, 4000, remove_stop = TRUE) if(!usar_cache){ mod_x &lt;- correr_modelo_cv(df_ent, df_pr, vocabulario, lambda = exp(seq(-10,5,0.1))) saveRDS(mod_x, file = &#39;./cache_obj/mod_sentiment_2.rds&#39;) } else { mod_x &lt;- readRDS(&#39;./cache_obj/mod_sentiment_2.rds&#39;) describir_modelo_cv(mod_x) } ## [1] &quot;Lambda min: 0.49658530379141&quot; ## [1] &quot;Error entrenamiento: 0&quot; ## [1] &quot;Error prueba: 0.18&quot; ## [1] &quot;Devianza entrena:0.295&quot; ## [1] &quot;Devianza prueba:0.883&quot; Y parece que nuestra estrategia no está funcionando muy bien. Regresamos a nuestro modelo con ridge vocabulario &lt;- calc_vocabulario(df_ent, 3000, remove_stop = FALSE) if(!usar_cache){ mod_x &lt;- correr_modelo_cv(df_ent, df_pr, vocabulario, lambda = exp(seq(-5,2,0.1))) saveRDS(mod_x, file = &#39;./cache_obj/mod_sentiment_3.rds&#39;) } else { mod_x &lt;- readRDS(&#39;./cache_obj/mod_sentiment_3.rds&#39;) describir_modelo_cv(mod_x) } ## [1] &quot;Lambda min: 0.110803158362334&quot; ## [1] &quot;Error entrenamiento: 0&quot; ## [1] &quot;Error prueba: 0.18&quot; ## [1] &quot;Devianza entrena:0.128&quot; ## [1] &quot;Devianza prueba:0.775&quot; Podemos intentar aumentar el número de palabras y aumentar también la regularización vocabulario &lt;- calc_vocabulario(df_ent, 4000, remove_stop = FALSE) if(!usar_cache){ mod_x &lt;- correr_modelo_cv(df_ent, df_pr, vocabulario, lambda = exp(seq(-5,2,0.1))) saveRDS(mod_x, file = &#39;./cache_obj/mod_sentiment_4.rds&#39;) } else { mod_x &lt;- readRDS(&#39;./cache_obj/mod_sentiment_4.rds&#39;) describir_modelo_cv(mod_x) } ## [1] &quot;Lambda min: 0.22313016014843&quot; ## [1] &quot;Error entrenamiento: 0&quot; ## [1] &quot;Error prueba: 0.16&quot; ## [1] &quot;Devianza entrena:0.173&quot; ## [1] &quot;Devianza prueba:0.776&quot; 10.6 Consiguiendo más datos Si nuestro problema es varianza, conseguir más datos de entrenamiento puede ayudarnos, especialmente si producir estos datos es relativamente barato y rápido. Como nuestro principal problema es varianza, podemos mejorar buscando más datos. Supongamos que hacemos eso en este caso, conseguimos el doble casos de entrenamiento. En este ejemplo, podríamos etiquetar más reviews: esto es relativamente barato y rápido vocabulario &lt;- calc_vocabulario(df_ent_grande, 3000, remove_stop = FALSE) if(!usar_cache){ mod_x &lt;- correr_modelo_cv(df_ent_grande, df_pr, vocabulario, lambda = exp(seq(-5,2,0.1))) saveRDS(mod_x, file = &#39;./cache_obj/mod_sentiment_5.rds&#39;) } else { mod_x &lt;- readRDS(&#39;./cache_obj/mod_sentiment_5.rds&#39;) describir_modelo_cv(mod_x) } ## [1] &quot;Lambda min: 0.0907179532894125&quot; ## [1] &quot;Error entrenamiento: 0&quot; ## [1] &quot;Error prueba: 0.12&quot; ## [1] &quot;Devianza entrena:0.18&quot; ## [1] &quot;Devianza prueba:0.653&quot; Y ya casi logramos nuestro objetivo. Podemos intentar con más palabras vocabulario &lt;- calc_vocabulario(df_ent_grande, 4000, remove_stop = FALSE) if(!usar_cache){ mod_x &lt;- correr_modelo_cv(df_ent_grande, df_pr, vocabulario, lambda = exp(seq(-5,2,0.1))) saveRDS(mod_x, file = &#39;./cache_obj/mod_sentiment_6.rds&#39;) } else { mod_x &lt;- readRDS(&#39;./cache_obj/mod_sentiment_6.rds&#39;) describir_modelo_cv(mod_x) } ## [1] &quot;Lambda min: 0.0742735782143339&quot; ## [1] &quot;Error entrenamiento: 0&quot; ## [1] &quot;Error prueba: 0.12&quot; ## [1] &quot;Devianza entrena:0.127&quot; ## [1] &quot;Devianza prueba:0.621&quot; Y esto funcionó bien. Subir más la regularización no ayuda mucho (pruébalo). Parece que el sesgo lo podemos hacer chico (reducir el error de entrenamiento considerablemente), pero tenemos un problema más grande con la varianza. Quizá muchas palabras que estamos usando no tienen qué ver con la calidad de positivo/negativo, y eso induce varianza. Estos modelos no utilizan la estructura que hay en las reseñas, simplemente cuentan qué palabras aparecen. Quizá aprovechar esta estructura podemos incluir variables más informativas que induzcan menos varianza sin aumentar el sesgo. Podemos conseguir más datos. Obsérvese que: ¿Podríamos intentar con una red neuronal totalmente conexa? Probablemente esto no va a ayudar, pues es un modelo más complejo y nuestro problema es varianza. 10.7 Usar datos adicionales Considerar fuentes adicionales de datos muchas veces puede ayudar a mejorar nuestras entradas, lo cual puede tener beneficios en predicción (tanto sesgo como varianza). Intentemos el primer camino. Probamos usar palabras que tengan afinidad como parte de su significado (positivas y negativas). Estos datos están incluidos en el paquete tidytext. bing &lt;- filter(sentiments, lexicon == &#39;bing&#39;) tail(bing) ## # A tibble: 6 x 4 ## word sentiment lexicon score ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 zealous negative bing NA ## 2 zealously negative bing NA ## 3 zenith positive bing NA ## 4 zest positive bing NA ## 5 zippy positive bing NA ## 6 zombie negative bing NA dim(vocabulario) ## [1] 4106 2 vocabulario &lt;- calc_vocabulario(df_ent_grande, 8000, remove_stop = FALSE) voc_bing &lt;- vocabulario %&gt;% inner_join(bing %&gt;% rename(palabra = word)) ## Joining, by = &quot;palabra&quot; dim(voc_bing) ## [1] 1476 5 mod_x &lt;- correr_modelo_cv(df_ent_grande, df_pr, voc_bing, alpha=0, lambda = exp(seq(-5,2,0.1))) ## Joining, by = &quot;palabra&quot; ## Joining, by = &quot;palabra&quot; describir_modelo_cv(mod_x) ## [1] &quot;Lambda min: 0.122456428252982&quot; ## [1] &quot;Error entrenamiento: 0.02&quot; ## [1] &quot;Error prueba: 0.17&quot; ## [1] &quot;Devianza entrena:0.381&quot; ## [1] &quot;Devianza prueba:0.774&quot; Estas variables solas no dan un resultado tan bueno (tenemos tanto sesgo como varianza altas). Podemos combinar: vocabulario &lt;- calc_vocabulario(df_ent_grande, 3000, remove_stop =FALSE) voc &lt;- bind_rows(vocabulario, voc_bing %&gt;% select(palabra, frec)) %&gt;% unique dim(voc) ## [1] 4021 2 mod_x &lt;- correr_modelo_cv(df_ent_grande, df_pr, voc, alpha=0, lambda = exp(seq(-5,2,0.1))) ## Joining, by = &quot;palabra&quot; ## Joining, by = &quot;palabra&quot; describir_modelo_cv(mod_x) ## [1] &quot;Lambda min: 0.0907179532894125&quot; ## [1] &quot;Error entrenamiento: 0&quot; ## [1] &quot;Error prueba: 0.13&quot; ## [1] &quot;Devianza entrena:0.147&quot; ## [1] &quot;Devianza prueba:0.633&quot; Este camino no se ve mal, pero no hemos logrado mejoras. Aunque quizá valdría la pena intentar refinar más y ver qué pasa. 10.8 Examen de modelo y Análisis de errores Ahora podemos ver qué errores estamos cometiendo, y cómo está funcionando el modelo. Busquemos los peores. Corremos el mejor modelo hasta ahora: vocabulario &lt;- calc_vocabulario(df_ent_grande, 4000, remove_stop = FALSE) mod_x &lt;- correr_modelo_cv(df_ent_grande, df_pr, vocabulario, lambda = exp(seq(-5,2,0.1))) ## Joining, by = &quot;palabra&quot; ## Joining, by = &quot;palabra&quot; describir_modelo_cv(mod_x) ## [1] &quot;Lambda min: 0.0820849986238988&quot; ## [1] &quot;Error entrenamiento: 0&quot; ## [1] &quot;Error prueba: 0.12&quot; ## [1] &quot;Devianza entrena:0.136&quot; ## [1] &quot;Devianza prueba:0.623&quot; coeficientes &lt;- predict(mod_x$mod, lambda = &#39;lambda.min&#39;, type = &#39;coefficients&#39;) coef_df &lt;- data_frame(palabra = rownames(coeficientes), coef = coeficientes[,1]) arrange(coef_df, coef) %&gt;% print(n=20) ## # A tibble: 4,107 x 2 ## palabra coef ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) -0.5104659 ## 2 sloppy -0.3049259 ## 3 tiresome -0.3041222 ## 4 tedious -0.2995033 ## 5 designed -0.2752456 ## 6 forgot -0.2752196 ## 7 profanity -0.2741518 ## 8 insulting -0.2635200 ## 9 redeeming -0.2582221 ## 10 ludicrous -0.2569009 ## 11 asleep -0.2525127 ## 12 embarrassing -0.2501103 ## 13 miserably -0.2436195 ## 14 alas -0.2433399 ## 15 lifeless -0.2375580 ## 16 random -0.2340050 ## 17 abilities -0.2284854 ## 18 inept -0.2272352 ## 19 ridiculous -0.2266569 ## 20 stupidity -0.2233172 ## # ... with 4,087 more rows arrange(coef_df, desc(coef)) %&gt;% print(n=20) ## # A tibble: 4,107 x 2 ## palabra coef ## &lt;chr&gt; &lt;dbl&gt; ## 1 refreshing 0.2928763 ## 2 beings 0.2754203 ## 3 underneath 0.2751466 ## 4 commanding 0.2502302 ## 5 outstanding 0.2367210 ## 6 marvelous 0.2269661 ## 7 finest 0.2226550 ## 8 identify 0.2198893 ## 9 enjoyment 0.2178993 ## 10 ralph 0.2132248 ## 11 exceptional 0.2124156 ## 12 anger 0.2082285 ## 13 mature 0.2080578 ## 14 threatens 0.2079872 ## 15 luckily 0.2052777 ## 16 enters 0.2048924 ## 17 overall 0.2012073 ## 18 breathtaking 0.2004415 ## 19 popcorn 0.1985406 ## 20 portrait 0.1957565 ## # ... with 4,087 more rows Y busquemos las diferencias más grandes del la probabilidad ajustada con la clase observada y &lt;- mod_x$prueba$y x &lt;- mod_x$prueba$x probs &lt;- predict(mod_x$mod, newx = x, type =&#39;response&#39;, s=&#39;lambda.min&#39;) df_1 &lt;- data_frame(id = rownames(x), y=y, prob = probs[,1]) %&gt;% mutate(error = y - prob) %&gt;% arrange(desc(abs(error))) df_1 ## # A tibble: 425 x 4 ## id y prob error ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1508 1 0.04079128 0.9592087 ## 2 1461 1 0.04895588 0.9510441 ## 3 1490 1 0.09491609 0.9050839 ## 4 1933 1 0.10969336 0.8903066 ## 5 222 0 0.88869902 -0.8886990 ## 6 25 0 0.85875753 -0.8587575 ## 7 1642 1 0.14257363 0.8574264 ## 8 728 0 0.85126829 -0.8512683 ## 9 1050 1 0.15208815 0.8479119 ## 10 415 0 0.84431790 -0.8443179 ## # ... with 415 more rows filter(df, id == 1461) %&gt;% pull(texto) [1] “Review deep rising is one of &quot; those &quot; movies . the kind of movie which serves no purpose except to entertain us . it does not ask us to think about important questions like life on other planets or the possibility that there is no god . . . screw that , it says boldly , let’s see some computer generated monsters rip into , decapitate and generally cause irreparable booboos to a bunch of little known actors . heh ! them wacky monsters , gotta love ’em . of course , since we can rent about a thousand b movies with the same kind of story , hollywood must give that little extra &quot; oumph &quot; to get people in theaters . that is where deep rising fails , which is a good thing . confused ? let me explain : despite all them flashy effects and big explosions , deep rising is still , at heart , a good ’ol b movie . luckily , it’s a very good b movie . the worst cliches in movie history are a b movie’s bread and butter . therefore , things that would destroy a serious movie actually help us have a good time while watching a movie of lower calibre . of course we know there’s a big slimy creature behind that door , that one person will wander off to be picked off by said monster and we always know which persons or person will make it out alive . we just don’t know when or how horrible it will be . i went to see deep rising with my expections low and my tolerance for bad dialogue high . imagine my surprise when i discover that deep rising is actually , well , pretty darn funny at times . a funny b movie ? well , that’s new . these flicks are not supposed to make us laugh . ( except for a few unintended laughs once a while . ) and before you know it , treat williams , wes studi and famke jansen appear on the big screen . hey ! i know them guys ( and gal ) from a couple of other movies . cool . familiar faces . so far so good . our man treat is the hero , he’ll live . wes is a staple of b movies , he is the token victim . we know he’ll buy the farm but he will take a few creeps with him on the way out . famke is the babe , ’nuff said . there is also a guy with glasses ( the guy with glasses always dies ) a black person ( b movie buffs know that the black guy always dies , never fails ) and a very funny , nerdy guy . ( ah ! comic relief . how can we possibly explain having to kill him . . . let him live . ) after the first fifteen minutes i felt right at home . i know who to root for and who i need to boo too and a gum to chew . ( please kill me . ) suffice it to say that for the next hour and a half i jumped out of my seat a few times , went &quot; ewwww &quot; about a dozen times and nearly had an orgasm over all the explosions and firepower our heroes were packing . i’m a man , we nottice these things . all in all , i’d recommend deep rising if you are looking for a good time and care to leave your brain at the door . . . but bring your sense of humor and excitement in with you . the acting is decent , the effects top rate . how to best describe it ? put together the jet ski scene from hard rain , the bug attacks from starship troopers , a couple of james bond like stunts and all those scenes from friday the thirteenth and freddy where you keep screaming &quot; don’t go there , he’s behind you &quot; and you end up with deep rising . for creepy crawly goodness , tight t-shirts , major firepower and the need to go to the bathroom every fifteen minutes from seing all that water .” filter(df, id == 1508) %&gt;% pull(texto) [1] “Review capsule : side-splitting comedy that follows its own merciless logic almost through to the end . . . but not without providing a good deal of genuine laughs . most comedies these days have one flaw . they’re not funny . they think they’re funny , but they are devoid of anything really penetrating or dastardly . occasionally a good funny movie sneaks past the deadening hollywood preconceptions of humor and we get a real gem : ruthless people , for instance , which established a microcosm of a setup and played it out to the bitter end . liar liar is built the same way and is just about as funny . this is one of the few movies i’ve seen where i was laughing consistently almost all the way through : instead of a couple of set-pieces that inspired a laugh ( think of the dismal fatal instinct ) , the whole movie works like clockwork . jim carrey playes a high-powered lawyer , to whom lying is as natural as breathing . there is one thing he takes seriously , though : his son , and we can sense the affection that they have for each other right away . but his wife is divorced and seeing another man , and now it looks like they may move away together . the son goes with them , of course . the movie sets up this early material with good timing and a remarkable balance of jim carrey’s over-the-top persona with reality . then the plot springs into action : after being snubbed ( not deliberately ) by his father at his birthday , the kid makes a wish as he blows out the birthday candles : that for just one day , dad can’t lie . he gets the wish . what happens next is sidesplitting . everything turns into a confrontation : when cornered by a bum for some change , he shouts , &quot; no ! i’m not giving you any money because i know you’ll spend it on booze ! all i want to do is to get to the office without having to step over the debris of our decaying society ! &quot; he can’t even get into an elevator without earning a black eye . and what’s worse , he’s now gotten himself into an expensive divorce settlement that requires him to twist the truth like abstract wire sculpture . carrey , who i used to find unfunny , has gotten better at his schtick , even if it’s a limited one . he uses it to great effect in this movie . there is a scene where he tries to test his ability to lie and nearly demolishes his office in the process ( there’s a grin breaking out across my face right now , just remembering the scene ) . he can’t even write the lie ; his fingers twitch , his body buckles like someone in the throes of cyanide poisoning , and when he tries to talk it’s like he’s speaking in tongues . equally funny is a scene where he beats himself to a pulp ( don’t ask why ) , tries to drink water to keep from having outbursts in the courtroom ( it fails , with semi-predictable results ) , and winds up biting the bullet when he gets called into the boardroom to have everyone ask what they think of them . this scene alone may force people to stop the tape for minutes on end . the movie sustains its laughs and also its flashes of insight until almost the end . a shame , too , because the movie insists on having a big , ridiculous climax that involves carrey’s character flagging down a plane using a set of motorized stairs , then breaking his leg , etc . a simple reconciliation would do the trick . why is this stupid pent-up climax always obligatory ? it’s not even part of the movie’s real agenda . thankfully , liar liar survives it , and so does carrey . maybe they were being merciful , on reflection . if i’d laughed any more , i might have needed an iron lung .” filter(df, id == 222) %&gt;% pull(texto) #negativa [1] “Review it’s probably inevitable that the popular virtual reality genre ( &quot; the matrix , &quot; &quot; existenz &quot; ) would collide with the even more popular serial-killer genre ( &quot; kiss the girls , &quot; &quot; se7en &quot; ) . the result should have been more interesting than &quot; the cell . &quot; as the movie opens , therapist catharine deane ( jennifer lopez ) treats a catatonic boy ( colton james ) by entering his mind through some sort of virtual reality technique that’s never fully explained . after months of therapy sessions in a surreal desert , catharine has no success to report . meanwhile , killer carl stargher ( vincent d’onofrio ) has claimed another victim . his particular hobby is to kidnap young women , keep them in a glass cell overnight , and drown them . he takes the corpse and soaks it in bleach , then suspends himself over the body and jerks off while watching a video tape of the drowning . although carl’s been doing this for awhile , he’s recently become sloppy , and fbi agent peter novak ( vince vaughn ) is closing in fast . not fast enough , though , to keep carl from sticking another woman ( tara subkoff ) in the cell or to catch him before he suffers a schizophrenic attack that leaves him in a coma . from the videos in carl’s house , peter can see that the drowning cell is automated and will fill with water forty hours after the abduction . to save the kidnapped girl , peter has to find the cell before the end of the day , and comatose carl’s not talking . so off they go to catharine in the hope that she can go inside carl’s mind and find out where the cell is in time . the focus of &quot; the cell &quot; in on the ornate interior of carl’s mind , but the universe director tarsem singh creates seems more an exercise in computer-generated spectacle than an exploration of the psychotic personality . for the most part , it’s style without substance . in his own mind , carl is a decadent emperor in flowing robes , ming the merciless , as well as a frightened boy ( jake thomas ) abused by his father . all in all , the mind of a psycho killer turns out to be a strangely dull place , and i kept wishing i could fast-forward to the next development . singh is best known for directing music videos , particularly rem’s &quot; losing my religion , &quot; and &quot; the cell &quot; seems very much like a really long , really slow mtv video with the sound deleted . singer lopez seems to think she’s in a video as well ; she devotes more time to posing in elaborate costumes than she does to acting . the premise had great promise . the computer-generated world within carl’s mind could have been a bizarre , surreal universe governed by insanity and symbolism rather than logic . the first room catharine enters in carl’s head shows this promise . she finds a horse standing in center of the room ; suddenly , sheets of sharp-edged glass fall into the horse , dividing it into segments . the panes of glass separate , pulling apart the pieces of the still-living horse . this scene is twisted , disturbing , and thought-provoking , because the psychological importance of the horse and its fate is left to the viewer to ponder . another element that should have been developed is the effect on catharine of merging with the mind of a psychopath . their minds begin to bleed together at one point in the movie , and this should have provided an opportunity to discover the dark corners of catharine’s own psyche . like sidney lumet’s &quot; the offence &quot; or michael mann’s &quot; manhunter , &quot; &quot; the cell &quot; could have explored how the madness of the killer brings out a repressed darkness in the investigator . however , catharine’s character is hardly developed at all , and lopez has no depth to offer the role . bottom line : don’t get trapped in this one .” filter(df, id == 25) %&gt;% pull(texto) #negativa [1] “Review forgive the fevered criticism but the fervor of the crucible infects . set in 1692 at salem , massachusetts , the crucible opens with a group of teenage girls passionately singing and dancing around a boiling cauldron in the middle of a forest under the glow of a full moon . they beckon the names of men as the targets of their love spells . then one of the girls lets her hair down and sheds her clothes . not to be outdone in her quest to regain the attention of john proctor ( daniel day lewis ) , abigail ( winona ryder ) suddenly seizes a chicken , beats it against the ground and smears her face and lips with the fresh blood . taking even adolescent hormone surges into account , surely this chicken-bashing bit is a bit excessive , especially for prim puritan sensibilities ? surely to the puritan eye this is as close to a coven of witches as it gets ? the crucible errs from the beginning and arthur miller’s name should be summoned for blame here for the addition of the above scene to his screen adaptation of his play . this is far from a harmless event , a bad start to an already shaky morality tale . the play describes the film’s opening scene during tense exchanges that makes one wonder about the veracity of both accusation and reply , and this adds to the play’s charged atmosphere . in the film , the opening scene becomes an unintentional pandora’s box . not only is credulity stretched but abigail’s obsession is unfortunately spotlighted . it positions the crucible more as a cautionary fable about obsessive and malevolent women than against witch hunts ; it will bring back the memory of a rabbit boiling away in a pot . not surprisingly , the nighttime forest frenzy does not go unnoticed and when two girls fail to wake the following morning , witches are invoked by those eager to blame . when the girls are questioned , their confession of guilt is accompanied with an announcement of their return to god and they are thereafter converted to immaculate witnesses , led lustfully by abigail . with alarming synchronicity our hormonally-advantaged girls zealously gesture and point accusing fingers at innocents , constant reminders that abigail’s passion sets all this into inexorable motion . abigail seizes on this opportunity to rid herself of her rival for john proctor’s love , his wife elizabeth ( joan allen ) , by including her among those accused of witchcraft . appropriately narrow-waisted and equipped with a distractingly white smile ( watch his teeth deteriorate much too quickly to a murky yellow ) , day lewis plays the dashing moral hero with an over-earnestness that longs to be watched . director nicholas hytner is guilty of encouraging day lewis’ foaming-mouth fervour with shots where we stare up at proctor as if he was mounted on a pedestal for our admiration . otherwise , hytner’s direction is unremarkable . ryder’s performance as abigail is as consistent as her mood swings . her fits of frenzy are energetic enough but the quieter moments are less successful . abigail supposedly revels in her newfound power , but ryder fails at being convincingly haughty although there is much haughtiness to spare here . paul scofield is fine as the overzealous judge danforth , but the incessant moral posturings of all the characters along with the recurrent histrionics of the young girls pricks at the nerves . probably because she is the only refuge of restraint amidst all the huffing and puffing , allen’s elizabeth comes out as the most sympathetic character . a scene near the end featuring a private conversation between the imprisoned elizabeth and john is undeniably powerful because for once we are given a reprieve from the moral bantering and the human consequences are revealed . unfortunately , when john’s audience again increases to more than one his urge to pontificate returns and the human urgency of his situation is lost . it is clear that miller meant well but i do wish he did it with more delicacy and fewer diversions . his screenplay is an imperfect creature with the distractions coming out as loud as the message . the result is a clumsy muddle - i felt like the chicken from the opening scene , head ceaselessly banged with piousness too heavy-handed to be wholly believable . when the gallows beckoned , it was sweet release indeed . far from bewitching , the crucible tests the patience .” filter(df, id == 728) %&gt;% pull(texto) #negativa [1] “Review girl 6 is , in a word , a mess . i was never able to determine what spike lee was trying to accomplish with this film . there was no sense of where the film was going , or any kind of coherent narrative . if there was a point to the film , i missed it . girl 6 , by the way , is the way theresa randle’s character is addressed in the phone sex workplace ; all the girls are known by their numbers . the plot , such as it is : theresa randle is a struggling n . y . actress , and eventually takes a job as a phone-sex operator . she begins to lose contact with reality , as her job consumes her . also , she must deal with the advances of her ex-husband ( isiah washington ) . he is an ex- con thief , and she tries to keep him away , while at the same time , it’s clear that she still harbors feelings for him . her neighbor , jimmy ( spike lee ) functions as the observer ; mediating between the ex- husband and girl 6 . he also functions as a point of stability , as he watches her become seduced by the lurid world of phone sex . the soundtrack , consisting of songs by prince , was jarring . it kept taking my attention from the film - not altogether a bad thing , i’ll grant you , as what was transpiring onscreen wasn’t that riveting . for parts of the middle of the film , the music stayed blissfully in the background . in the opening sequence and one scene later in the film , however , the music was particularly loud and distracting . of course , i’ve never really cared for prince’s ( or tafkap if you like ) music . prince fans might love the soundtrack , but it will probably be distracting , even to die-hard fans . of the performances , the only one that stood out was spike lee’s buddy character , jimmy . he was excellent as the always-broke neighbor of girl 6 . he should have stuck to acting in this film . there are several sequences that gave me the impression that he’d like to be oliver stone when he grows up . there are scenes shot with different types of film , which are purposely grainy , and reminiscent of some of the scenes in oliver stone’s natural born killers . in that film , they worked to propel the narrative . in this film , they just made me more confused . there are some amusing moments , and a few insights into the lives of the women who use their voices to make the phone-sex industry the multi-billion dollar industry that it has become . other than that , though , nothing much happens . there are a few intense moments , as when one caller becomes frightening , but even that is rather lackluster . i’m not the biggest fan of spike lee , though i’d agree that he has done some very good work in the past . in girl 6 , though , he seems to be floundering . he had an interesting idea , a fairly good setup , and seemed to wander aimlessly from there . girl 6 earns a grade of d .” Algunos de los errores son difíciles (por ejemplo, una reseña que dice que la película es tan mala que es buena). Otros quizá podemos hacer algo con nuestro método: en algunas partes vemos algunas problemas con nuestro método, por ejemplo, “no energy” - nuestro método no toma en cuenta el orden de las palabras. Podemos intentar capturar algo de esto usando bigramas (pares de palabras) en lugar de simplemente usar palabras. vocabulario &lt;- calc_vocabulario(df_ent_grande, 3800, remove_stop = FALSE) mod_x &lt;- correr_modelo_cv(df_ent_grande, df_pr, vocabulario, lambda = exp(seq(-5,2,0.1))) ## Joining, by = &quot;palabra&quot; ## Joining, by = &quot;palabra&quot; describir_modelo_cv(mod_x) ## [1] &quot;Lambda min: 0.0907179532894125&quot; ## [1] &quot;Error entrenamiento: 0&quot; ## [1] &quot;Error prueba: 0.11&quot; ## [1] &quot;Devianza entrena:0.153&quot; ## [1] &quot;Devianza prueba:0.628&quot; vocabulario_bigramas &lt;- calc_vocabulario(df_ent_grande, 500, bigram = TRUE) vocabulario_bigramas %&gt;% arrange(desc(frec)) ## # A tibble: 509 x 2 ## palabra frec ## &lt;chr&gt; &lt;int&gt; ## 1 of the 6984 ## 2 in the 4609 ## 3 the film 3167 ## 4 is a 2325 ## 5 to be 2218 ## 6 to the 2187 ## 7 and the 2019 ## 8 on the 1780 ## 9 in a 1756 ## 10 the movie 1580 ## # ... with 499 more rows vocabulario_bigramas %&gt;% arrange((frec)) ## # A tibble: 509 x 2 ## palabra frec ## &lt;chr&gt; &lt;int&gt; ## 1 and one 117 ## 2 are so 117 ## 3 decides to 117 ## 4 for some 117 ## 5 might have 117 ## 6 piece of 117 ## 7 sci fi 117 ## 8 science fiction 117 ## 9 that his 117 ## 10 the case 117 ## # ... with 499 more rows library(stringr) mod_bigramas &lt;- correr_modelo_cv(df_ent_grande, df_pr, vocabulario_bigramas, alpha=1, lambda = exp(seq(-7,2,0.1)), bigram = TRUE) ## Joining, by = &quot;palabra&quot; ## Joining, by = &quot;palabra&quot; describir_modelo_cv(mod_bigramas) ## [1] &quot;Lambda min: 0.0111089965382423&quot; ## [1] &quot;Error entrenamiento: 0.19&quot; ## [1] &quot;Error prueba: 0.28&quot; ## [1] &quot;Devianza entrena:0.892&quot; ## [1] &quot;Devianza prueba:1.078&quot; Este resultado no es tan malo. Podemos intentar construir un modelo juntando unigramas y bigramas: y &lt;- mod_x$entrena$y x_1 &lt;- mod_x$entrena$x x_2 &lt;- mod_bigramas$entrena$x mod_ub &lt;- cv.glmnet(x = cbind(x_1, x_2), y = y, alpha = 0.0, family =&#39;binomial&#39;, lambda = exp(seq(-5,1,0.1))) plot(mod_ub) x_1p &lt;- mod_x$prueba$x x_2p &lt;- mod_bigramas$prueba$x preds_ent &lt;- predict(mod_ub, newx = cbind(x_1,x_2), type=&#39;class&#39;, lambda =&#39;lambda.min&#39;) mean(preds_ent != mod_x$entrena$y) ## [1] 0.0006349206 preds_1 &lt;- predict(mod_ub, newx = cbind(x_1p,x_2p), type=&#39;class&#39;, lambda =&#39;lambda.min&#39;) mean(preds_1 != mod_x$prueba$y) ## [1] 0.1129412 10.8.0.1 Ejemplo (opcional) En este ejemplo no tenemos muchos datos, pero puedes intentar de todas formas ajustar una red neuronal adaptada al problema (word embeddings, que veremos más adelante, y convoluciones de una dimensión a lo largo de oraciones). Quizá es buena idea empezar con un conjunto de datos más grandes, como dataset_imdb() en el paquete keras. if(Sys.info()[&#39;nodename&#39;] == &#39;vainilla.local&#39;){ # esto es por mi instalación particular de tensorflow - típicamente # no es necesario que corras esta línea. Sys.setenv(TENSORFLOW_PYTHON=&quot;/usr/local/bin/python&quot;) } library(keras) vocabulario &lt;- calc_vocabulario(df_ent_grande, 2000, remove_stop = FALSE) dim(vocabulario) entrena &lt;- convertir_lista(df_ent_grande, vocabulario) prueba &lt;- convertir_lista(df_pr, vocabulario) quantile(sapply(entrena$x, length)) x_train &lt;- entrena$x %&gt;% pad_sequences(maxlen = 2000, truncating=&quot;post&quot;) x_test &lt;- prueba$x %&gt;% pad_sequences(maxlen = 2000, truncating=&#39;post&#39;) model &lt;- keras_model_sequential() model %&gt;% layer_embedding(input_dim = nrow(vocabulario)+1, output_dim = 30, input_length=2000, embeddings_regularizer = regularizer_l2(0.01)) %&gt;% #layer_dropout(0.5) %&gt;% layer_conv_1d( filters = 20, kernel_size=3, padding = &quot;valid&quot;, activation = &quot;relu&quot;, strides = 1, kernel_regularizer = regularizer_l2(0.01)) %&gt;% # layer_dropout(0.5) %&gt;% layer_global_max_pooling_1d() %&gt;% layer_dense(20, activation =&#39;relu&#39;, kernel_regularizer = regularizer_l2(0.001)) %&gt;% #layer_dropout(0.5) %&gt;% layer_dense(1, activation=&#39;sigmoid&#39;, kernel_regularizer = regularizer_l2(0.001)) # Compile model model %&gt;% compile( loss = &quot;binary_crossentropy&quot;, #optimizer = optimizer_sgd(lr=0.001, momentum=0.0), optimizer = optimizer_adam(), metrics = c(&quot;accuracy&quot;,&quot;binary_crossentropy&quot;) ) model %&gt;% fit( x_train, entrena$y, batch_size = 128, epochs = 200, # callback = callback_early_stopping(monitor=&#39;val_loss&#39;, patience=50), validation_data = list(x_test, prueba$y) ) print(&quot;Entrenamiento&quot;) evaluate(model, x_train, entrena$y) print(&quot;Prueba&quot;) evaluate(model, x_test, prueba$y ) "],
["metodos-basados-en-arboles.html", "Clase 11 Métodos basados en árboles 11.1 Árboles para regresión y clasificación. 11.2 Bagging de árboles 11.3 Bosques aleatorios", " Clase 11 Métodos basados en árboles 11.1 Árboles para regresión y clasificación. La idea básica de los árboles es buscar puntos de cortes en las variables de entrada para hacer predicciones, ir dividiendo la muestra, y encontrar cortes sucesivos para refinar las predicciones. Ejemplo Buscamos clasificar hogares según su ingreso, usando como entradas características de los hogares. Podríamos tener, por ejemplo: knitr::include_graphics(&#39;./imagenes/arboles_1.png&#39;) Con este árbol podemos clasificar nuevos hogares. Nótese que los árboles pueden capturar interacciones entre las variables de entradas. En nuestro ejemplo ficticio, “automóvil” nos da información acerca del ingreso, pero solo caundo el nivel de educación del jefe de familia es bajo. (Ejercicio: si el ingreso fuera una cantidad numérica, ¿cómo escribirías este modelo con una suma de términos que involucren las variables mostradas en el diagrama?) Los árboles también pueden aproximar relaciones no lineales entre entradas y variable de salida (es similar a los ejemplos donde haciamos categorización de variables de entrada). Igual que en redes neuronales, en lugar de buscar puntos de corte o interacciones a mano, con los árboles intentamos encontrarlos de manera automática. 11.1.1 Árboles para clasificación Un árbol particiona el espacio de entradas en rectángulos paralelos a los ejes, y hace predicciones basadas en un modelo simple dentro de cada una de esas particiones. Por ejemplo: knitr::include_graphics(&#39;./imagenes/arboles_2.png&#39;) El proceso de partición binaria recursiva (con una entrada a la vez) puede representarse mediante árboles binarios. Los nodos terminales representan a la partición obtenida. Para definir el proceso de construcción de los árboles, debemos definir: ¿Cómo escoger las particiones? Idea: buscar hacer los nodos sucesivamente más puros (que una sola clase domine). ¿Cuándo declarar a un nodo como terminal? ¿Cuándo particionar más profundamente? Idea: dependiendo de la aplicación, buscamos hacer árboles chicos, o en otras árboles grandes que después podamos para no sobreajustar. ¿Cómo hacer predicciones en nodos terminales? Idea: escoger la clase más común en cada nodo terminal (la de máxima probabilidad). 11.1.2 Tipos de partición Supongamos que tenemos variables de entrada \\((X_1,\\ldots, X_p)\\). Recursivamente particionamos cada nodo escogiendo entre particiones tales que: Dependen de una sola variable de entrada \\(X_i\\) Si \\(X_i\\) es continua, la partición es de la forma \\(\\{X_i\\leq c\\},\\{X_i&gt; c\\}\\), para alguna \\(c\\) (punto de corte) Si \\(X_i\\) es categórica, la partición es de la forma \\(\\{X_i\\in S\\},\\{X_i\\notin S\\}\\), para algún subconjunto \\(S\\) de categorías de \\(X_i\\). En cada nodo candidato, escogemos uno de estos cortes para particionar. ¿Cómo escogemos la partición en cada nodo? En cada nodo, la partición se escoge de una manera miope o local, intentando separar las clases lo mejor que se pueda (sin considerar qué pasa en cortes hechos más adelante). En un nodo dado, escogemos la partición que reduce lo más posible su impureza. 11.1.3 Medidas de impureza Consideramos un nodo \\(t\\) de un árbol \\(T\\), y sean \\(p_1(t),\\ldots, p_K(t)\\) las proporciones de casos de \\(t\\) que caen en cada categoría. La impureza de un nodo \\(t\\) está dada por \\[i(t) = -\\sum_{j=1}^K p_j(t)\\log p_j(t)\\] Este medida se llama entropía. Hay otras posibilidades como medida de impureza (por ejemplo, coeficiente de Gini). 11.1.3.1 Ejemplo Graficamos la medida de impureza para dos clases: impureza &lt;- function(p){ -(p*log(p) + (1-p)*log(1-p)) } curve(impureza, 0,1) Donde vemos que la máxima impureza se alcanza cuando las proporciones de clase en un nodo so 50-50, y la mínima impureza (máxima pureza) se alcanza cuando en el nodo solo hay casos de una clase. Nótese que esta cantidad es proporcional a la devianza del nodo, donde tenemos porbabilidad constante de clase 1 igual a \\(p\\). 11.1.4 Reglas de partición y tamaño del árobl Podemos escribir la regla de partición, que se aplica a cada nodo de un árbol Regla de partición En cada nodo, buscamos entre todas las variables \\(X_i\\) y todos los puntos de corte \\(c\\) la que da la mayor reducción de impureza posible (donde la impureza de un corte es el promedio ponderado por casos de las impurezas de los nodos resultantes). Ejemplo Consideremos un nodo \\(t\\), cuyos casos de entrenamiento son: n_t &lt;- c(200,100, 150) impureza &lt;- function(p){ -sum(p*log(p)) } impureza(n_t/sum(n_t)) ## [1] 1.060857 Y comparamos con n_t &lt;- c(300,10, 140) impureza &lt;- function(p){ p &lt;- p[p&gt;0] -sum(p*log(p)) } impureza(n_t/sum(n_t)) ## [1] 0.7181575 Ahora supongamos que tenemos un posible corte, el primero resulta en n_t &lt;- c(300,10, 140) n_1 = c(300,0,0) n_2 = c(0,10,140) (sum(n_1)/sum(n_t))*impureza(n_1/sum(n_1)) + (sum(n_2)/sum(n_t))*impureza(n_2/sum(n_2)) ## [1] 0.08164334 Un peor corte es: n_t &lt;- c(300,10, 140) n_1 = c(200,0,40) n_2 = c(100,10,100) (sum(n_1)/sum(n_t))*impureza(n_1/sum(n_1)) + (sum(n_2)/sum(n_t))*impureza(n_2/sum(n_2)) ## [1] 0.6377053 Lo que resta explicar es qué criterio de paro utilizamos para dejar de particionar. Regla de paro Cuando usemos árboles en ótros métodos, generalmente hay dos opciones: Particionar hasta cierta profundidad fija (por ejemplo, máximo 8 nodos terminales). Este enfoque generalmente usa árboles relativamente chicos (se usa en boosting de árboles). Dejar de particionar cuando encontramos un número mínimo de casos en un nodo (por ejemplo, 5 o 10 casos). Este enfoque resulta en árboles grandes, probablemente sobreajustados (se usa en bosques aleatorios). Y cuando utilizamos los árboles por sí solos para hacer predicciones: Podemos probar distintos valores de tamaño de árbol, y escogemos por validación (muestra o cruzada) el tamaño final. Podemos usar el método CART de Breiman, que consiste en construir un árbol grande y luego podar al tamaño correcto. Ejemplo Construímos algunos árboles con los datos de spam: library(rpart) library(rpart.plot) library(ggplot2) library(dplyr) library(tidyr) spam_entrena &lt;- read.csv(&#39;./datos/spam-entrena.csv&#39;) spam_prueba &lt;- read.csv(&#39;./datos/spam-prueba.csv&#39;) head(spam_entrena) ## X wfmake wfaddress wfall wf3d wfour wfover wfremove wfinternet wforder ## 1 1 0.00 0.57 0.00 0 0.00 0 0 0 0.00 ## 2 2 1.24 0.41 1.24 0 0.00 0 0 0 0.00 ## 3 3 0.00 0.00 0.00 0 0.00 0 0 0 0.00 ## 4 4 0.00 0.00 0.48 0 0.96 0 0 0 0.48 ## 5 5 0.54 0.00 0.54 0 1.63 0 0 0 0.00 ## 6 6 0.00 0.00 0.00 0 0.00 0 0 0 0.00 ## wfmail wfreceive wfwill wfpeople wfreport wfaddresses wffree wfbusiness ## 1 0 0.57 0.57 1.15 0 0 0.00 0.00 ## 2 0 0.00 0.41 0.00 0 0 0.41 0.00 ## 3 0 0.00 0.00 0.00 0 0 0.00 0.00 ## 4 0 0.00 0.00 0.00 0 0 0.96 0.96 ## 5 0 0.00 0.54 0.00 0 0 0.54 0.54 ## 6 0 0.00 0.00 0.00 0 0 0.00 0.00 ## wfemail wfyou wfcredit wfyour wffont wf000 wfmoney wfhp wfhpl wfgeorge ## 1 1.73 3.46 0 1.15 0 0.00 0.00 0 0 0.0 ## 2 0.82 3.73 0 1.24 0 0.00 0.41 0 0 0.0 ## 3 0.00 12.19 0 4.87 0 0.00 9.75 0 0 0.0 ## 4 0.00 1.44 0 0.48 0 0.96 0.00 0 0 0.0 ## 5 0.00 2.17 0 5.97 0 0.54 0.00 0 0 0.0 ## 6 0.00 5.00 0 0.00 0 0.00 0.00 0 0 2.5 ## wf650 wflab wflabs wftelnet wf857 wfdata wf415 wf85 wftechnology wf1999 ## 1 0 0 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 0 0 0 0 0 ## 4 0 0 0 0 0 0 0 0 0 0 ## 5 0 0 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 0 0 0 0 0 0 ## wfparts wfpm wfdirect wfcs wfmeeting wforiginal wfproject wfre wfedu ## 1 0 0 0 0 0 0 0 0.00 0 ## 2 0 0 0 0 0 0 0 0.41 0 ## 3 0 0 0 0 0 0 0 0.00 0 ## 4 0 0 0 0 0 0 0 0.48 0 ## 5 0 0 0 0 0 0 0 0.00 0 ## 6 0 0 0 0 0 0 0 0.00 0 ## wftable wfconference cfsc cfpar cfbrack cfexc cfdollar cfpound ## 1 0 0 0 0.000 0.000 0.107 0.000 0.000 ## 2 0 0 0 0.065 0.000 0.461 0.527 0.000 ## 3 0 0 0 0.000 0.000 0.000 0.000 0.000 ## 4 0 0 0 0.133 0.066 0.468 0.267 0.000 ## 5 0 0 0 0.000 0.000 0.715 0.318 0.000 ## 6 0 0 0 0.000 0.000 0.833 0.000 0.416 ## crlaverage crllongest crltotal spam ## 1 1.421 7 54 1 ## 2 3.166 19 114 1 ## 3 1.000 1 7 0 ## 4 3.315 61 242 1 ## 5 2.345 22 129 1 ## 6 1.937 8 31 0 Podemos construir un árbol grande. En este caso, buscamos que los nodos resultantes tengan al menos un caso y para particionar pedimos que el nodo tenga al menos 10 casos: set.seed(22) control_completo &lt;- rpart.control(cp=0, minsplit=10, minbucket=1, xval=10, maxdepth=30) spam_tree_completo&lt;-rpart(spam ~ ., data = spam_entrena, method = &quot;class&quot;, control = control_completo) prp(spam_tree_completo, type=4, extra=4) ## Warning: labs do not fit even at cex 0.15, there may be some overplotting Podemos examinar la parte de arriba del árbol: arbol.chico.1 &lt;- prune(spam_tree_completo, cp=0.07) prp(arbol.chico.1, type = 4, extra = 4) Podemos hacer predicciones con este árbol grande. Por ejemplo, en entrenamiento tenemos: prop &lt;- predict(spam_tree_completo, newdata = spam_entrena) table(prop[,2]&gt;0.5, spam_entrena$spam ) ## ## 0 1 ## FALSE 1835 34 ## TRUE 26 1172 y en prueba: prop_arbol_grande &lt;- predict(spam_tree_completo, newdata = spam_prueba) tab_confusion &lt;- table(prop_arbol_grande[,2]&gt;0.5, spam_prueba$spam ) prop.table(tab_confusion, 2) ## ## 0 1 ## FALSE 0.90507012 0.11202636 ## TRUE 0.09492988 0.88797364 Y notamos la brecha grande entre prueba y entrenamiento, lo que sugiere sobreajuste. Este árbol es demasiado grande. 11.1.5 Costo - Complejidad (Breiman) Una manera de escoger árboles del tamaño correcto es utilizando una medida inventada por Breiman para medir la calidad de un árbol. La complejidad de un árbol \\(T\\) está dada por (para \\(\\alpha\\) fija): \\[C_\\alpha (T) = \\overline{err}(T) + \\alpha \\vert T\\vert\\] donde \\(\\overline{err}(T)\\) es el error de clasificación de \\(T\\) \\(\\vert T\\vert\\) es el número de nodos terminales del árbol \\(\\alpha&gt;0\\) es un parámetro de penalización del tamaño del árbol. Esta medida de complejidad incluye qué tan bien clasifica el árbol en la muestra de entrenamiento, pero penaliza por el tamaño del árbol. Para escoger el tamaño del árbol correcto, definimos \\(T_\\alpha \\subset T\\) como el subárbol de \\(T\\) que minimiza la medida \\(C_\\alpha (T_\\alpha)\\). Para entender esta decisión, obsérvese que: Un subárbol grande de \\(T\\) tiene menor valor de \\(\\overline{err}(T)\\) (pues usa más cortes) Pero un subárbol grande de \\(T\\) tiene más penalización por complejidad \\(\\alpha\\vert T\\vert\\). De modo que para \\(\\alpha\\) fija, el árbol \\(T_\\alpha\\) hace un balance entre error de entrenamiento y penalización por complejidad. 11.1.5.1 Ejemplo Podemos ver subárboles más chicos creados durante el procedimiento de división de nodos (prp está el paquete rpart.plot). En este caso pondemos \\(\\alpha = 0.2\\) (cp = \\(\\alpha\\) = complexity parameter): arbol.chico.1 &lt;- prune(spam_tree_completo, cp=0.2) prp(arbol.chico.1, type = 4, extra = 4) Si disminuimos el coeficiente \\(\\alpha\\). arbol.chico.1 &lt;- prune(spam_tree_completo, cp=0.07) prp(arbol.chico.1, type = 4, extra = 4) y vemos que en efecto el árbol \\(T_{0.07}\\) contiene al árbol \\(T_{0.2}\\), y ambos son subárboles del árbol gigante que construimos al principio. Para podar un árbol con costo-complejidad, encontramos para cada \\(\\alpha&gt;0\\) (coeficiente de complejidad) un árbol \\(T_\\alpha\\subset T\\) que minimiza el costo-complejidad. Esto resulta en una sucesión de árboles \\(T_0\\subset T_1\\subset T_2\\subset \\cdots T_m\\subset T\\), de donde podemos escoger con validación el árbol óptimo. Nota: Esto es un teorema que hace falta demostrar: el resultado principal es que conforme aumentamos \\(\\alpha\\), vamos eliminiando ramas del árbol, de manera que los arbol.chico.1 &lt;- prune(spam_tree_completo, cp=0.05) prp(arbol.chico.1, type = 4, extra = 4) arbol.chico.1 &lt;- prune(spam_tree_completo, cp=0.02) prp(arbol.chico.1, type = 4, extra = 4) source(&#39;./scripts/fancyRpartPlot.R&#39;) fancyRpartPlot(arbol.chico.1, sub=&#39;&#39;) ## Loading required package: RColorBrewer Nota: Enfoques de predicción basados en un solo árbol para clasificación y regresión son típicamente superados en predicción por otros métodos. ¿Cuál crees que sea la razón? ¿Es un problema de varianza o sesgo? 11.1.6 (Opcional) Predicciones con CART Podemos hacer predicciones con un sólo árbol. En el caso de spam, haríamos set.seed(9293) # para hacer reproducible la validación cruzada spam_tree &lt;-rpart(spam ~ ., data = spam_entrena, method = &quot;class&quot;, control=list(cp=0, minsplit=5,minbucket=1)) Ahora mostramos los resultados de cada árbol para cada valor de \\(\\alpha\\). La siguiente función nos da una estimación de validación cruzada del error: printcp(spam_tree) ## ## Classification tree: ## rpart(formula = spam ~ ., data = spam_entrena, method = &quot;class&quot;, ## control = list(cp = 0, minsplit = 5, minbucket = 1)) ## ## Variables actually used in tree construction: ## [1] cfbrack cfdollar cfexc cfpar cfsc ## [6] crlaverage crllongest crltotal wf1999 wf3d ## [11] wf650 wfaddress wfall wfbusiness wfconference ## [16] wfcredit wfdata wfdirect wfedu wfemail ## [21] wffont wffree wfgeorge wfhp wfhpl ## [26] wfinternet wflabs wfmail wfmake wfmeeting ## [31] wfmoney wforder wforiginal wfour wfover ## [36] wfpeople wfpm wfproject wfre wfreceive ## [41] wfremove wfreport wftechnology wfwill wfyou ## [46] wfyour X ## ## Root node error: 1206/3067 = 0.39322 ## ## n= 3067 ## ## CP nsplit rel error xerror xstd ## 1 0.49087894 0 1.000000 1.00000 0.022431 ## 2 0.13681592 1 0.509121 0.54975 0.018903 ## 3 0.05223881 2 0.372305 0.44942 0.017516 ## 4 0.03980100 3 0.320066 0.34163 0.015659 ## 5 0.03150912 4 0.280265 0.30514 0.014922 ## 6 0.01160862 5 0.248756 0.28275 0.014436 ## 7 0.01077944 6 0.237148 0.27612 0.014286 ## 8 0.00663350 7 0.226368 0.25954 0.013901 ## 9 0.00497512 9 0.213101 0.24046 0.013436 ## 10 0.00414594 18 0.166667 0.21227 0.012701 ## 11 0.00331675 20 0.158375 0.21144 0.012679 ## 12 0.00276396 24 0.145108 0.20481 0.012496 ## 13 0.00248756 27 0.136816 0.19320 0.012167 ## 14 0.00165837 31 0.126036 0.18740 0.011997 ## 15 0.00130301 44 0.104478 0.18408 0.011899 ## 16 0.00124378 52 0.092869 0.18657 0.011973 ## 17 0.00118455 54 0.090381 0.18740 0.011997 ## 18 0.00110558 61 0.082090 0.18740 0.011997 ## 19 0.00082919 67 0.075456 0.18823 0.012022 ## 20 0.00066335 100 0.048093 0.19569 0.012238 ## 21 0.00041459 107 0.043118 0.19652 0.012262 ## 22 0.00033167 121 0.037313 0.20896 0.012611 ## 23 0.00031095 126 0.035655 0.21144 0.012679 ## 24 0.00027640 140 0.029851 0.21393 0.012746 ## 25 0.00020730 146 0.028192 0.21393 0.012746 ## 26 0.00010365 150 0.027363 0.21725 0.012836 ## 27 0.00000000 158 0.026534 0.21725 0.012836 Y usamos la regla de mínimo error o a una desviación estándar del error mínimo: arbol_podado &lt;- prune(spam_tree, cp = 0.00130301) prp(arbol_podado) Cuyo error de predicción es: prop_arbol_podado &lt;- predict(arbol_podado, newdata=spam_prueba) head(prop_arbol_podado) ## 0 1 ## 1 0.02578797 0.974212 ## 2 0.02578797 0.974212 ## 3 0.03703704 0.962963 ## 4 0.12500000 0.875000 ## 5 0.02578797 0.974212 ## 6 0.02578797 0.974212 prop.table(table((prop_arbol_podado[,2]&gt;0.5),spam_prueba$spam),2) ## ## 0 1 ## FALSE 0.94282632 0.12191104 ## TRUE 0.05717368 0.87808896 11.1.7 Árboles para regresión Para problemas de regresión, el criterio de pureza y la predicción en cada nodo terminal es diferente: En los nodos terminales usamos el promedio los casos de entrenamiento que caen en tal nodo (en lugar de la clase más común) La impureza de define como varianza: si \\(t\\) es un nodo, su impureza está dada por \\(\\frac{1}{n(t)}\\sum (y - m)^2\\), donde la suma es sobre los casos que están en el nodo y \\(m\\) es la media de las \\(y\\)’s del nodo. 11.1.8 Variabilidad en el proceso de construcción Existe variabilidad considerable en el proceso de división, lo cual es una debilidad de los árboles. Por ejemplo: set.seed(9923) muestra.1 &lt;- spam_entrena[sample(1:nrow(spam_entrena), nrow(spam_entrena), replace=T), ] spam.tree.completo.1 &lt;-rpart(spam ~ ., data = muestra.1, method = &quot;class&quot;, control = control_completo) arbol.chico.1 &lt;- prune(spam.tree.completo.1, cp=0.03) prp(arbol.chico.1, type = 4, extra = 4) muestra.1 &lt;- spam_entrena[sample(1:nrow(spam_entrena), nrow(spam_entrena), replace=T), ] spam.tree.completo.1 &lt;-rpart(spam ~ ., data = muestra.1, method = &quot;class&quot;, control = control_completo) arbol.chico.1 &lt;- prune(spam.tree.completo.1, cp=0.03) prp(arbol.chico.1, type = 4, extra = 4) Pequeñas diferencias en la muestra de entrenamiento produce distintas selecciones de variables y puntos de corte, y estructuras de árboles muchas veces distintas. Esto introduce varianza considerable en las predicciones. 11.1.9 Relaciones lineales Los árboles pueden requerir ser muy grandes para estimar apropiadamente relaciones lineales. x &lt;- runif(200,0,1) y &lt;- 2*x + rnorm(200,0,0.1) arbol &lt;- rpart(y~x, data=data_frame(x=x, y=y), method = &#39;anova&#39;) x_pred &lt;- seq(0,1,0.05) y_pred &lt;- predict(arbol, newdata = data_frame(x=x_pred)) y_verdadera &lt;- 2*x_pred dat &lt;- data_frame(x_pred=x_pred, y_pred=y_pred, y_verdadera=y_verdadera) %&gt;% gather(y, valor, y_pred:y_verdadera) ggplot(dat, aes(x=x_pred, y=valor, colour=y)) + geom_line() 11.1.10 Ventajas y desventajas de árboles Ventajas: Árboles chicos son fáciles de explicar e interpretar Capturan interacciones entre las variables de entrada Son robustos en el sentido de que valores numéricos atípicos no hacen fallar al método no es necesario transformar variables hay formas fáciles de lidiar con datos faltantes (cortes sucedáneos) Se ajustan rápidamente yson relativamente fáciles de interpretar (por ejemplo, son útiles para clasificar en campo) Árboles grandes generalmente no sufren de sesgo. Desventajas: Tienen dificultades en capturar estructuras lineales En la interpretación, tienen la dificultad de que muchas veces algunas variables de entrada “enmascaran” a otras. Que una variable de entrada no esté en el árbol no quiere decir que no sea “importante” para predecir (regresión ridge lidia mejor con esto). Son inestables (varianza alta) por construcción: es local/miope, basada en cortes duros si/no. Esto produce desempeño predictivo relativamente malo (p ej: una pequeña diferencia en cortes iniciales puede resultar en estructuras de árbol totalmente distintas). Adicoinalmente, no son apropiados cuando hay variables categóricas con muchas niveles: en estos casos, el árbol sobreajusta desde los primeros cortes, y las predicciones son malas. 11.2 Bagging de árboles Bosques aleatorios es un método de predicción que utiliza familias de árboles para hacer predicciones. Los árboles grandes tienen la ventaja de tener sesgo bajo, pero sufren de varianza alta. Podemos explotar el sesgo bajo si logramos controlar la varianza. Una idea primera para lograr esto es es hacer bagging de árboles: Perturbar la muestra de entrenamiento de distintas maneras y producir árboles distintos (grandes). La perturbación más usada es tomar muestras bootstrap de los datos y ajustar un árbol a cada muestra bootstrap Promediar el resultado de todos estos árboles para hacer predicciones. El proceso de promediar reduce la varianza, sin tener pérdidas en sesgo. La idea básica de bagging (bootstrap aggregation) es la siguiente: Consideramos el proceso \\({\\mathcal L} \\to T_{\\mathcal L}\\), que representa el proceso de ajuste de un árbol \\(T_{\\mathcal L}\\) a partir de la muestra de entrenamiento \\({\\mathcal L}\\). Si pudiéramos obtener distintas muestras de entrenamiento \\[{\\mathcal L}_1, {\\mathcal L}_2, \\ldots, {\\mathcal L}_B,\\] y supongamos que construimos los árboles (que suponemos de regresión) \\[T_1, T_2, \\ldots, T_B,\\] Podríamos mejorar nuestras predicciones construyendo el árbol promedio \\[T(x) = \\frac{1}{B}\\sum_{i=b}^B T_b (x)\\] ¿Por qué es mejor este árbol promedio que cualquiera de sus componentes? Veamos primero el sesgo. El valor esperado del árbol promedio es \\[E[T(x)] = \\frac{1}{B}\\sum_{i=b}^B E[T_b (x)]\\] y como cada \\(T_b(x)\\) se construye de la misma manera a partir de \\({\\mathcal L}_b\\), y todas las muestras \\({\\mathcal L}_b\\) se extraen de la misma forma, todos los términos de la suma de la derecha son iguales: \\[E[T(x)] = E[T_1 (x)],\\] lo que implica que el sesgo del promedio es igual al sesgo de un solo árbol (que es bajo, pues suponemos que los árboles son grandes). Ahora veamos la varianza. Como las muestras \\({\\mathcal L}_b\\) se extraen de manera independiente, entonces \\[Var[T(x)] = Var\\left( \\frac{1}{B}\\sum_{i=b}^B T_b (x)\\right) = \\frac{1}{B^2}\\sum_{i=b}^B Var[T_b (x)],\\] pues los distintos \\(T_b(x)\\) no están correlacionados (en ese caso, varianza de la suma es la suma de las varianzas), y las constantes salen de la varianza al cuadrado. Por las mismas razones que arriba, todos los términos de la derecha son iguales, y \\[Var[T(x)] = \\frac{1}{B}\\ Var[T_1 (x)]\\] de modo que la varianza del árbol promedio es mucho más chica que la varianza de un árbol dado (si \\(B\\) es grande). Sin embargo, no podemos tomar muestras de entrenamiento repetidamente para ajustar estos árboles. ¿Cómo podemos simular extraer distintas muestras de entrenamiento? Sabemos que si tenemos una muestra de entrenamiento fija \\({\\mathcal L}\\), podemos evaluar la variación de esta muestra tomando muestras bootstrap de \\({\\mathcal L}\\), que denotamos por \\[{\\mathcal L}_1^*, {\\mathcal L}_2^*, \\ldots, {\\mathcal L}_B^*,\\] Recordatorio: una muestra bootstrap de \\(\\mathcal L\\) es una muestra con con reemplazo de \\({\\mathcal L}\\) del mismo tamaño que \\({\\mathcal L}\\). Entonces la idea es que construimos los árboles (que suponemos de regresión) \\[T_1^*, T_2^*, \\ldots, T_B^*,\\] podríamos mejorar nuestras predicciones construyendo el árbol promedio \\[T^*(x) = \\frac{1}{B}\\sum_{i=b}^B T_b^* (x)\\] para suavizar la variación de cada árbol individual. El argumento del sesgo aplica en este caso, pero el de la varianza no exactamente, pues las muestras bootstrap no son independientes (están correlacionadas a través de la muestra de entrenamiento de donde se obtuvieron),a pesar de que las muestras bootstrap se extraen de manera independiente de \\({\\mathcal L}\\). De esta forma, no esperamos una reducción de varianza tan grande como en el caso de muestras independientes. Bagging Sea \\({\\mathcal L} =\\{(x^{(i)}, y^{(i)})\\}_{i=1}^n\\) una muestra de entrenamiento, y sean \\[{\\mathcal L}_1^*, {\\mathcal L}_2^*, \\ldots, {\\mathcal L}_B^*,\\] muestras bootstrap de \\({\\mathcal L}\\) (muestreamos con reemplazo los pares \\((x^{(i)}, y^{(i)})\\), para obtener una muestra de tamaño \\(n\\)). Para cada muestra bootstrap construimos un árbol \\[{\\mathcal L}_b^* \\to T_b^*\\]. (Regresión) Promediamos árboles para reducir varianza \\[T^*(x) = \\frac{1}{B}\\sum_{i=b}^B T_b^*(x)\\] (Clasificación) Tomamos votos sobre todos los árboles: \\[T^*(x) = argmax_g \\{ \\# \\{i|T_b^*(x)=g\\}\\}.\\] Podemos también calcular probabilidades promedio sobre todos los árboles. Bagging muchas veces reduce el error de predicción gracias a una reducción modesta de varianza. Nota: No hay garantía de bagging reduzca el error de entrenamiento, especialmente si los árboles base son muy malos clasificadores ¿Puedes pensar en un ejemplo donde empeora? 11.2.1 Ejemplo Probemos con el ejemplo de spam. Construimos árboles con muestras bootstrap de los datos originales de entrenamiento: muestra_bootstrap &lt;- function(df){ df %&gt;% sample_n(nrow(df), replace = TRUE) } arboles_bagged &lt;- lapply(1:30, function(i){ muestra &lt;- muestra_bootstrap(spam_entrena) arbol &lt;- rpart(spam ~ ., data = muestra, method = &quot;class&quot;, control=list(cp=0, minsplit=5,minbucket=1)) arbol }) Examinemos la parte de arriba de algunos de estos árboles: prp(prune(arboles_bagged[[1]], cp =0.01)) prp(prune(arboles_bagged[[2]], cp =0.01)) prp(prune(arboles_bagged[[3]], cp =0.01)) Ahora probemos hacer predicciones con los 30 árboles: library(purrr) preds_clase &lt;- lapply(arboles_bagged, function(arbol){ preds &lt;- predict(arbol, newdata = spam_prueba)[,2] }) preds &lt;- preds_clase %&gt;% reduce(cbind) dim(preds) ## [1] 1534 30 prop_bagging &lt;- apply(preds, 1, mean) prop.table(table(prop_bagging &gt; 0.5, spam_prueba$spam),2) ## ## 0 1 ## FALSE 0.96224380 0.09555189 ## TRUE 0.03775620 0.90444811 Y vemos que tenemos una mejora inmediata con respecto un sólo árbol grande (tanto un árbol grande como uno podado con costo-complejidad). El único costo es el cómputo adicional para procesar las muestras bootstrap ¿Cuántas muestras bootstrap? Bagging generalmente funciona mejor cuando tomamos tantas muestras como sea razonable - aunque también es un parámetro que se puede afinar. Bagging por sí solo se usa rara vez. El método más potente es bosques aleatorios, donde el proceso básico es bagging de árboles, pero añadimos ruido adicional en la construcción de árboles. 11.2.2 Mejorando bagging El factor que limita la mejora de desempeño de bagging es que los árboles están correlacionados a través de la muestra de entrenamiento. Como vimos, si los árboles fueran independientes, entonces mejoramos por un factor de \\(B\\) (número de muestras independientes). Veamos un argumento para entender cómo esa correlación limita las mejoras: Quiséramos calcular (para una \\(x\\) fija) \\[Var(T(x)) = Var\\left(\\frac{1}{B}\\sum_{i=1}^B T^*_i\\right)\\] donde cada \\(T^*_i\\) se construye a partir de una muestra bootstrap de \\({\\mathcal L}\\). Nótese que esta varianza es sobre la muestra de entrenamiento \\({\\mathcal L}\\). Usando la fórmula de la varianza para sumas generales: \\[\\begin{equation} Var(T(x)) = Var\\left(\\frac{1}{B}\\sum_{i=1}^B T^*_i\\right) = \\sum_{i=1}^B \\frac{1}{B^2} Var(T^*_i(x)) + \\frac{2}{B^2}\\sum_{i &lt; j} Cov(T_i^*, T_j^*) \\tag{11.1} \\end{equation}\\] Ponemos ahora \\[\\sigma^2(x) = Var(T_i^*)\\] que son todas iguales porque los árboles bootstrap se extraen de la misma manera (\\({\\mathcal L}\\to {\\mathcal L}^*\\to T^*\\)). Escribimos ahora \\[\\rho(x) = corr(T_i^*, T_j^*)\\] que es una correlación sobre \\({\\mathcal L}\\) (asegúrate que entiendes este término). Todas estas correlaciones son iguales pues cada par de árboles se construye de la misma forma. Así que la fórmula (11.1) queda \\[\\begin{equation} Var(T(x)) = \\frac{1}{B} \\sigma^2(x) + \\frac{B-1}{B} \\rho(x)\\sigma^2(x) = \\sigma^2(x)\\left(\\frac{1}{B} + \\left(1-\\frac{1}{B}\\right )\\rho(x) \\right) \\tag{11.2} \\end{equation}\\] En el límite (cuando B es muy grande, es decir, promediamos muchos árboles): \\[\\begin{equation} Var(T(x)) = Var\\left(\\frac{1}{B}\\sum_{i=1}^B T^*_i\\right) \\approx \\sigma^2(x)\\rho(x) \\tag{11.3} \\end{equation}\\] Si \\(\\rho(x)=0\\) (árboles no correlacionados), la varianza del ensemble es la fracción \\(1/B\\) de la varianza de un solo árbol, y obtenemos una mejora considerable en varianza. En el otro extremo, si la correlación es alta \\(\\rho(x)\\approx 1\\), entonces no obtenemos ganancias por promediar árboles y la varianza del ensamble es similar a la de un solo árbol. Cuando hacemos bagging de árboles, la limitación de mejora cuando promediamos muchos árboles está dada por la correlación entre ellos: cuanto más grande es la correlación, menor beneficio en reducción de varianza obtenemos. Si alteramos el proceso para producir árboles menos correlacionados (menor \\(\\rho(x)\\)), podemos mejorar el desempeño de bagging. Sin embargo, estas alteraciones generalmente están acompañadas de incrementos en la varianza (\\(\\sigma^x(x)\\)). 11.3 Bosques aleatorios Los bosques aleatorios son una versión de árboles de bagging decorrelacionados. Esto se logra introduciendo variabilidad en la construcción de los árboles (esto es paradójico - pero la explicación está arriba: aunque la varianza empeora (de cada árbol), la decorrelación de árboles puede valer la pena). 11.3.1 Sabiduría de las masas Una explicación simple de este proceso que se cita frecuentemente es el fenómeno de la sabiduría de las masas: cuando promediamos estimaciones pobres de un gran número de personas (digamos ignorantes), obtenemos mejores estimaciones que cualquiera de las componentes individuales, o incluso mejores que estimaciones de expertos. Supongamos por ejemplo que \\(G_1,G_2,\\ldots, G_M\\) son clasificadores débiles, por ejemplo \\[P(correcto) = P(G_i=G)=0.6\\] para un problema con probabilidad base \\(P(G=1)=0.5\\). Supongamos que los predictores son independientes, y sea \\(G^*\\) el clasificador que se construye por mayoría de votos a partir de \\(G_1,G_2,\\ldots, G_M\\), es decir \\(G^*=1\\) si y sólo si \\(\\#\\{ G_i = 1\\} &gt; M/2\\). Podemos ver que el número de aciertos (X) de \\(G_1,G_2,\\ldots, G_M\\), por independencia, es binomial \\(Bin(M, 0.6)\\). Si \\(M\\) es grande, podemos aproximar esta distribución con una normal con media \\(M*0.6\\) y varianza \\(0.6*0.4*M\\). Esto implica que \\[P(G^* correcto)=P(X &gt; 0.5M) \\approx P\\left( Z &gt; \\frac{0.5M-0.6M}{\\sqrt(0.24M)}\\right) = P\\left(Z &gt; -2.041 \\sqrt{M}\\right)\\] Y ahora observamos que cuando \\(M\\) es grande, la cantidad de la derecha tiende a 1: la masa, en promedio, tiene la razón! Nótese, sin embargo, que baja dependencia entre las “opiniones” es parte crucial del argumento, es decir, las opiniones deben estar decorrelacionadas. El proceso de decorrelación de bosques aleatorios consiste en que cada vez que tengamos que hacer un corte en un árbol de bagging, escoger al azar un número de variables y usar estas para buscar la mejor variable y el mejor punto de corte, como hicimos en la construcción de árboles. Bosques aleatorios Sea \\(m\\) fija. Sea \\({\\mathcal L} =\\{(x^{(i)}, y^{(i)})\\}_{i=1}^n\\) una muestra de entrenamiento, y sean \\[{\\mathcal L}_1^*, {\\mathcal L}_2^*, \\ldots, {\\mathcal L}_B^*,\\] muestras bootstrap de \\({\\mathcal L}\\) (muestreamos con reemplazo los pares \\((x^{(i)}, y^{(i)})\\), para obtener una muestra de tamaño \\(n\\)). Para cada muestra bootstrap construimos un árbol \\[{\\mathcal L}_b^* \\to T_b^*\\] de la siguiente forma: En cada nodo candidato a particionar, escogemos al azar \\(m\\) variables de las disponibles Buscamos la mejor variable y punto de corte (como en un árbol normal) pero solo entre las variables que seleccionamos al azar. Seguimos hasta construir un árbol grande. (Regresión) Promediamos árboles para reducir varianza \\[T^*(x) = \\frac{1}{B}\\sum_{i=b}^B T_b^*(x)\\] (Clasificación) Tomamos votos sobre todos los árboles: \\[T^*(x) = argmax_g \\{ \\# \\{i|T_b^*(x)=g\\}\\}.\\] Podemos también calcular probabilidades promedio sobre todos los árboles. Bosques aleatorios muchas veces reduce el error de predicción gracias a una reducción a veces considerable de varianza. El objetivo final es reducir la varianza alta que producen árboles normales debido a la forma tan agresiva de construir sus cortes. Observaciones 1. El número de variables \\(m\\) que se seleccionan en cada nodo es un parámetro que hay que escoger (usando validación, validación cruzada). 2. Ojo: no se selecciona un conjunto de \\(m\\) variables para cada árbol. En la construcción de cada árbol, en cada nodo se seleccionan \\(m\\) variables como candidatas para cortes. 3. Como inducimos aleatoriedad en la construcción de árboles, este proceso reduce la correlación entre árboles del bosque, aunque también incrementa su varianza. Los bosques aleatorios funcionan bien cuando la mejora en correlación es más grande que la pérdida en varianza. 4. Reducir \\(m\\), a grandes rasgos: - Aumenta el sesgo del bosque (pues es más restringido el proceso de construcción) - Disminuye la correlación entre árboles y aumenta la varianza de cada árbol 5. Intrementar \\(m\\) - Disminuye el sesgo del bosque (menos restricción) - Aumenta la correlacción entre árobles y disminuye la varianza de cada árbol 11.3.2 Ejemplo Regresamos a nuestro ejemplo de spam. Intentemos con 500 árboles, y 6 variables (de 58 variables) para escoger como candidatos en cada corte: library(randomForest) bosque_spam &lt;-randomForest(factor(spam) ~ ., data = spam_entrena, ntree = 1500, mtry = 6, importance=TRUE) Evaluamos desempeño, donde vemos que obtenemos una mejora inmediata con respecto a bagging: probas &lt;- predict(bosque_spam, newdata = spam_prueba, type=&#39;prob&#39;) head(probas) ## 0 1 ## 1 0.01000000 0.9900000 ## 2 0.01400000 0.9860000 ## 3 0.07133333 0.9286667 ## 4 0.41933333 0.5806667 ## 5 0.05933333 0.9406667 ## 6 0.02066667 0.9793333 prop_bosque &lt;- probas[,2] table(prop_bosque&gt; 0.5, spam_prueba$spam) %&gt;% prop.table(2) %&gt;% round(3) ## ## 0 1 ## FALSE 0.971 0.091 ## TRUE 0.029 0.909 Comparemos las curvas ROC para: árbol grande sin podar árbol podado con costo-complejidad bagging de árboles bosque aleatorio Las curvas de precision-recall library(ROCR) pred_arbol &lt;- prediction(prop_arbol_grande[,2], spam_prueba$spam) pred_podado &lt;- prediction(prop_arbol_podado[,2], spam_prueba$spam) pred_bagging &lt;- prediction(prop_bagging, spam_prueba$spam) pred_bosque &lt;- prediction(prop_bosque, spam_prueba$spam) preds_roc &lt;- list(pred_arbol, pred_podado, pred_bagging, pred_bosque) perfs &lt;- lapply(preds_roc, function(pred){ performance(pred, x.measure = &#39;prec&#39;, measure = &#39;rec&#39;) }) plot(perfs[[1]], lwd=2) plot(perfs[[2]], add=TRUE, col=&#39;orange&#39;, lwd=2) plot(perfs[[3]], add=TRUE, col=&#39;gray&#39;, lwd=2) plot(perfs[[4]], add=TRUE, col=&#39;purple&#39;, lwd=2) O las curvas ROC perfs &lt;- lapply(preds_roc, function(pred){ performance(pred, x.measure = &#39;fpr&#39;, measure = &#39;sens&#39;) }) plot(perfs[[1]], lwd=2) plot(perfs[[2]], add=TRUE, col=&#39;orange&#39;, lwd=2) plot(perfs[[3]], add=TRUE, col=&#39;gray&#39;, lwd=2) plot(perfs[[4]], add=TRUE, col=&#39;purple&#39;, lwd=2) 11.3.3 Más detalles de bosques aleatorios. Los bosques aleatorios, por su proceso de construcción, tienen aspectos interesantes. En primer lugar, tenemos la estimación de error de prueba Out-of-Bag (OOB), que es una estimación honesta del error de predicción basada en el proceso de bagging. Obsérvese en primer lugar, que cuando tomamos muestras con reemplazo para construir cada árbol, algunos casos de entrenamiento aparecen más de una vez, y otros casos no se usan en la construcción del árbol. La idea es entonces es usar esos casos excluidos para hacer una estimación honesta del error. Ejemplo Si tenemos una muestra de entrenamiento entrena &lt;- data_frame(x=1:10, y=rnorm(10, 1:10, 5)) entrena ## # A tibble: 10 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.7717251 ## 2 2 -2.6969885 ## 3 3 5.1126130 ## 4 4 0.1699694 ## 5 5 -5.1408733 ## 6 6 7.0528895 ## 7 7 9.5412102 ## 8 8 9.4230486 ## 9 9 7.3489948 ## 10 10 10.1212700 Tomamos una muestra bootstrap: entrena_boot &lt;- sample_n(entrena, 10, replace = TRUE) entrena_boot ## # A tibble: 10 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 2 -2.6969885 ## 2 6 7.0528895 ## 3 5 -5.1408733 ## 4 7 9.5412102 ## 5 2 -2.6969885 ## 6 8 9.4230486 ## 7 1 0.7717251 ## 8 2 -2.6969885 ## 9 9 7.3489948 ## 10 5 -5.1408733 Construimos un predictor mod_boot &lt;- lm(y~x, data = entrena_boot) y ahora obtenemos los datos que no se usaron: prueba_boot &lt;- anti_join(entrena, entrena_boot) ## Joining, by = c(&quot;x&quot;, &quot;y&quot;) prueba_boot ## # A tibble: 3 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 3 5.1126130 ## 2 4 0.1699694 ## 3 10 10.1212700 y usamos estos tres casos para estimar el error de predicción: mean(abs(predict(mod_boot, prueba_boot)-prueba_boot$y)) ## [1] 2.314668 Esta es la estimación OOB (out-of-bag) para este modelo particular. En un principio podemos pensar que quizá por mala suerte obtenemos pocos elementos OOB para evaluar el error, pero en realidad para muestras no tan chicas obtenemos una fracción considerable. Cuando el tamaño de muestra \\(n\\) es grande, el porcentaje esperado de casos que no están en la muestra bootstrap es alrededor del 37% Demuestra usando probabilidad y teoría de muestras con reemplazo. Estimación OOB del error Consideramos un bosque aleatorio \\(T_{ba}\\)con árboles \\(T_1^*, T_2^*, \\ldots, T_B^*\\), y conjunto de entrenamiento original \\({\\mathcal L} =\\{(x^{(i)}, y^{(i)}\\}_{i=1}^n\\). Para cada caso de entrenamiento \\((x^{(i)}, y^{(i)})\\) consideramos todos los árboles que no usaron este caso para construirse, y construimos un bosque \\(T_{ba}^{(i)}\\) basado solamente en esos árboles. La predicción OOB de \\(T_{ba}^{(i)}\\) para \\((x^{(i)}, y^{(i)})\\) es \\[y_{oob}^{(i)} = T_{ba}^{(i)}(x^{(i)})\\] El error OOB del árbol \\(T_{ba}\\) está dado por 1. Regresión (error cuadrático medio) \\[\\hat{Err}_{oob} = \\frac{1}{n} \\sum_{i=1}^n (y^{(i)} - y_{oob}^{(i)})^2\\] 2. Clasificación (error de clasificación) \\[\\hat{Err}_{oob} = \\frac{1}{n}\\sum_{i=1}^n I(y^{(i)} = y_{oob}^{(i)})\\] Para cada dato de entrenamiento, hacemos predicciones usando solamente los árboles que no consideraron ese dato en su construcción. Estas predicciones son las que evaluamos Es una especie de validación cruzada (se puede demostrar que es similar a validacion cruzada leave-one-out), pero es barata en términos computacionales. Como discutimos en validación cruzada, esto hace de OOB una buena medida de error para afinar los parámetros del modelo (principalmente el número \\(m\\) de variables que se escogen en cada corte). Ejempo Para el ejemplo de spam, podemos ver el error OOB ( y matriz de confusión también OOB): bosque_spam ## ## Call: ## randomForest(formula = factor(spam) ~ ., data = spam_entrena, ntree = 1500, mtry = 6, importance = TRUE) ## Type of random forest: classification ## Number of trees: 1500 ## No. of variables tried at each split: 6 ## ## OOB estimate of error rate: 4.96% ## Confusion matrix: ## 0 1 class.error ## 0 1806 55 0.02955400 ## 1 97 1109 0.08043118 Que comparamos con probas &lt;- predict(bosque_spam, newdata = spam_prueba, type=&#39;prob&#39;) prop_bosque &lt;- probas[,2] tab &lt;- table(prop_bosque&gt; 0.5, spam_prueba$spam) %&gt;% prop.table(2) %&gt;% round(3) 1-diag(tab) ## [1] 0.029 0.091 Podemos comparar con el cálculo de entrenamiento, que como sabemos típicamente es una mala estimación del error de predicción: probas &lt;- predict(bosque_spam, newdata = spam_entrena, type=&#39;prob&#39;) prop_bosque &lt;- probas[,2] table(prop_bosque&gt; 0.5, spam_entrena$spam) ## ## 0 1 ## FALSE 1861 10 ## TRUE 0 1196 tab &lt;- table(prop_bosque&gt; 0.5, spam_entrena$spam) %&gt;% prop.table(2) %&gt;% round(3) 1-diag(tab) ## [1] 0.000 0.008 Podemos también monitorear el error OOB conforme agregamos más árboles. Esta gráfica es útil para entender qué tanto esta mejorando el bosque dependiendo del número de árboles: err_spam &lt;- bosque_spam$err.rate %&gt;% as_data_frame %&gt;% mutate(ntrees = row_number()) head(err_spam) ## # A tibble: 6 x 4 ## OOB `0` `1` ntrees ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 0.10861759 0.07647908 0.1615202 1 ## 2 0.10976282 0.08649603 0.1485294 2 ## 3 0.10337276 0.08333333 0.1361015 3 ## 4 0.09633385 0.07545973 0.1296859 4 ## 5 0.09373856 0.07378524 0.1250000 5 ## 6 0.09259259 0.07573150 0.1188561 6 err_spam &lt;- err_spam %&gt;% gather(métrica, valor, -ntrees) ggplot(err_spam, aes(x=ntrees, y=valor, colour=métrica)) + geom_line() Además de la estimación OOB del error de clasificación, en la gráfica están las estimaciones OOB dada cada una de las clases (probabilidad de clasificar correctamente dada la clase: en problemas binarios son tasa de falsos positivos y tasa de falsos negativos). 11.3.4 Importancia de variables Usando muestras bootstrap y error OOB, es posible tener mediciones útiles de la importancia de una variable en el modelo en un bosque aleatorio (todo esto también fue inventado por Breiman). En primer lugar, consideremos qué significa que una variable sea importante desde el punto predictivo en un modelo. Podemos considerar, por ejemplo: Si quitamos una variable, y el error de predicción se degrada, la variable es importante. Este no es un muy buen enfoque, porque muchas veces tenemos conjuntos de variables correlacionadas. Aún cuando una variable influya en la predicción, si la quitamos, otras variable pueden hacer su trabajo, y el modelo no se degrada mucho (piensa en regresión, en donde incluso esta variable eliminada puede tener un coeficiente grande e influir mucho en la predicción). También requiere ajustar modelos adicionales. Si las predicciones cambian mucho cuando una variable cambia, entonces la variable es importante. Este concepto funciona mejor, al menos desde el punto de vista predictivo. Su defecto es que debemos decidir qué cambios queremos medir. Si el modelo es simple (por ejemplo, lineal), entonces es relativamente fácil usar cambios marginales. Pero en modelos no lineales cambios marginales no necesariamente evalúan correctamente el efecto de la variable sobre las predicciones. La situación se complica adicionalmente si hay interacciones con otras variables, lo que típicamente sucede en métodos basados en árbles. La idea de Breiman, que intenta atender estas observaciones, es como sigue: Consideramos un árbol \\(T^*_j\\) del bosque, con muestra bootstrap \\({\\mathcal L}^*_i\\). Calculamos un tipo de error out-of-bag para el árbol, promediando sobre todos los elementos de \\({\\mathcal L}\\) que no están en \\({\\mathcal L}^*_i\\) \\[\\widehat{Err}_{oob}(T^*_j) = \\frac{1}{A_j}\\sum_{(x^{(i)}, y^{(i)}) \\in {\\mathcal L} -{\\mathcal L}^*_i} L(y^{(i)}, T^*_j(x^{(i)}))\\] donde \\(A_j\\) es el tamaño de \\({\\mathcal L} -{\\mathcal L}^*_i\\). Ahora permutamos al azar la variable \\(X_k\\) en la muestra OOB \\({\\mathcal L} -{\\mathcal L}^*_i\\). Describimos esta operación como \\(x^{(i)} \\to x^{(i)}_k\\). Calculamos el error nuevamente: \\[\\widehat{Err}_{k}(T^*_j) = \\frac{1}{A_j}\\sum_{(x^{(i)}, y^{(i)}) \\in {\\mathcal L} -{\\mathcal L}^*_i} L(y^{(i)}, T^*_j(x_k^{(i)}))\\] Ahora calculamos la degradación del error out-of-bag debido a la permutación: \\[ D_k(T_j^*) = \\widehat{Err}_{k}(T^*_j) - \\widehat{Err}_{oob}(T^*_j) \\] Y promediamos sobre el bosque entero \\[I_k =\\frac{1}{B} \\sum_{j=1}^B D_k(T^*_j)\\] y a esta cantidad le llamamos la importancia (basada en permutaciones) de la variable \\(k\\) en el bosque aleatorio. Es el decremento promedio de capacidad predictiva cuando “quitamos” la variable \\(X_k\\). Nótese que: No podemos “quitar” la variable durante el entrenamiento de los árboles, pues entonces otras variables pueden hacer su trabajo, subestimando su importancia. No podemos “quitar” la variable al medir el error OOB, pues se necesitan todas las variables para poder clasificar con cada árbol (pues cada árbol usa esa variable, o tiene probabilidad de usarla). Pero podemos permutar a la hora calcular el error OOB (y no durante el entrenamiento), rompiendo la relación que hay entre \\(X_k\\) y la variable respuesta. Aunque podríamos usar esta medida para árboles, no es muy buena idea por el problema de “enmascaramiento”. Este problema se aminora en los bosques aleatorios pues todas las variables tienen oportunidad de aportar cortes en ausencia de otras variables. Otra manera de medir importancia para árboles de regresión y clasificación es mediante el decremento de impureza promedio sobre el bosque, para cada variable. Cada vez que una variable aporta un corte en un árbol, la impureza del árbol disminuye. Sumamos, en cada árbol, todos estos decrementos de impureza (cada vez que aparece la variable en un corte) Finalmente, promediamos esta medida de importancia dentro de cada árbol sobre el bosque completo. Repetimos para cada variable. Para árboles de clasificación, usualmente se toma la importancia de Gini, que está basada in la impureza de Gini en lugar de la entropía. La impureza de Gini está dada por \\[I_G(p_1, \\ldots, p_K) = \\sum_{k=1}^K p_k(1-p_k),\\] que es similar a la impureza de entropía que discutimos en la construcción de árboles: \\[I_G(p_1, \\ldots, p_K) = \\sum__{k=1}^K -p_k\\log (p_k),\\] Nótese por ejemplo que ambas toman su valor máximo en \\(p_k=1/K\\) (distribución más uniforme posible sobre las clases), y que son iguales a cero cuando \\(p_k=1\\) para alguna \\(k\\). #### Ejemplo{-} En nuestro ejemplo de spam imp &lt;- importance(bosque_spam, type=1) importancia_df &lt;- data_frame(variable = rownames(imp), MeanDecreaseAccuracy = imp[,1]) %&gt;% arrange(desc(MeanDecreaseAccuracy)) importancia_df ## # A tibble: 58 x 2 ## variable MeanDecreaseAccuracy ## &lt;chr&gt; &lt;dbl&gt; ## 1 cfexc 73.23231 ## 2 crlaverage 61.20036 ## 3 wfremove 60.82482 ## 4 cfdollar 58.27833 ## 5 wffree 57.10130 ## 6 crllongest 53.44965 ## 7 wfhp 52.07446 ## 8 crltotal 52.02168 ## 9 wfedu 47.85035 ## 10 wfyour 46.59530 ## # ... with 48 more rows importancia_df &lt;- importancia_df %&gt;% mutate(variable = reorder(variable, MeanDecreaseAccuracy)) ggplot(importancia_df , aes(x=variable, y= MeanDecreaseAccuracy)) + geom_point() + coord_flip() Observación: en el paquete randomForest, las importancias están escaladas por su la desviación estándar sobre los árboles - la idea es que puedan ser interpretados como valores-\\(z\\) (estandarizados). En este caso, nos podríamos fijar en importancias que están por arriba de \\(2\\), por ejemplo. Para obtener los valores no estandarizados (y ver la degradación en desempeño directamente) podemos calcular importance(bosque_spam, type = 1, scale = FALSE) ## MeanDecreaseAccuracy ## X -3.167359e-04 ## wfmake 9.469937e-04 ## wfaddress 1.692136e-03 ## wfall 3.893455e-03 ## wf3d 4.707905e-05 ## wfour 1.397900e-02 ## wfover 2.561807e-03 ## wfremove 3.473959e-02 ## wfinternet 5.428969e-03 ## wforder 1.111770e-03 ## wfmail 2.329121e-03 ## wfreceive 4.035962e-03 ## wfwill 3.829818e-03 ## wfpeople 6.090923e-04 ## wfreport 6.590103e-04 ## wfaddresses 9.724666e-04 ## wffree 2.341364e-02 ## wfbusiness 5.478105e-03 ## wfemail 2.386219e-03 ## wfyou 1.142595e-02 ## wfcredit 2.960069e-03 ## wfyour 1.920687e-02 ## wffont 1.875817e-03 ## wf000 1.273778e-02 ## wfmoney 1.213523e-02 ## wfhp 3.248315e-02 ## wfhpl 1.356723e-02 ## wfgeorge 1.669735e-02 ## wf650 3.470366e-03 ## wflab 1.140603e-03 ## wflabs 2.544115e-03 ## wftelnet 1.207992e-03 ## wf857 4.267066e-04 ## wfdata 8.081002e-04 ## wf415 4.643810e-04 ## wf85 2.296557e-03 ## wftechnology 1.431399e-03 ## wf1999 6.884388e-03 ## wfparts 8.941783e-05 ## wfpm 1.164572e-03 ## wfdirect 7.717049e-04 ## wfcs 4.878773e-04 ## wfmeeting 2.966489e-03 ## wforiginal 8.513556e-04 ## wfproject 6.897514e-04 ## wfre 4.053317e-03 ## wfedu 1.000909e-02 ## wftable 2.073989e-05 ## wfconference 4.692198e-04 ## cfsc 1.556489e-03 ## cfpar 4.222729e-03 ## cfbrack 1.060870e-03 ## cfexc 3.910984e-02 ## cfdollar 3.312698e-02 ## cfpound 9.925973e-04 ## crlaverage 2.800918e-02 ## crllongest 3.531938e-02 ## crltotal 3.173977e-02 11.3.5 Ajustando árboles aleatorios. El parámetro más importante de afinar es usualmente \\(m\\), el número de variables que se escogen al azar en cada nodo. A veces podemos obtener algunas ventajas de afinar el número mínimo de observaciones por nodo terminal y/o el número mínimo de observaciones por nodo para considerar hacer cortes adicionales Usualmente corremos tantos árboles como podamos (cientos, miles), o hasta que se estabiliza el error. Aumentar más arboles rara vez producen sobreajuste adicional (aunque esto no quiere decir que los bosques aleatorios no puedan sobreajustar!) Ejemplo Consideremos datos de (casas en venta en Ames, Iowa)[https://ww2.amstat.org/publications/jse/v19n3/decock.pdf]. Queremos predecir el precio listado de una casa en función de las características de las casa. El análisis completo (desde limpieza y exploración) está en scripts/bosque-housing.Rmd 11.3.6 Ventajas y desventajas de árboles aleatorios Ventajas: Entre los métodos estándar (off-the shelf), son quizá el mejor método: tienen excelentes tasas de error de predicción. Los bosques aleatorios son relativamente fáciles de entrenar (usualmente 1 o 2 parámetros) y rápidos de ajustar. Heredan las ventajas de los árboles: no hay necesidad de transformar variables o construir interacciones (pues los árboles pueden descubrirlas), son robustos a valores atípicos. Igual que con los árboles, las predicciones de los bosques siempre están en el rango de las variables de predicción (no extrapolan) Desventajas: - Pueden ser lentos en la predicción, pues muchas veces requieren evaluar grandes cantidades de árboles. - No es tan simple adaptarlos a distintos tipos de problemas (por ejemplo, como redes neuronales, que combinando capas podemos construir modelos ad-hoc a problemas particulares). - La falta de extrapolación puede ser también un defecto (por ejemplo, cuando hay una estructura lineal aproximada). 11.3.7 Tarea (para 23 de octubre) Las instrucciones están en scripts/tarea_arboles_bosques.Rmd "],
["metodos-basados-en-arboles-boosting.html", "Clase 12 Métodos basados en árboles: boosting 12.1 Forward stagewise additive modeling (FSAM) 12.2 Discusión 12.3 Algoritmo FSAM 12.4 FSAM para clasificación binaria. 12.5 Gradient boosting 12.6 Algoritmo de gradient boosting 12.7 Funciones de pérdida 12.8 Modificaciones de Gradient Boosting 12.9 Gráficas de dependencia parcial 12.10 xgboost y gbm Tarea", " Clase 12 Métodos basados en árboles: boosting Boosting también utiliza la idea de un “ensamble” de árboles. La diferencia grande con bagging y bosques aleatorios en que la sucesión de árboles de boosting se ‘adapta’ al comportamiento del predictor a lo largo de las iteraciones, haciendo reponderaciones de los datos de entrenamiento para que el algoritmo se concentre en las predicciones más pobres. Boosting generalmente funciona bien con árboles chicos (cada uno con sesgo alto), mientras que bosques aleatorios funciona con árboles grandes (sesgo bajo). En boosting usamos muchos árboles chicos adaptados secuencialmente. La disminución del sesgo proviene de usar distintos árboles que se encargan de adaptar el predictor a distintas partes del conjunto de entrenamiento. El control de varianza se logra con tasas de aprendizaje y tamaño de árboles, como veremos más adelante. En bosques aleatorios usamos muchos árboles grandes, cada uno con una muestra de entrenamiento perturbada (bootstrap). El control de varianza se logra promediando sobre esas muestras bootstrap de entrenamiento. Igual que bosques aleatorios, boosting es también un método que generalmente tiene alto poder predictivo. 12.1 Forward stagewise additive modeling (FSAM) Aunque existen versiones de boosting (Adaboost) desde los 90s, una buena manera de entender los algoritmos es mediante un proceso general de modelado por estapas (FSAM). 12.2 Discusión Consideramos primero un problema de regresión, que queremos atacar con un predictor de la forma \\[f(x) = \\sum_{k=1}^m \\beta_k b_k(x),\\] donde los \\(b_k\\) son árboles. Podemos absorber el coeficiente \\(\\beta_k\\) dentro del árbol \\(b_k(x)\\), y escribimos \\[f(x) = \\sum_{k=1}^m T_k(x),\\] Para ajustar este tipo de modelos, buscamos minimizar la pérdida de entrenamiento: \\[\\begin{equation} \\min \\sum_{i=1}^N L(y^{(i)}, \\sum_{k=1}^M T_k(x^{(i)})) \\end{equation}\\] Este puede ser un problema difícil, dependiendo de la familia que usemos para los árboles \\(T_k\\), y sería difícil resolver por fuerza bruta. Para resolver este problema, podemos intentar una heurística secuencial o por etapas: Si tenemos \\[f_{m-1}(x) = \\sum_{k=1}^{m-1} T_k(x),\\] intentamos resolver el problema (añadir un término adicional) \\[\\begin{equation} \\min_{T} \\sum_{i=1}^N L(y^{(i)}, f_{m-1}(x^{(i)}) + T(x^{(i)})) \\end{equation}\\] Por ejemplo, para pérdida cuadrática (en regresión), buscamos resolver \\[\\begin{equation} \\min_{T} \\sum_{i=1}^N (y^{(i)} - f_{m-1}(x^{(i)}) - T(x^{(i)}))^2 \\end{equation}\\] Si ponemos \\[ r_{m-1}^{(i)} = y^{(i)} - f_{m-1}(x^{(i)}),\\] que es el error para el caso \\(i\\) bajo el modelo \\(f_{m-1}\\), entonces reescribimos el problema anterior como \\[\\begin{equation} \\min_{T} \\sum_{i=1}^N ( r_{m-1}^{(i)} - T(x^{(i)}))^2 \\end{equation}\\] Este problema consiste en ajustar un árbol a los residuales o errores del paso anterior. Otra manera de decir esto es que añadimos un término adicional que intenta corregir los que el modelo anterior no pudo predecir bien. La idea es repetir este proceso para ir reduciendo los residuales, agregando un árbol a la vez. La primera idea central de boosting es concentrarnos, en el siguiente paso, en los datos donde tengamos errores, e intentar corregir añadiendo un término adicional al modelo. 12.3 Algoritmo FSAM Esta idea es la base del siguiente algoritmo: Algoritmo FSAM (forward stagewise additive modeling) Tomamos \\(f_0(x)=0\\) Para \\(m=1\\) hasta \\(M\\), Resolvemos \\[T_m = argmin_{T} \\sum_{i=1}^N L(y^{(i)}, f_{m-1}(x^{(i)}) + T(x^{(i)}))\\] Ponemos \\[f_m(x) = f_{m-1}(x) + T_m(x)\\] Nuestro predictor final es \\(f(x) = \\sum_{m=1}^M T_(x)\\). Observaciones: Generalmente los árboles sobre los que optimizamos están restringidos a una familia relativamente chica: por ejemplo, árboles de profundidad no mayor a \\(2,3,\\ldots, 8\\). Este algoritmo se puede aplicar directamente para problemas de regresión, como vimos en la discusión anterior: simplemente hay que ajustar árboles a los residuales del modelo del paso anterior. Sin embargo, no está claro cómo aplicarlo cuando la función de pérdida no es mínimos cuadrados (por ejemplo, regresión logística). Ejemplo (regresión) Podemos hacer FSAM directamente sobre un problema de regresión. set.seed(227818) library(rpart) library(tidyverse) x &lt;- rnorm(200, 0, 30) y &lt;- 2*ifelse(x &lt; 0, 0, sqrt(x)) + rnorm(200, 0, 0.5) dat &lt;- data.frame(x=x, y=y) Pondremos los árboles de cada paso en una lista. Podemos comenzar con una constante en lugar de 0. arboles_fsam &lt;- list() arboles_fsam[[1]] &lt;- rpart(y~x, data = dat, control = list(maxdepth=0)) arboles_fsam[[1]] ## n= 200 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 200 5370.398 4.675925 * Ahora construirmos nuestra función de predicción y el paso que agrega un árbol predecir_arboles &lt;- function(arboles_fsam, x){ preds &lt;- lapply(arboles_fsam, function(arbol){ predict(arbol, data.frame(x=x)) }) reduce(preds, `+`) } agregar_arbol &lt;- function(arboles_fsam, dat, plot=TRUE){ n &lt;- length(arboles_fsam) preds &lt;- predecir_arboles(arboles_fsam, x=dat$x) dat$res &lt;- y - preds arboles_fsam[[n+1]] &lt;- rpart(res ~ x, data = dat, control = list(maxdepth = 1)) dat$preds_nuevo &lt;- predict(arboles_fsam[[n+1]]) dat$preds &lt;- predecir_arboles(arboles_fsam, x=dat$x) g_res &lt;- ggplot(dat, aes(x = x)) + geom_line(aes(y=preds_nuevo)) + geom_point(aes(y=res)) + labs(title = &#39;Residuales&#39;) + ylim(c(-10,10)) g_agregado &lt;- ggplot(dat, aes(x=x)) + geom_line(aes(y=preds), col = &#39;red&#39;, size=1.1) + geom_point(aes(y=y)) + labs(title =&#39;Ajuste&#39;) if(plot){ print(g_res) print(g_agregado) } arboles_fsam } Ahora construiremos el primer árbol. Usaremos ‘troncos’ (stumps), árboles con un solo corte: Los primeros residuales son simplemente las \\(y\\)’s observadas arboles_fsam &lt;- agregar_arbol(arboles_fsam, dat) ## Warning: Removed 8 rows containing missing values (geom_point). Ajustamos un árbol de regresión a los residuales: arboles_fsam &lt;- agregar_arbol(arboles_fsam, dat) arboles_fsam &lt;- agregar_arbol(arboles_fsam, dat) arboles_fsam &lt;- agregar_arbol(arboles_fsam, dat) arboles_fsam &lt;- agregar_arbol(arboles_fsam, dat) arboles_fsam &lt;- agregar_arbol(arboles_fsam, dat) Después de 20 iteraciones obtenemos: for(j in 1:19){ arboles_fsam &lt;- agregar_arbol(arboles_fsam, dat, plot = FALSE) } arboles_fsam &lt;- agregar_arbol(arboles_fsam, dat) 12.4 FSAM para clasificación binaria. Para problemas de clasificación, no tiene mucho sentido trabajar con un modelo aditivo sobre las probabilidades: \\[p(x) = \\sum_{k=1}^m T_k(x),\\] Así que hacemos lo mismo que en regresión logística. Ponemos \\[f(x) = \\sum_{k=1}^m T_k(x),\\] y entonces las probabilidades son \\[p(x) = h(f(x)),\\] donde \\(h(z)=1/(1+e^{-z})\\) es la función logística. La optimización de la etapa \\(m\\) según fsam es \\[\\begin{equation} T = argmin_{T} \\sum_{i=1}^N L(y^{(i)}, f_{m-1}(x^{(i)}) + T(x^{(i)})) \\tag{12.1} \\end{equation}\\] y queremos usar la devianza como función de pérdida. Por razones de comparación (con nuestro libro de texto y con el algoritmo Adaboost que mencionaremos más adelante), escogemos usar \\[y \\in \\{1,-1\\}\\] en lugar de nuestro tradicional \\(y \\in \\{1,0\\}\\). En ese caso, la devianza binomial se ve como \\[L(y, z) = -\\left [ (y+1)\\log h(z) - (y-1)\\log(1-h(z))\\right ],\\] que a su vez se puede escribir como (demostrar): \\[L(y,z) = 2\\log(1+e^{-yz})\\] Ahora consideremos cómo se ve nuestro problema de optimización: \\[T = argmin_{T} 2\\sum_{i=1}^N \\log (1+ e^{-y^{(i)}(f_{m-1}(x^{(i)}) + T(x^{(i)})})\\] Nótese que sólo optimizamos con respecto a \\(T\\), así que podemos escribir \\[T = argmin_{T} 2\\sum_{i=1}^N \\log (1+ d_{m,i}e^{- y^{(i)}T(x^{(i)})})\\] Y vemos que el problema es más difícil que en regresión. No podemos usar un ajuste de árbol usual de regresión o clasificación, como hicimos en regresión. No está claro, por ejemplo, cuál debería ser el residual que tenemos que ajustar (aunque parece un problema donde los casos de entrenamiento están ponderados por \\(d_{m,i}\\)). Una solución para resolver aproximadamente este problema de minimización, es gradient boosting. 12.5 Gradient boosting La idea de gradient boosting es replicar la idea del residual en regresión, y usar árboles de regresión para resolver (12.1). Gradient boosting es una técnica general para funciones de pérdida generales.Regresamos entonces a nuestro problema original \\[(\\beta_m, b_m) = argmin_{T} \\sum_{i=1}^N L(y^{(i)}, f_{m-1}(x^{(i)}) + T(x^{(i)}))\\] La pregunta es: ¿hacia dónde tenemos qué mover la predicción de \\(f_{m-1}(x^{(i)})\\) sumando el término \\(T(x^{(i)})\\)? Consideremos un solo término de esta suma, y denotemos \\(z_i = T(x^{(i)})\\). Queremos agregar una cantidad \\(z_i\\) tal que el valor de la pérdida \\[L(y, f_{m-1}(x^{(i)})+z_i)\\] se reduzca. Entonces sabemos que podemos mover la z en la dirección opuesta al gradiente \\[z_i = -\\gamma \\frac{\\partial L}{\\partial z}(y^{(i)}, f_{m-1}(x^{(i)}))\\] Sin embargo, necesitamos que las \\(z_i\\) estén generadas por una función \\(T(x)\\) que se pueda evaluar en toda \\(x\\). Quisiéramos que \\[T(x^{(i)})\\approx -\\gamma \\frac{\\partial L}{\\partial z}(y^{(i)}, f_{m-1}(x^{(i)}))\\] Para tener esta aproximación, podemos poner \\[g_{i,m} = -\\frac{\\partial L}{\\partial z}(y^{(i)}, f_{m-1}(x^{(i)}))\\] e intentar resolver \\[\\begin{equation} \\min_T \\sum_{i=1}^n (g_{i,m} - T(x^{(i)}))^2, \\tag{12.2} \\end{equation}\\] es decir, intentamos replicar los gradientes lo más que sea posible. Este problema lo podemos resolver con un árbol usual de regresión. Finalmente, podríamos escoger \\(\\nu\\) (tamaño de paso) suficientemente chica y ponemos \\[f_m(x) = f_{m-1}(x)+\\nu T(x).\\] Podemos hacer un refinamiento adicional que consiste en encontrar los cortes del árbol \\(T\\) según (12.2), pero optimizando por separado los valores que T(x) toma en cada una de las regiones encontradas. 12.6 Algoritmo de gradient boosting Gradient boosting (versión simple) Inicializar con \\(f_0(x) =\\gamma\\) Para \\(m=0,1,\\ldots, M\\), Para \\(i=1,\\ldots, n\\), calculamos el residual \\[r_{i,m}=-\\frac{\\partial L}{\\partial z}(y^{(i)}, f_{m-1}(x^{(i)}))\\] Ajustamos un árbol de regresión a la respuesta \\(r_{1,m},r_{2,m},\\ldots, r_{n,m}\\). Supongamos que tiene regiones \\(R_{j,m}\\). Resolvemos (optimizamos directamente el valor que toma el árbol en cada región - este es un problema univariado, más fácil de resolver) \\[\\gamma_{j,m} = argmin_\\gamma \\sum_{x^{(i)}\\in R_{j,m}} L(y^{(i)},f_{m-1}(x^{i})+\\gamma )\\] para cada región \\(R_{j,m}\\) del árbol del inciso anterior. Actualizamos \\[f_m (x) = f_{m-1}(x) + \\sum_j \\gamma_{j,m} I(x\\in R_{j,m})\\] El predictor final es \\(f_M(x)\\). 12.7 Funciones de pérdida Para aplicar gradient boosting, tenemos primero que poder calcular el gradiente de la función de pérdida. Algunos ejemplos populares son: Pérdida cuadrática: \\(L(y,f(x))=(y-f(x))^2\\), \\(\\frac{\\partial L}{\\partial z} = -2(y-f(x))\\). Pérdida absoluta (más robusta a atípicos que la cuadrática) \\(L(y,f(x))=|y-f(x)|\\), \\(\\frac{\\partial L}{\\partial z} = signo(y-f(x))\\). Devianza binomial \\(L(y, f(x))\\) = -(1+e^{-yf(x)}), \\(y\\in\\{-1,1\\}\\), \\(\\frac{\\partial L}{\\partial z} = I(y=1) - h(f(x))\\). Adaboost, pérdida exponencial (para clasificación) \\(L(y,z) = e^{-yf(x)}\\), \\(y\\in\\{-1,1\\}\\), \\(\\frac{\\partial L}{\\partial z} = -ye^{-yf(x)}\\). 12.7.1 Discusión: adaboost (opcional) Adaboost es uno de los algoritmos originales para boosting, y no es necesario usar gradient boosting para aplicarlo. La razón es que los árboles de clasificación \\(T(x)\\) toman valores \\(T(x)\\in \\{-1,1\\}\\), y el paso de optimización (12.1) de cada árbol queda \\[T = argmin_{T} \\sum_{i=1}^N e^{-y^{(i)}f_{m-1}(x^{(i)})} e^{-y^{(i)}T(x^{(i)})} \\] \\[T = argmin_{T} \\sum_{i=1}^N d_{m,i} e^{-y^{(i)}T(x^{(i)})} \\] De modo que la función objetivo toma dos valores: Si \\(T(x^{i})\\) clasifica correctamente, entonces \\(e^{-y^{(i)}T(x^{(i)})}=e^{-1}\\), y si clasifica incorrectamente \\(e^{-y^{(i)}T(x^{(i)})}=e^{1}\\). Podemos entonces encontrar el árbol \\(T\\) construyendo un árbol usual pero con datos ponderados por \\(d_{m,i}\\), donde buscamos maximizar la tasa de clasificación correcta (puedes ver más en nuestro libro de texto, o en (Hastie, Tibshirani, and Friedman 2017). ¿Cuáles son las consecuencias de usar la pérdida exponencial? Una es que perdemos la conexión con los modelos logísticos e interpretación de probabilidad que tenemos cuando usamos la devianza. Sin embargo, son similares: compara cómo se ve la devianza (como la formulamos arriba, con \\(y\\in\\{-1,1\\}\\)) con la pérdida exponencial. Ejemplo Podemos usar el paquete de R gbm para hacer gradient boosting. Para el caso de precios de casas de la sección anterior (un problema de regresión). Fijaremos el número de árboles en 200, de profundidad 3, usando 75% de la muestra para entrenar y el restante para validación: library(gbm) entrena &lt;- read_rds(&#39;datos/ameshousing-entrena-procesado.rds&#39;) set.seed(23411) ajustar_boost &lt;- function(entrena, ...){ mod_boosting &lt;- gbm(log(vSalePrice) ~., data = entrena, distribution = &#39;gaussian&#39;, n.trees = 200, interaction.depth = 3, shrinkage = 1, # tasa de aprendizaje bag.fraction = 1, train.fraction = 0.75) mod_boosting } house_boosting &lt;- ajustar_boost(entrena) dat_entrenamiento &lt;- data_frame(entrena = sqrt(house_boosting$train.error), valida = sqrt(house_boosting$valid.error), n_arbol = 1:length(house_boosting$train.error)) %&gt;% gather(tipo, valor, -n_arbol) print(house_boosting) ## gbm(formula = log(vSalePrice) ~ ., distribution = &quot;gaussian&quot;, ## data = entrena, n.trees = 200, interaction.depth = 3, shrinkage = 1, ## bag.fraction = 1, train.fraction = 0.75) ## A gradient boosted model with gaussian loss function. ## 200 iterations were performed. ## The best test-set iteration was 20. ## There were 79 predictors of which 28 had non-zero influence. ggplot(dat_entrenamiento, aes(x=n_arbol, y=valor, colour=tipo, group=tipo)) + geom_line() Que se puede graficar también así: gbm.perf(house_boosting) ## Using test method... ## [1] 20 Como vemos, tenemos que afinar los parámetros del algoritmo. 12.8 Modificaciones de Gradient Boosting Hay algunas adiciones al algoritmo de gradient boosting que podemos usar para mejorar el desempeño. Los dos métodos que comunmente se usan son encogimiento (shrinkage), que es una especie de tasa de aprendizaje, y submuestreo, donde construimos cada árbol adicional usando una submuestra de la muestra de entrenamiento. Ambas podemos verlas como técnicas de regularización, que limitan sobreajuste producido por el algoritmo agresivo de boosting. 12.8.1 Tasa de aprendizaje (shrinkage) Funciona bien modificar el algoritmo usando una tasa de aprendizae \\(0&lt;\\nu&lt;1\\): \\[f_m(x) = f_{m-1}(x) + \\nu \\sum_j \\gamma_{j,m} I(x\\in R_{j,m})\\] Este parámetro sirve como una manera de evitar sobreajuste rápido cuando construimos los predictores. Si este número es muy alto, podemos sobreajustar rápidamente con pocos árboles, y terminar con predictor de varianza alta. Si este número es muy bajo, puede ser que necesitemos demasiadas iteraciones para llegar a buen desempeño. Igualmente se prueba con varios valores de \\(0&lt;\\nu&lt;1\\) (típicamente \\(\\nu&lt;0.1\\)) para mejorar el desempeño en validación. Nota: cuando hacemos \\(\\nu\\) más chica, es necesario hacer \\(M\\) más grande (correr más árboles) para obtener desempeño óptimo. Veamos que efecto tiene en nuestro ejemplo: modelos_dat &lt;- data_frame(n_modelo = 1:4, shrinkage = c(0.05, 0.1, 0.5, 1)) modelos_dat &lt;- modelos_dat %&gt;% mutate(modelo = map(shrinkage, boost)) %&gt;% mutate(eval = map(modelo, eval_modelo)) modelos_dat ## # A tibble: 4 x 4 ## n_modelo shrinkage modelo eval ## &lt;int&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; ## 1 1 0.05 &lt;S3: gbm&gt; &lt;tibble [1,000 x 3]&gt; ## 2 2 0.10 &lt;S3: gbm&gt; &lt;tibble [1,000 x 3]&gt; ## 3 3 0.50 &lt;S3: gbm&gt; &lt;tibble [1,000 x 3]&gt; ## 4 4 1.00 &lt;S3: gbm&gt; &lt;tibble [1,000 x 3]&gt; graf_eval &lt;- modelos_dat %&gt;% select(shrinkage, eval) %&gt;% unnest graf_eval ## # A tibble: 4,000 x 4 ## shrinkage n_arbol tipo valor ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 0.05 1 entrena 0.3914774 ## 2 0.05 2 entrena 0.3795092 ## 3 0.05 3 entrena 0.3683144 ## 4 0.05 4 entrena 0.3578551 ## 5 0.05 5 entrena 0.3480027 ## 6 0.05 6 entrena 0.3387286 ## 7 0.05 7 entrena 0.3300476 ## 8 0.05 8 entrena 0.3210405 ## 9 0.05 9 entrena 0.3127008 ## 10 0.05 10 entrena 0.3047083 ## # ... with 3,990 more rows ggplot(filter(graf_eval, tipo==&#39;valida&#39;), aes(x = n_arbol, y= valor, colour=factor(shrinkage), group = shrinkage)) + geom_line() + facet_wrap(~tipo) Obsérvese que podemos obtener un mejor resultado de validación afinando la tasa de aprendizaje. Cuando es muy grande, el modelo rápidamente sobreajusta cuando agregamos árboles. Si la tasa es demasiado chica, podos tardar mucho en llegar a un predictor de buen desempeño. ¿Cómo crees que se ven las gráfica de error de entrenamiento? 12.8.2 Submuestreo (bag.fraction) Funciona bien construir cada uno de los árboles con submuestras de la muestra de entrenamiento, como una manera adicional de reducir varianza al construir nuestro predictor (esta idea es parecida a la de los bosques aleatorios, aquí igualmente perturbamos la muestra de entrenamiento en cada paso para evitar sobreajuste). Adicionalmente, este proceso acelera considerablemente las iteraciones de boosting, y en algunos casos sin penalización en desempeño. En boosting generalmente se toman submuestras (una fracción de alrededor de 0.5 de la muestra de entrenamiento, pero puede ser más chica para conjuntos grandes de entrenamiento) sin reemplazo. Este parámetro también puede ser afinado con muestra de validación o validación cruzada. boost &lt;- ajustar_boost(entrena) modelos_dat &lt;- data_frame(n_modelo = 1:3, bag.fraction = c(0.25, 0.5, 1), shrinkage = 0.25) modelos_dat &lt;- modelos_dat %&gt;% mutate(modelo = pmap(., boost)) %&gt;% mutate(eval = map(modelo, eval_modelo)) modelos_dat ## # A tibble: 3 x 5 ## n_modelo bag.fraction shrinkage modelo eval ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; ## 1 1 0.25 0.25 &lt;S3: gbm&gt; &lt;tibble [1,000 x 3]&gt; ## 2 2 0.50 0.25 &lt;S3: gbm&gt; &lt;tibble [1,000 x 3]&gt; ## 3 3 1.00 0.25 &lt;S3: gbm&gt; &lt;tibble [1,000 x 3]&gt; graf_eval &lt;- modelos_dat %&gt;% select(bag.fraction, eval) %&gt;% unnest graf_eval ## # A tibble: 3,000 x 4 ## bag.fraction n_arbol tipo valor ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 0.25 1 entrena 0.3404206 ## 2 0.25 2 entrena 0.2948882 ## 3 0.25 3 entrena 0.2582633 ## 4 0.25 4 entrena 0.2316890 ## 5 0.25 5 entrena 0.2106434 ## 6 0.25 6 entrena 0.1946327 ## 7 0.25 7 entrena 0.1825413 ## 8 0.25 8 entrena 0.1733467 ## 9 0.25 9 entrena 0.1652359 ## 10 0.25 10 entrena 0.1596591 ## # ... with 2,990 more rows ggplot((graf_eval), aes(x = n_arbol, y= valor, colour=factor(bag.fraction), group = bag.fraction)) + geom_line() + facet_wrap(~tipo, ncol = 1) En este ejemplo, podemos reducir el tiempo de ajuste usando una fracción de submuestro de 0.5, con quizá algunas mejoras en desempeño. Ahora veamos los dos parámetros actuando en conjunto: modelos_dat &lt;- list(bag.fraction = c(0.1, 0.25, 0.5, 1), shrinkage = c(0.01, 0.1, 0.25, 0.5)) %&gt;% expand.grid modelos_dat &lt;- modelos_dat %&gt;% mutate(modelo = pmap(., boost)) %&gt;% mutate(eval = map(modelo, eval_modelo)) graf_eval &lt;- modelos_dat %&gt;% select(shrinkage, bag.fraction, eval) %&gt;% unnest head(graf_eval) ## shrinkage bag.fraction n_arbol tipo valor ## 1 0.01 0.1 1 entrena 0.4016655 ## 2 0.01 0.1 2 entrena 0.3991252 ## 3 0.01 0.1 3 entrena 0.3964301 ## 4 0.01 0.1 4 entrena 0.3942135 ## 5 0.01 0.1 5 entrena 0.3914665 ## 6 0.01 0.1 6 entrena 0.3891097 ggplot(filter(graf_eval, tipo ==&#39;valida&#39;), aes(x = n_arbol, y= valor, colour=factor(bag.fraction), group = bag.fraction)) + geom_line() + facet_wrap(~shrinkage) Bag fraction demasiado chico no funciona bien, especialmente si la tasa de aprendizaje es alta (¿Por qué?). Filtremos para ver con detalle el resto de los datos: ggplot(filter(graf_eval, tipo ==&#39;valida&#39;, bag.fraction&gt;0.1), aes(x = n_arbol, y= valor, colour=factor(bag.fraction), group = bag.fraction)) + geom_line() + facet_wrap(~shrinkage) + scale_y_log10() Y parece ser que para este número de iteraciones, una tasa de aprendizaje de 0.1 junto con un bag fraction de 0.5 funciona bien: graf_eval %&gt;% filter(tipo==&#39;valida&#39;) %&gt;% group_by(shrinkage, bag.fraction) %&gt;% summarise(valor = min(valor)) %&gt;% arrange(valor) %&gt;% head(10) ## # A tibble: 10 x 3 ## # Groups: shrinkage [3] ## shrinkage bag.fraction valor ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.10 0.50 0.1246740 ## 2 0.25 0.50 0.1283885 ## 3 0.10 1.00 0.1305941 ## 4 0.10 0.25 0.1310472 ## 5 0.01 0.25 0.1311681 ## 6 0.10 0.10 0.1320537 ## 7 0.25 0.25 0.1333946 ## 8 0.25 1.00 0.1349612 ## 9 0.01 0.50 0.1366676 ## 10 0.01 0.10 0.1367777 12.8.3 Número de árboles M Se monitorea el error sobre una muestra de validación cuando agregamos cada árboles. Escogemos el número de árboles de manera que minimize el error de validación. Demasiados árboles pueden producir sobreajuste. Ver el ejemplo de arriba. 12.8.4 Tamaño de árboles Los árboles se construyen de tamaño fijo \\(J\\), donde \\(J\\) es el número de cortes. Usualmente \\(J=1,2,\\ldots, 10\\), y es un parámetro que hay que elegir. \\(J\\) más grande permite interacciones de orden más alto entre las variables de entrada. Se intenta con varias \\(J\\) y \\(M\\) para minimizar el error de vaidación. 12.8.5 Controlar número de casos para cortes Igual que en bosques aleatorios, podemos establecer mínimos de muestra en nodos terminales, o mínimo de casos necesarios para hacer un corte. Ejemplo modelos_dat &lt;- list(bag.fraction = c( 0.25, 0.5, 1), shrinkage = c(0.01, 0.1, 0.25, 0.5), depth = c(1,5,10,12)) %&gt;% expand.grid modelos_dat &lt;- modelos_dat %&gt;% mutate(modelo = pmap(., boost)) %&gt;% mutate(eval = map(modelo, eval_modelo)) graf_eval &lt;- modelos_dat %&gt;% select(shrinkage, bag.fraction, depth, eval) %&gt;% unnest ggplot(filter(graf_eval, tipo ==&#39;valida&#39;), aes(x = n_arbol, y= valor, colour=factor(bag.fraction), group = bag.fraction)) + geom_line() + facet_grid(depth~shrinkage) + scale_y_log10() Podemos ver con más detalle donde ocurre el mejor desempeño: ggplot(filter(graf_eval, tipo ==&#39;valida&#39;, shrinkage == 0.1, n_arbol&gt;100), aes(x = n_arbol, y= valor, colour=factor(bag.fraction), group = bag.fraction)) + geom_line() + facet_grid(depth~shrinkage) head(arrange(filter(graf_eval,tipo==&#39;valida&#39;), valor)) ## shrinkage bag.fraction depth n_arbol tipo valor ## 1 0.1 0.5 10 98 valida 0.1218075 ## 2 0.1 0.5 10 95 valida 0.1218644 ## 3 0.1 0.5 10 97 valida 0.1218835 ## 4 0.1 0.5 10 96 valida 0.1219046 ## 5 0.1 0.5 10 100 valida 0.1220013 ## 6 0.1 0.5 10 94 valida 0.1220148 12.8.6 Evaluación con validación cruzada. Para datos no muy grandes, conviene escoger modelos usando validación cruzada. Por ejemplo, set.seed(9983) rm(&#39;modelos_dat&#39;) mod_boosting &lt;- gbm(log(vSalePrice) ~., data = entrena, distribution = &#39;gaussian&#39;, n.trees = 200, interaction.depth = 10, shrinkage = 0.1, # tasa de aprendizaje bag.fraction = 0.5, cv.folds = 10) gbm.perf(mod_boosting) eval_modelo_2 &lt;- function(modelo){ dat_eval &lt;- data_frame(entrena = sqrt(modelo$train.error), valida = sqrt(modelo$cv.error), n_arbol = 1:length(modelo$train.error)) %&gt;% gather(tipo, valor, -n_arbol) dat_eval } dat &lt;- eval_modelo_2(mod_boosting) sqrt(min(mod_boosting$cv.error)) ggplot(dat, aes(x = n_arbol, y=valor, colour=tipo, group=tipo)) + geom_line() 12.9 Gráficas de dependencia parcial La idea de dependencia parcial que veremos a continuación se puede aplicar a cualquier método de aprendizaje, y en boosting ayuda a entender el funcionamiento del predictor complejo que resulta del algoritmo. Aunque podemos evaluar el predictor en distintos valores y observar cómo se comporta, cuando tenemos varias variables de entrada este proceso no siempre tiene resultados muy claros o completos. Dependencia parcial es un intento por entender de manera más sistemática parte del funcionamiento de un modelo complejo. 12.9.1 Dependencia parcial Supongamos que tenemos un predictor \\(f(x_1,x_2)\\) que depende de dos variables de entrada. Podemos considerar la función \\[{f}_{1}(x_1) = E_{x_2}[f(x_1,x_2)],\\] que es el promedio de \\(f(x)\\) fijando \\(x_1\\) sobre la marginal de \\(x_2\\). Si tenemos una muestra de entrenamiento, podríamos estimarla promediando sobre la muestra de entrenamiento \\[\\bar{f}_1(x_1) = \\frac{1}{n}\\sum_{i=1}^n f(x_1, x_2^{(i)}),\\] que consiste en fijar el valor de \\(x_1\\) y promediar sobre todos los valores de la muestra de entrenamiento para \\(x_2\\). Ejemplo Construimos un modelo con solamente tres variables para nuestro ejemplo anterior mod_2 &lt;- gbm(log(vSalePrice) ~ vGrLivArea +vNeighborhood +vOverallQual, data = entrena, distribution = &#39;gaussian&#39;, n.trees = 100, interaction.depth = 3, shrinkage = 0.1, bag.fraction = 0.5, train.fraction = 0.75) gbm.perf(mod_2) ## Using test method... ## [1] 87 Podemos calcular a mano la gráfica de dependencia parcial para el tamaño de la “General Living Area”. dat_dp &lt;- entrena %&gt;% select(vGrLivArea, vNeighborhood, vOverallQual) Consideramos el rango de la variable: cuantiles &lt;- quantile(entrena$vGrLivArea, probs= seq(0, 1, 0.1)) cuantiles ## 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% ## 334.0 912.0 1066.6 1208.0 1339.0 1464.0 1578.0 1709.3 1869.0 2158.3 ## 100% ## 5642.0 Por ejemplo, vamos evaluar el efecto parcial cuando vGrLivArea = 912. Hacemos dat_dp_1 &lt;- dat_dp %&gt;% mutate(vGrLivArea = 912) %&gt;% mutate(pred = predict(mod_2, .)) %&gt;% summarise(mean_pred = mean(pred)) ## Using 87 trees... dat_dp_1 ## mean_pred ## 1 11.84206 Evaluamos en vGrLivArea = 912 dat_dp_1 &lt;- dat_dp %&gt;% mutate(vGrLivArea = 1208) %&gt;% mutate(pred = predict(mod_2, .)) %&gt;% summarise(mean_pred = mean(pred)) ## Using 87 trees... dat_dp_1 ## mean_pred ## 1 11.96576 (un incremento de alrededor del 10% en el precio de venta). Hacemos todos los cuantiles como sigue: cuantiles &lt;- quantile(entrena$vGrLivArea, probs= seq(0, 1, 0.01)) prom_parcial &lt;- function(x, variable, df, mod){ variable &lt;- enquo(variable) variable_nom &lt;- quo_name(variable) salida &lt;- df %&gt;% mutate(!!variable_nom := x) %&gt;% mutate(pred = predict(mod, ., n.trees=100)) %&gt;% group_by(!!variable) %&gt;% summarise(f_1 = mean(pred)) salida } dep_parcial &lt;- map_dfr(cuantiles, ~prom_parcial(.x, vGrLivArea, entrena, mod_2)) ggplot(dep_parcial, aes(x=vGrLivArea, y= f_1)) + geom_line() + geom_line() + geom_rug(sides=&#39;b&#39;) Y transformando a las unidades originales ggplot(dep_parcial, aes(x=vGrLivArea, y= exp(f_1))) + geom_line() + geom_line() + geom_rug(sides=&#39;b&#39;) Y vemos que cuando aumenta el area de habitación, aumenta el precio. Podemos hacer esta gráfica más simple haciendo plot(mod_2, 1) # 1 pues es vGrLivArea la primer variable Y para una variable categórica se ve como sigue: plot(mod_2, 2, return.grid = TRUE) %&gt;% arrange(y) ## vNeighborhood y ## 1 IDOTRR 11.74437 ## 2 BrDale 11.79033 ## 3 OldTown 11.81014 ## 4 SWISU 11.85419 ## 5 MeadowV 11.85748 ## 6 Edwards 11.86859 ## 7 BrkSide 11.91787 ## 8 Otros 11.95479 ## 9 NAmes 11.98455 ## 10 Sawyer 12.00362 ## 11 SawyerW 12.03340 ## 12 Mitchel 12.06104 ## 13 NWAmes 12.08210 ## 14 Crawfor 12.08735 ## 15 Gilbert 12.09874 ## 16 Blmngtn 12.11353 ## 17 CollgCr 12.12275 ## 18 Somerst 12.15674 ## 19 Timber 12.17366 ## 20 ClearCr 12.18768 ## 21 NoRidge 12.20368 ## 22 StoneBr 12.22289 ## 23 NridgHt 12.24190 ## 24 Veenker 12.25304 plot(mod_2, 2, return.grid = FALSE) En general, si nuestro predictor depende de más variables \\(f(x_1,x_2, \\ldots, x_p)\\) entrada. Podemos considerar las funciones \\[{f}_{j}(x_j) = E_{(x_1,x_2, \\ldots x_p) - x_j}[f(x_1,x_2, \\ldots, x_p)],\\] que es el valor esperado de \\(f(x)\\) fijando \\(x_j\\), y promediando sobre el resto de las variables. Si tenemos una muestra de entrenamiento, podríamos estimarla promediando sobre la muestra de entrenamiento \\[\\bar{f}_j(x_j) = \\frac{1}{n}\\sum_{i=1}^n f(x_1^{(i)}, x_2^{(i)}, \\ldots, x_{j-1}^{(i)},x_{j+1}^{(i)},\\ldots, x_p^{(i)}).\\] Podemos hacer también gráficas de dependencia parcial para más de una variable, si fijamos un subconjunto de variables y promediamos sobre el resto. plot(mod_2, c(1,3)) 12.9.2 Discusión En primer lugar, veamos qué obtenemos de la dependencia parcial cuando aplicamos al modelo lineal sin interacciones. En el caso de dos variables, \\[f_1(x_1) = E_{x_2}[f(x_1,x_2)] =E_{x_2}[a + bx_1 + cx_2)] = \\mu + bx_1,\\] que es equivalente al análisis marginal que hacemos en regresión lineal ( incrementos en la variable \\(x_1\\) con todo lo demás fijo, donde el incremento marginal de la respuesta es el coeficiente \\(b\\)). Desde este punto de vista, dependencia parcial da una interpretación similar a la del análisis usual de coeficientes en regresión lineal, donde pensamos en “todo lo demás constante”. Nótese también que cuando hay interacciones fuertes entre las variables, ningún análisis marginal (dependencia parcial o examen de coeficientes) da un resultado fácilmente interpretable - la única solución es considerar el efecto conjunto de las variables que interactúan. De modo que este tipo de análisis funciona mejor cuando no hay interacciones grandes entre las variables (es cercano a un modelo aditivo con efectos no lineales). Ejemplo Considera qué pasa con las gráficas de dependencia parcial cuando \\(f(x_1,x_2) = -10 x_1x_2\\), y \\(x_1\\) y \\(x_2\\) tienen media cero. Explica por qué en este caso es mejor ver el efecto conjunto de las dos variables. Es importante también evitar la interpretación incorrecta de que la función de dependencia parcial da el valor esperado del predictor condicionado a valores de la variable cuya dependencia examinamos. Es decir, \\[f_1(x_1) = E_{x_2}(f(x_1,x_2)) \\neq E(f(x_1,x_2)|x_1).\\] La última cantidad es un valor esperado diferente (calculado sobre la condicional de \\(x_2\\) dada \\(x_1\\)), de manera que utiliza información acerca de la relación que hay entre \\(x_1\\) y \\(x_2\\). La función de dependencia parcial da el efecto de \\(x_1\\) tomando en cuenta los efectos promedio de las otras variables. 12.10 xgboost y gbm Los paquetes xgboost y gbm parecen ser los más populares para hacer gradient boosting. xgboost, adicionalmente, parece ser más rápido y más flexible que gbm (paralelización, uso de GPU integrado). Existe una lista considerable de competencias de predicción donde el algoritmo/implementación ganadora es xgboost. library(xgboost) ## ## Attaching package: &#39;xgboost&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## slice x &lt;- entrena %&gt;% select(-vSalePrice) %&gt;% model.matrix(~., .) x_entrena &lt;- x[1:1100, ] x_valida &lt;- x[1101:1460, ] set.seed(1293) d_entrena &lt;- xgb.DMatrix(x_entrena, label = log(entrena$vSalePrice[1:1100])) d_valida &lt;- xgb.DMatrix(x_valida, label = log(entrena$vSalePrice[1101:1460])) watchlist &lt;- list(eval = d_valida, train = d_entrena) params &lt;- list(booster = &quot;gbtree&quot;, max_depth = 3, eta = 0.03, nthread = 1, subsample = 0.75, lambda = 0.001, objective = &quot;reg:linear&quot;, eval_metric = &quot;rmse&quot;) bst &lt;- xgb.train(params, d_entrena, nrounds = 1000, watchlist = watchlist, verbose=1) ## [1] eval-rmse:11.183367 train-rmse:11.185970 ## [2] eval-rmse:10.848030 train-rmse:10.850441 ## [3] eval-rmse:10.522327 train-rmse:10.524883 ## [4] eval-rmse:10.206756 train-rmse:10.209215 ## [5] eval-rmse:9.901018 train-rmse:9.903183 ## [6] eval-rmse:9.604361 train-rmse:9.606353 ## [7] eval-rmse:9.316499 train-rmse:9.318384 ## [8] eval-rmse:9.037509 train-rmse:9.038843 ## [9] eval-rmse:8.766625 train-rmse:8.767856 ## [10] eval-rmse:8.504042 train-rmse:8.504976 ## [11] eval-rmse:8.249103 train-rmse:8.249992 ## [12] eval-rmse:8.001815 train-rmse:8.002570 ## [13] eval-rmse:7.762049 train-rmse:7.762605 ## [14] eval-rmse:7.529464 train-rmse:7.529853 ## [15] eval-rmse:7.303868 train-rmse:7.304183 ## [16] eval-rmse:7.085173 train-rmse:7.085240 ## [17] eval-rmse:6.873233 train-rmse:6.872812 ## [18] eval-rmse:6.667545 train-rmse:6.666743 ## [19] eval-rmse:6.467870 train-rmse:6.466797 ## [20] eval-rmse:6.274136 train-rmse:6.272933 ## [21] eval-rmse:6.085811 train-rmse:6.084670 ## [22] eval-rmse:5.903286 train-rmse:5.902312 ## [23] eval-rmse:5.726332 train-rmse:5.725288 ## [24] eval-rmse:5.555059 train-rmse:5.553725 ## [25] eval-rmse:5.388524 train-rmse:5.387316 ## [26] eval-rmse:5.226866 train-rmse:5.225660 ## [27] eval-rmse:5.070144 train-rmse:5.069087 ## [28] eval-rmse:4.918210 train-rmse:4.917084 ## [29] eval-rmse:4.770644 train-rmse:4.769682 ## [30] eval-rmse:4.627788 train-rmse:4.626754 ## [31] eval-rmse:4.489161 train-rmse:4.488088 ## [32] eval-rmse:4.354557 train-rmse:4.353536 ## [33] eval-rmse:4.223987 train-rmse:4.223086 ## [34] eval-rmse:4.097580 train-rmse:4.096651 ## [35] eval-rmse:3.974845 train-rmse:3.973879 ## [36] eval-rmse:3.855764 train-rmse:3.854761 ## [37] eval-rmse:3.740262 train-rmse:3.739241 ## [38] eval-rmse:3.628328 train-rmse:3.627258 ## [39] eval-rmse:3.520058 train-rmse:3.518757 ## [40] eval-rmse:3.414457 train-rmse:3.413175 ## [41] eval-rmse:3.312317 train-rmse:3.310966 ## [42] eval-rmse:3.213198 train-rmse:3.211770 ## [43] eval-rmse:3.117117 train-rmse:3.115627 ## [44] eval-rmse:3.023922 train-rmse:3.022294 ## [45] eval-rmse:2.933381 train-rmse:2.931745 ## [46] eval-rmse:2.845738 train-rmse:2.843973 ## [47] eval-rmse:2.760700 train-rmse:2.758906 ## [48] eval-rmse:2.678176 train-rmse:2.676355 ## [49] eval-rmse:2.598284 train-rmse:2.596345 ## [50] eval-rmse:2.520782 train-rmse:2.518759 ## [51] eval-rmse:2.445457 train-rmse:2.443495 ## [52] eval-rmse:2.372283 train-rmse:2.370391 ## [53] eval-rmse:2.301332 train-rmse:2.299367 ## [54] eval-rmse:2.232533 train-rmse:2.230658 ## [55] eval-rmse:2.166043 train-rmse:2.163950 ## [56] eval-rmse:2.101366 train-rmse:2.099274 ## [57] eval-rmse:2.038792 train-rmse:2.036566 ## [58] eval-rmse:1.978027 train-rmse:1.975905 ## [59] eval-rmse:1.919023 train-rmse:1.916914 ## [60] eval-rmse:1.861887 train-rmse:1.859752 ## [61] eval-rmse:1.806416 train-rmse:1.804133 ## [62] eval-rmse:1.752722 train-rmse:1.750435 ## [63] eval-rmse:1.700586 train-rmse:1.698211 ## [64] eval-rmse:1.649718 train-rmse:1.647352 ## [65] eval-rmse:1.600789 train-rmse:1.598278 ## [66] eval-rmse:1.553377 train-rmse:1.550703 ## [67] eval-rmse:1.507320 train-rmse:1.504523 ## [68] eval-rmse:1.462657 train-rmse:1.459791 ## [69] eval-rmse:1.418975 train-rmse:1.416199 ## [70] eval-rmse:1.377086 train-rmse:1.374123 ## [71] eval-rmse:1.336371 train-rmse:1.333250 ## [72] eval-rmse:1.296991 train-rmse:1.293647 ## [73] eval-rmse:1.258685 train-rmse:1.255167 ## [74] eval-rmse:1.221414 train-rmse:1.217896 ## [75] eval-rmse:1.185352 train-rmse:1.181752 ## [76] eval-rmse:1.150467 train-rmse:1.146795 ## [77] eval-rmse:1.116551 train-rmse:1.112809 ## [78] eval-rmse:1.083800 train-rmse:1.079873 ## [79] eval-rmse:1.051880 train-rmse:1.047876 ## [80] eval-rmse:1.021156 train-rmse:1.016918 ## [81] eval-rmse:0.991035 train-rmse:0.986742 ## [82] eval-rmse:0.962071 train-rmse:0.957656 ## [83] eval-rmse:0.933817 train-rmse:0.929293 ## [84] eval-rmse:0.906693 train-rmse:0.901907 ## [85] eval-rmse:0.880243 train-rmse:0.875294 ## [86] eval-rmse:0.854782 train-rmse:0.849542 ## [87] eval-rmse:0.830050 train-rmse:0.824504 ## [88] eval-rmse:0.805921 train-rmse:0.800332 ## [89] eval-rmse:0.782437 train-rmse:0.776819 ## [90] eval-rmse:0.759826 train-rmse:0.754110 ## [91] eval-rmse:0.737987 train-rmse:0.731999 ## [92] eval-rmse:0.716596 train-rmse:0.710499 ## [93] eval-rmse:0.695845 train-rmse:0.689677 ## [94] eval-rmse:0.675893 train-rmse:0.669581 ## [95] eval-rmse:0.656634 train-rmse:0.650188 ## [96] eval-rmse:0.637819 train-rmse:0.631238 ## [97] eval-rmse:0.619622 train-rmse:0.612868 ## [98] eval-rmse:0.602074 train-rmse:0.595118 ## [99] eval-rmse:0.585125 train-rmse:0.577886 ## [100] eval-rmse:0.568653 train-rmse:0.561235 ## [101] eval-rmse:0.552748 train-rmse:0.545138 ## [102] eval-rmse:0.537232 train-rmse:0.529475 ## [103] eval-rmse:0.522264 train-rmse:0.514420 ## [104] eval-rmse:0.507737 train-rmse:0.499568 ## [105] eval-rmse:0.493683 train-rmse:0.485270 ## [106] eval-rmse:0.480088 train-rmse:0.471373 ## [107] eval-rmse:0.467011 train-rmse:0.458003 ## [108] eval-rmse:0.454471 train-rmse:0.445108 ## [109] eval-rmse:0.442196 train-rmse:0.432627 ## [110] eval-rmse:0.430161 train-rmse:0.420445 ## [111] eval-rmse:0.418728 train-rmse:0.408734 ## [112] eval-rmse:0.407583 train-rmse:0.397402 ## [113] eval-rmse:0.396891 train-rmse:0.386446 ## [114] eval-rmse:0.386403 train-rmse:0.375683 ## [115] eval-rmse:0.376224 train-rmse:0.365231 ## [116] eval-rmse:0.366555 train-rmse:0.355268 ## [117] eval-rmse:0.357100 train-rmse:0.345474 ## [118] eval-rmse:0.348049 train-rmse:0.336072 ## [119] eval-rmse:0.339081 train-rmse:0.326911 ## [120] eval-rmse:0.330555 train-rmse:0.318151 ## [121] eval-rmse:0.322332 train-rmse:0.309662 ## [122] eval-rmse:0.314413 train-rmse:0.301313 ## [123] eval-rmse:0.306936 train-rmse:0.293350 ## [124] eval-rmse:0.299666 train-rmse:0.285707 ## [125] eval-rmse:0.292627 train-rmse:0.278202 ## [126] eval-rmse:0.285736 train-rmse:0.271015 ## [127] eval-rmse:0.279120 train-rmse:0.263965 ## [128] eval-rmse:0.272776 train-rmse:0.257218 ## [129] eval-rmse:0.266653 train-rmse:0.250740 ## [130] eval-rmse:0.260639 train-rmse:0.244375 ## [131] eval-rmse:0.254979 train-rmse:0.238334 ## [132] eval-rmse:0.249520 train-rmse:0.232493 ## [133] eval-rmse:0.244305 train-rmse:0.226824 ## [134] eval-rmse:0.239293 train-rmse:0.221404 ## [135] eval-rmse:0.234296 train-rmse:0.216137 ## [136] eval-rmse:0.229649 train-rmse:0.211090 ## [137] eval-rmse:0.225018 train-rmse:0.206201 ## [138] eval-rmse:0.220736 train-rmse:0.201420 ## [139] eval-rmse:0.216580 train-rmse:0.196735 ## [140] eval-rmse:0.212492 train-rmse:0.192254 ## [141] eval-rmse:0.208535 train-rmse:0.187981 ## [142] eval-rmse:0.204758 train-rmse:0.183909 ## [143] eval-rmse:0.201153 train-rmse:0.179919 ## [144] eval-rmse:0.197871 train-rmse:0.176156 ## [145] eval-rmse:0.194602 train-rmse:0.172538 ## [146] eval-rmse:0.191536 train-rmse:0.169072 ## [147] eval-rmse:0.188481 train-rmse:0.165594 ## [148] eval-rmse:0.185642 train-rmse:0.162233 ## [149] eval-rmse:0.182883 train-rmse:0.159094 ## [150] eval-rmse:0.180309 train-rmse:0.156101 ## [151] eval-rmse:0.177700 train-rmse:0.153151 ## [152] eval-rmse:0.175306 train-rmse:0.150395 ## [153] eval-rmse:0.173040 train-rmse:0.147771 ## [154] eval-rmse:0.170899 train-rmse:0.145262 ## [155] eval-rmse:0.168613 train-rmse:0.142754 ## [156] eval-rmse:0.166581 train-rmse:0.140343 ## [157] eval-rmse:0.164577 train-rmse:0.137980 ## [158] eval-rmse:0.162924 train-rmse:0.135876 ## [159] eval-rmse:0.161292 train-rmse:0.133715 ## [160] eval-rmse:0.159499 train-rmse:0.131702 ## [161] eval-rmse:0.157958 train-rmse:0.129754 ## [162] eval-rmse:0.156499 train-rmse:0.127893 ## [163] eval-rmse:0.155177 train-rmse:0.126165 ## [164] eval-rmse:0.153927 train-rmse:0.124461 ## [165] eval-rmse:0.152500 train-rmse:0.122801 ## [166] eval-rmse:0.151255 train-rmse:0.121221 ## [167] eval-rmse:0.150158 train-rmse:0.119813 ## [168] eval-rmse:0.149063 train-rmse:0.118423 ## [169] eval-rmse:0.148007 train-rmse:0.117068 ## [170] eval-rmse:0.147039 train-rmse:0.115699 ## [171] eval-rmse:0.146059 train-rmse:0.114400 ## [172] eval-rmse:0.145131 train-rmse:0.113225 ## [173] eval-rmse:0.144247 train-rmse:0.112032 ## [174] eval-rmse:0.143229 train-rmse:0.110904 ## [175] eval-rmse:0.142309 train-rmse:0.109833 ## [176] eval-rmse:0.141448 train-rmse:0.108782 ## [177] eval-rmse:0.140599 train-rmse:0.107804 ## [178] eval-rmse:0.139951 train-rmse:0.106835 ## [179] eval-rmse:0.139426 train-rmse:0.106012 ## [180] eval-rmse:0.138739 train-rmse:0.105105 ## [181] eval-rmse:0.138129 train-rmse:0.104319 ## [182] eval-rmse:0.137643 train-rmse:0.103520 ## [183] eval-rmse:0.137096 train-rmse:0.102771 ## [184] eval-rmse:0.136695 train-rmse:0.102021 ## [185] eval-rmse:0.136162 train-rmse:0.101332 ## [186] eval-rmse:0.135784 train-rmse:0.100630 ## [187] eval-rmse:0.135153 train-rmse:0.100006 ## [188] eval-rmse:0.134752 train-rmse:0.099359 ## [189] eval-rmse:0.134304 train-rmse:0.098714 ## [190] eval-rmse:0.133908 train-rmse:0.098198 ## [191] eval-rmse:0.133472 train-rmse:0.097655 ## [192] eval-rmse:0.133200 train-rmse:0.097155 ## [193] eval-rmse:0.132919 train-rmse:0.096691 ## [194] eval-rmse:0.132500 train-rmse:0.096222 ## [195] eval-rmse:0.132190 train-rmse:0.095725 ## [196] eval-rmse:0.131938 train-rmse:0.095303 ## [197] eval-rmse:0.131612 train-rmse:0.094883 ## [198] eval-rmse:0.131358 train-rmse:0.094440 ## [199] eval-rmse:0.131160 train-rmse:0.094012 ## [200] eval-rmse:0.130773 train-rmse:0.093560 ## [201] eval-rmse:0.130392 train-rmse:0.093120 ## [202] eval-rmse:0.130193 train-rmse:0.092782 ## [203] eval-rmse:0.130018 train-rmse:0.092456 ## [204] eval-rmse:0.129709 train-rmse:0.092056 ## [205] eval-rmse:0.129464 train-rmse:0.091781 ## [206] eval-rmse:0.129370 train-rmse:0.091516 ## [207] eval-rmse:0.129279 train-rmse:0.091226 ## [208] eval-rmse:0.129123 train-rmse:0.090931 ## [209] eval-rmse:0.128873 train-rmse:0.090689 ## [210] eval-rmse:0.128592 train-rmse:0.090422 ## [211] eval-rmse:0.128390 train-rmse:0.090174 ## [212] eval-rmse:0.128191 train-rmse:0.089976 ## [213] eval-rmse:0.128042 train-rmse:0.089730 ## [214] eval-rmse:0.127812 train-rmse:0.089486 ## [215] eval-rmse:0.127772 train-rmse:0.089292 ## [216] eval-rmse:0.127696 train-rmse:0.089040 ## [217] eval-rmse:0.127402 train-rmse:0.088843 ## [218] eval-rmse:0.127250 train-rmse:0.088664 ## [219] eval-rmse:0.127196 train-rmse:0.088456 ## [220] eval-rmse:0.127058 train-rmse:0.088213 ## [221] eval-rmse:0.126928 train-rmse:0.088038 ## [222] eval-rmse:0.126807 train-rmse:0.087839 ## [223] eval-rmse:0.126666 train-rmse:0.087630 ## [224] eval-rmse:0.126576 train-rmse:0.087455 ## [225] eval-rmse:0.126575 train-rmse:0.087251 ## [226] eval-rmse:0.126453 train-rmse:0.087012 ## [227] eval-rmse:0.126407 train-rmse:0.086894 ## [228] eval-rmse:0.126212 train-rmse:0.086739 ## [229] eval-rmse:0.126148 train-rmse:0.086609 ## [230] eval-rmse:0.126112 train-rmse:0.086461 ## [231] eval-rmse:0.125999 train-rmse:0.086229 ## [232] eval-rmse:0.125916 train-rmse:0.086092 ## [233] eval-rmse:0.125750 train-rmse:0.085931 ## [234] eval-rmse:0.125652 train-rmse:0.085789 ## [235] eval-rmse:0.125581 train-rmse:0.085694 ## [236] eval-rmse:0.125477 train-rmse:0.085538 ## [237] eval-rmse:0.125323 train-rmse:0.085363 ## [238] eval-rmse:0.125244 train-rmse:0.085206 ## [239] eval-rmse:0.125103 train-rmse:0.085010 ## [240] eval-rmse:0.125059 train-rmse:0.084839 ## [241] eval-rmse:0.124946 train-rmse:0.084698 ## [242] eval-rmse:0.124922 train-rmse:0.084563 ## [243] eval-rmse:0.124864 train-rmse:0.084399 ## [244] eval-rmse:0.124706 train-rmse:0.084260 ## [245] eval-rmse:0.124561 train-rmse:0.084146 ## [246] eval-rmse:0.124534 train-rmse:0.083968 ## [247] eval-rmse:0.124403 train-rmse:0.083835 ## [248] eval-rmse:0.124417 train-rmse:0.083757 ## [249] eval-rmse:0.124360 train-rmse:0.083682 ## [250] eval-rmse:0.124340 train-rmse:0.083557 ## [251] eval-rmse:0.124387 train-rmse:0.083407 ## [252] eval-rmse:0.124273 train-rmse:0.083294 ## [253] eval-rmse:0.124222 train-rmse:0.083200 ## [254] eval-rmse:0.124216 train-rmse:0.083046 ## [255] eval-rmse:0.124099 train-rmse:0.082856 ## [256] eval-rmse:0.124086 train-rmse:0.082729 ## [257] eval-rmse:0.124024 train-rmse:0.082578 ## [258] eval-rmse:0.123934 train-rmse:0.082505 ## [259] eval-rmse:0.123849 train-rmse:0.082396 ## [260] eval-rmse:0.123901 train-rmse:0.082174 ## [261] eval-rmse:0.123829 train-rmse:0.082047 ## [262] eval-rmse:0.123696 train-rmse:0.081944 ## [263] eval-rmse:0.123771 train-rmse:0.081764 ## [264] eval-rmse:0.123661 train-rmse:0.081604 ## [265] eval-rmse:0.123723 train-rmse:0.081512 ## [266] eval-rmse:0.123765 train-rmse:0.081384 ## [267] eval-rmse:0.123766 train-rmse:0.081230 ## [268] eval-rmse:0.123731 train-rmse:0.081143 ## [269] eval-rmse:0.123706 train-rmse:0.081044 ## [270] eval-rmse:0.123638 train-rmse:0.080949 ## [271] eval-rmse:0.123564 train-rmse:0.080852 ## [272] eval-rmse:0.123561 train-rmse:0.080704 ## [273] eval-rmse:0.123515 train-rmse:0.080591 ## [274] eval-rmse:0.123490 train-rmse:0.080461 ## [275] eval-rmse:0.123514 train-rmse:0.080345 ## [276] eval-rmse:0.123421 train-rmse:0.080258 ## [277] eval-rmse:0.123351 train-rmse:0.080180 ## [278] eval-rmse:0.123233 train-rmse:0.080123 ## [279] eval-rmse:0.123240 train-rmse:0.079971 ## [280] eval-rmse:0.123177 train-rmse:0.079863 ## [281] eval-rmse:0.123153 train-rmse:0.079803 ## [282] eval-rmse:0.123134 train-rmse:0.079740 ## [283] eval-rmse:0.123115 train-rmse:0.079607 ## [284] eval-rmse:0.123001 train-rmse:0.079496 ## [285] eval-rmse:0.123009 train-rmse:0.079360 ## [286] eval-rmse:0.122994 train-rmse:0.079154 ## [287] eval-rmse:0.122864 train-rmse:0.079067 ## [288] eval-rmse:0.122789 train-rmse:0.078955 ## [289] eval-rmse:0.122765 train-rmse:0.078871 ## [290] eval-rmse:0.122724 train-rmse:0.078797 ## [291] eval-rmse:0.122792 train-rmse:0.078655 ## [292] eval-rmse:0.122767 train-rmse:0.078552 ## [293] eval-rmse:0.122722 train-rmse:0.078462 ## [294] eval-rmse:0.122704 train-rmse:0.078383 ## [295] eval-rmse:0.122601 train-rmse:0.078297 ## [296] eval-rmse:0.122605 train-rmse:0.078228 ## [297] eval-rmse:0.122586 train-rmse:0.078139 ## [298] eval-rmse:0.122567 train-rmse:0.078061 ## [299] eval-rmse:0.122582 train-rmse:0.078019 ## [300] eval-rmse:0.122533 train-rmse:0.077904 ## [301] eval-rmse:0.122433 train-rmse:0.077847 ## [302] eval-rmse:0.122423 train-rmse:0.077783 ## [303] eval-rmse:0.122366 train-rmse:0.077723 ## [304] eval-rmse:0.122399 train-rmse:0.077579 ## [305] eval-rmse:0.122250 train-rmse:0.077434 ## [306] eval-rmse:0.122253 train-rmse:0.077337 ## [307] eval-rmse:0.122209 train-rmse:0.077224 ## [308] eval-rmse:0.122113 train-rmse:0.077034 ## [309] eval-rmse:0.122148 train-rmse:0.076988 ## [310] eval-rmse:0.122094 train-rmse:0.076833 ## [311] eval-rmse:0.122133 train-rmse:0.076676 ## [312] eval-rmse:0.122019 train-rmse:0.076605 ## [313] eval-rmse:0.122084 train-rmse:0.076458 ## [314] eval-rmse:0.122090 train-rmse:0.076424 ## [315] eval-rmse:0.122117 train-rmse:0.076371 ## [316] eval-rmse:0.122070 train-rmse:0.076279 ## [317] eval-rmse:0.122019 train-rmse:0.076147 ## [318] eval-rmse:0.121993 train-rmse:0.076030 ## [319] eval-rmse:0.122035 train-rmse:0.075942 ## [320] eval-rmse:0.121964 train-rmse:0.075850 ## [321] eval-rmse:0.121940 train-rmse:0.075687 ## [322] eval-rmse:0.121860 train-rmse:0.075602 ## [323] eval-rmse:0.121853 train-rmse:0.075555 ## [324] eval-rmse:0.121803 train-rmse:0.075502 ## [325] eval-rmse:0.121752 train-rmse:0.075335 ## [326] eval-rmse:0.121736 train-rmse:0.075250 ## [327] eval-rmse:0.121738 train-rmse:0.075189 ## [328] eval-rmse:0.121717 train-rmse:0.075117 ## [329] eval-rmse:0.121633 train-rmse:0.075041 ## [330] eval-rmse:0.121628 train-rmse:0.074992 ## [331] eval-rmse:0.121646 train-rmse:0.074922 ## [332] eval-rmse:0.121563 train-rmse:0.074865 ## [333] eval-rmse:0.121499 train-rmse:0.074779 ## [334] eval-rmse:0.121496 train-rmse:0.074722 ## [335] eval-rmse:0.121473 train-rmse:0.074687 ## [336] eval-rmse:0.121445 train-rmse:0.074597 ## [337] eval-rmse:0.121461 train-rmse:0.074565 ## [338] eval-rmse:0.121492 train-rmse:0.074494 ## [339] eval-rmse:0.121397 train-rmse:0.074431 ## [340] eval-rmse:0.121353 train-rmse:0.074378 ## [341] eval-rmse:0.121402 train-rmse:0.074337 ## [342] eval-rmse:0.121354 train-rmse:0.074212 ## [343] eval-rmse:0.121266 train-rmse:0.074134 ## [344] eval-rmse:0.121218 train-rmse:0.074059 ## [345] eval-rmse:0.121213 train-rmse:0.073919 ## [346] eval-rmse:0.121197 train-rmse:0.073782 ## [347] eval-rmse:0.121169 train-rmse:0.073707 ## [348] eval-rmse:0.121110 train-rmse:0.073593 ## [349] eval-rmse:0.120970 train-rmse:0.073566 ## [350] eval-rmse:0.120959 train-rmse:0.073479 ## [351] eval-rmse:0.120916 train-rmse:0.073416 ## [352] eval-rmse:0.120953 train-rmse:0.073364 ## [353] eval-rmse:0.120933 train-rmse:0.073320 ## [354] eval-rmse:0.120906 train-rmse:0.073206 ## [355] eval-rmse:0.120859 train-rmse:0.073055 ## [356] eval-rmse:0.120814 train-rmse:0.072954 ## [357] eval-rmse:0.120791 train-rmse:0.072881 ## [358] eval-rmse:0.120797 train-rmse:0.072791 ## [359] eval-rmse:0.120785 train-rmse:0.072672 ## [360] eval-rmse:0.120648 train-rmse:0.072599 ## [361] eval-rmse:0.120640 train-rmse:0.072511 ## [362] eval-rmse:0.120631 train-rmse:0.072433 ## [363] eval-rmse:0.120640 train-rmse:0.072374 ## [364] eval-rmse:0.120591 train-rmse:0.072320 ## [365] eval-rmse:0.120649 train-rmse:0.072254 ## [366] eval-rmse:0.120623 train-rmse:0.072196 ## [367] eval-rmse:0.120624 train-rmse:0.072106 ## [368] eval-rmse:0.120690 train-rmse:0.072027 ## [369] eval-rmse:0.120698 train-rmse:0.071934 ## [370] eval-rmse:0.120690 train-rmse:0.071884 ## [371] eval-rmse:0.120683 train-rmse:0.071799 ## [372] eval-rmse:0.120573 train-rmse:0.071709 ## [373] eval-rmse:0.120569 train-rmse:0.071582 ## [374] eval-rmse:0.120548 train-rmse:0.071523 ## [375] eval-rmse:0.120563 train-rmse:0.071455 ## [376] eval-rmse:0.120438 train-rmse:0.071371 ## [377] eval-rmse:0.120439 train-rmse:0.071304 ## [378] eval-rmse:0.120379 train-rmse:0.071237 ## [379] eval-rmse:0.120389 train-rmse:0.071135 ## [380] eval-rmse:0.120390 train-rmse:0.071085 ## [381] eval-rmse:0.120313 train-rmse:0.071015 ## [382] eval-rmse:0.120285 train-rmse:0.070960 ## [383] eval-rmse:0.120225 train-rmse:0.070905 ## [384] eval-rmse:0.120224 train-rmse:0.070794 ## [385] eval-rmse:0.120200 train-rmse:0.070695 ## [386] eval-rmse:0.120113 train-rmse:0.070638 ## [387] eval-rmse:0.120049 train-rmse:0.070614 ## [388] eval-rmse:0.120076 train-rmse:0.070502 ## [389] eval-rmse:0.120052 train-rmse:0.070460 ## [390] eval-rmse:0.120081 train-rmse:0.070360 ## [391] eval-rmse:0.120054 train-rmse:0.070329 ## [392] eval-rmse:0.119985 train-rmse:0.070212 ## [393] eval-rmse:0.120111 train-rmse:0.070136 ## [394] eval-rmse:0.120068 train-rmse:0.070077 ## [395] eval-rmse:0.120064 train-rmse:0.069998 ## [396] eval-rmse:0.120084 train-rmse:0.069939 ## [397] eval-rmse:0.120096 train-rmse:0.069883 ## [398] eval-rmse:0.120046 train-rmse:0.069825 ## [399] eval-rmse:0.120078 train-rmse:0.069789 ## [400] eval-rmse:0.120143 train-rmse:0.069707 ## [401] eval-rmse:0.120135 train-rmse:0.069651 ## [402] eval-rmse:0.120235 train-rmse:0.069563 ## [403] eval-rmse:0.120228 train-rmse:0.069473 ## [404] eval-rmse:0.120210 train-rmse:0.069368 ## [405] eval-rmse:0.120142 train-rmse:0.069347 ## [406] eval-rmse:0.120203 train-rmse:0.069248 ## [407] eval-rmse:0.120252 train-rmse:0.069166 ## [408] eval-rmse:0.120307 train-rmse:0.069091 ## [409] eval-rmse:0.120271 train-rmse:0.069025 ## [410] eval-rmse:0.120240 train-rmse:0.068916 ## [411] eval-rmse:0.120308 train-rmse:0.068826 ## [412] eval-rmse:0.120244 train-rmse:0.068742 ## [413] eval-rmse:0.120219 train-rmse:0.068670 ## [414] eval-rmse:0.120242 train-rmse:0.068632 ## [415] eval-rmse:0.120219 train-rmse:0.068606 ## [416] eval-rmse:0.120195 train-rmse:0.068559 ## [417] eval-rmse:0.120173 train-rmse:0.068503 ## [418] eval-rmse:0.120201 train-rmse:0.068420 ## [419] eval-rmse:0.120228 train-rmse:0.068372 ## [420] eval-rmse:0.120262 train-rmse:0.068329 ## [421] eval-rmse:0.120197 train-rmse:0.068264 ## [422] eval-rmse:0.120151 train-rmse:0.068171 ## [423] eval-rmse:0.120147 train-rmse:0.068085 ## [424] eval-rmse:0.120185 train-rmse:0.068000 ## [425] eval-rmse:0.120143 train-rmse:0.067926 ## [426] eval-rmse:0.120179 train-rmse:0.067883 ## [427] eval-rmse:0.120118 train-rmse:0.067845 ## [428] eval-rmse:0.120235 train-rmse:0.067781 ## [429] eval-rmse:0.120233 train-rmse:0.067710 ## [430] eval-rmse:0.120219 train-rmse:0.067667 ## [431] eval-rmse:0.120148 train-rmse:0.067587 ## [432] eval-rmse:0.120132 train-rmse:0.067521 ## [433] eval-rmse:0.120151 train-rmse:0.067448 ## [434] eval-rmse:0.120039 train-rmse:0.067387 ## [435] eval-rmse:0.120066 train-rmse:0.067346 ## [436] eval-rmse:0.120045 train-rmse:0.067312 ## [437] eval-rmse:0.120045 train-rmse:0.067240 ## [438] eval-rmse:0.120052 train-rmse:0.067180 ## [439] eval-rmse:0.119990 train-rmse:0.067064 ## [440] eval-rmse:0.119973 train-rmse:0.066980 ## [441] eval-rmse:0.119889 train-rmse:0.066878 ## [442] eval-rmse:0.119852 train-rmse:0.066841 ## [443] eval-rmse:0.119835 train-rmse:0.066788 ## [444] eval-rmse:0.119773 train-rmse:0.066747 ## [445] eval-rmse:0.119830 train-rmse:0.066701 ## [446] eval-rmse:0.119778 train-rmse:0.066647 ## [447] eval-rmse:0.119784 train-rmse:0.066599 ## [448] eval-rmse:0.119775 train-rmse:0.066509 ## [449] eval-rmse:0.119807 train-rmse:0.066440 ## [450] eval-rmse:0.119806 train-rmse:0.066383 ## [451] eval-rmse:0.119865 train-rmse:0.066342 ## [452] eval-rmse:0.119848 train-rmse:0.066277 ## [453] eval-rmse:0.119796 train-rmse:0.066188 ## [454] eval-rmse:0.119850 train-rmse:0.066084 ## [455] eval-rmse:0.119840 train-rmse:0.065997 ## [456] eval-rmse:0.119785 train-rmse:0.065925 ## [457] eval-rmse:0.119741 train-rmse:0.065843 ## [458] eval-rmse:0.119765 train-rmse:0.065792 ## [459] eval-rmse:0.119755 train-rmse:0.065730 ## [460] eval-rmse:0.119741 train-rmse:0.065662 ## [461] eval-rmse:0.119725 train-rmse:0.065609 ## [462] eval-rmse:0.119734 train-rmse:0.065561 ## [463] eval-rmse:0.119710 train-rmse:0.065514 ## [464] eval-rmse:0.119688 train-rmse:0.065436 ## [465] eval-rmse:0.119664 train-rmse:0.065383 ## [466] eval-rmse:0.119630 train-rmse:0.065288 ## [467] eval-rmse:0.119618 train-rmse:0.065198 ## [468] eval-rmse:0.119555 train-rmse:0.065158 ## [469] eval-rmse:0.119534 train-rmse:0.065097 ## [470] eval-rmse:0.119499 train-rmse:0.065047 ## [471] eval-rmse:0.119471 train-rmse:0.065005 ## [472] eval-rmse:0.119515 train-rmse:0.064898 ## [473] eval-rmse:0.119475 train-rmse:0.064843 ## [474] eval-rmse:0.119444 train-rmse:0.064804 ## [475] eval-rmse:0.119513 train-rmse:0.064734 ## [476] eval-rmse:0.119541 train-rmse:0.064697 ## [477] eval-rmse:0.119512 train-rmse:0.064626 ## [478] eval-rmse:0.119459 train-rmse:0.064566 ## [479] eval-rmse:0.119431 train-rmse:0.064519 ## [480] eval-rmse:0.119414 train-rmse:0.064446 ## [481] eval-rmse:0.119459 train-rmse:0.064396 ## [482] eval-rmse:0.119469 train-rmse:0.064348 ## [483] eval-rmse:0.119440 train-rmse:0.064313 ## [484] eval-rmse:0.119419 train-rmse:0.064240 ## [485] eval-rmse:0.119422 train-rmse:0.064177 ## [486] eval-rmse:0.119457 train-rmse:0.064101 ## [487] eval-rmse:0.119428 train-rmse:0.064038 ## [488] eval-rmse:0.119406 train-rmse:0.063976 ## [489] eval-rmse:0.119392 train-rmse:0.063904 ## [490] eval-rmse:0.119401 train-rmse:0.063834 ## [491] eval-rmse:0.119395 train-rmse:0.063805 ## [492] eval-rmse:0.119376 train-rmse:0.063730 ## [493] eval-rmse:0.119365 train-rmse:0.063688 ## [494] eval-rmse:0.119368 train-rmse:0.063600 ## [495] eval-rmse:0.119387 train-rmse:0.063532 ## [496] eval-rmse:0.119421 train-rmse:0.063484 ## [497] eval-rmse:0.119466 train-rmse:0.063433 ## [498] eval-rmse:0.119462 train-rmse:0.063391 ## [499] eval-rmse:0.119413 train-rmse:0.063363 ## [500] eval-rmse:0.119421 train-rmse:0.063324 ## [501] eval-rmse:0.119401 train-rmse:0.063295 ## [502] eval-rmse:0.119394 train-rmse:0.063227 ## [503] eval-rmse:0.119338 train-rmse:0.063156 ## [504] eval-rmse:0.119320 train-rmse:0.063093 ## [505] eval-rmse:0.119336 train-rmse:0.063027 ## [506] eval-rmse:0.119319 train-rmse:0.062961 ## [507] eval-rmse:0.119278 train-rmse:0.062897 ## [508] eval-rmse:0.119300 train-rmse:0.062840 ## [509] eval-rmse:0.119255 train-rmse:0.062808 ## [510] eval-rmse:0.119256 train-rmse:0.062759 ## [511] eval-rmse:0.119249 train-rmse:0.062702 ## [512] eval-rmse:0.119302 train-rmse:0.062653 ## [513] eval-rmse:0.119326 train-rmse:0.062540 ## [514] eval-rmse:0.119336 train-rmse:0.062477 ## [515] eval-rmse:0.119320 train-rmse:0.062452 ## [516] eval-rmse:0.119298 train-rmse:0.062357 ## [517] eval-rmse:0.119332 train-rmse:0.062284 ## [518] eval-rmse:0.119452 train-rmse:0.062226 ## [519] eval-rmse:0.119462 train-rmse:0.062201 ## [520] eval-rmse:0.119471 train-rmse:0.062128 ## [521] eval-rmse:0.119514 train-rmse:0.062075 ## [522] eval-rmse:0.119515 train-rmse:0.062056 ## [523] eval-rmse:0.119479 train-rmse:0.061963 ## [524] eval-rmse:0.119484 train-rmse:0.061865 ## [525] eval-rmse:0.119483 train-rmse:0.061789 ## [526] eval-rmse:0.119456 train-rmse:0.061727 ## [527] eval-rmse:0.119450 train-rmse:0.061676 ## [528] eval-rmse:0.119446 train-rmse:0.061625 ## [529] eval-rmse:0.119489 train-rmse:0.061572 ## [530] eval-rmse:0.119480 train-rmse:0.061549 ## [531] eval-rmse:0.119471 train-rmse:0.061490 ## [532] eval-rmse:0.119489 train-rmse:0.061405 ## [533] eval-rmse:0.119501 train-rmse:0.061346 ## [534] eval-rmse:0.119672 train-rmse:0.061270 ## [535] eval-rmse:0.119665 train-rmse:0.061186 ## [536] eval-rmse:0.119566 train-rmse:0.061126 ## [537] eval-rmse:0.119592 train-rmse:0.061090 ## [538] eval-rmse:0.119542 train-rmse:0.061020 ## [539] eval-rmse:0.119515 train-rmse:0.060941 ## [540] eval-rmse:0.119524 train-rmse:0.060909 ## [541] eval-rmse:0.119646 train-rmse:0.060873 ## [542] eval-rmse:0.119654 train-rmse:0.060842 ## [543] eval-rmse:0.119672 train-rmse:0.060740 ## [544] eval-rmse:0.119655 train-rmse:0.060684 ## [545] eval-rmse:0.119646 train-rmse:0.060624 ## [546] eval-rmse:0.119605 train-rmse:0.060566 ## [547] eval-rmse:0.119578 train-rmse:0.060524 ## [548] eval-rmse:0.119572 train-rmse:0.060451 ## [549] eval-rmse:0.119583 train-rmse:0.060401 ## [550] eval-rmse:0.119566 train-rmse:0.060363 ## [551] eval-rmse:0.119549 train-rmse:0.060324 ## [552] eval-rmse:0.119528 train-rmse:0.060270 ## [553] eval-rmse:0.119511 train-rmse:0.060187 ## [554] eval-rmse:0.119515 train-rmse:0.060134 ## [555] eval-rmse:0.119474 train-rmse:0.060095 ## [556] eval-rmse:0.119488 train-rmse:0.060059 ## [557] eval-rmse:0.119492 train-rmse:0.059988 ## [558] eval-rmse:0.119507 train-rmse:0.059932 ## [559] eval-rmse:0.119567 train-rmse:0.059876 ## [560] eval-rmse:0.119552 train-rmse:0.059840 ## [561] eval-rmse:0.119539 train-rmse:0.059761 ## [562] eval-rmse:0.119513 train-rmse:0.059726 ## [563] eval-rmse:0.119553 train-rmse:0.059688 ## [564] eval-rmse:0.119597 train-rmse:0.059655 ## [565] eval-rmse:0.119595 train-rmse:0.059636 ## [566] eval-rmse:0.119617 train-rmse:0.059543 ## [567] eval-rmse:0.119739 train-rmse:0.059491 ## [568] eval-rmse:0.119736 train-rmse:0.059441 ## [569] eval-rmse:0.119731 train-rmse:0.059355 ## [570] eval-rmse:0.119677 train-rmse:0.059330 ## [571] eval-rmse:0.119743 train-rmse:0.059270 ## [572] eval-rmse:0.119802 train-rmse:0.059205 ## [573] eval-rmse:0.119794 train-rmse:0.059164 ## [574] eval-rmse:0.119830 train-rmse:0.059114 ## [575] eval-rmse:0.119805 train-rmse:0.059064 ## [576] eval-rmse:0.119801 train-rmse:0.059027 ## [577] eval-rmse:0.119758 train-rmse:0.058982 ## [578] eval-rmse:0.119747 train-rmse:0.058942 ## [579] eval-rmse:0.119723 train-rmse:0.058914 ## [580] eval-rmse:0.119751 train-rmse:0.058863 ## [581] eval-rmse:0.119803 train-rmse:0.058799 ## [582] eval-rmse:0.119811 train-rmse:0.058745 ## [583] eval-rmse:0.119840 train-rmse:0.058674 ## [584] eval-rmse:0.119826 train-rmse:0.058634 ## [585] eval-rmse:0.119812 train-rmse:0.058593 ## [586] eval-rmse:0.119794 train-rmse:0.058535 ## [587] eval-rmse:0.119780 train-rmse:0.058492 ## [588] eval-rmse:0.119820 train-rmse:0.058462 ## [589] eval-rmse:0.119833 train-rmse:0.058420 ## [590] eval-rmse:0.119801 train-rmse:0.058371 ## [591] eval-rmse:0.119802 train-rmse:0.058341 ## [592] eval-rmse:0.119783 train-rmse:0.058300 ## [593] eval-rmse:0.119765 train-rmse:0.058263 ## [594] eval-rmse:0.119728 train-rmse:0.058247 ## [595] eval-rmse:0.119748 train-rmse:0.058185 ## [596] eval-rmse:0.119764 train-rmse:0.058165 ## [597] eval-rmse:0.119786 train-rmse:0.058123 ## [598] eval-rmse:0.119829 train-rmse:0.058079 ## [599] eval-rmse:0.119812 train-rmse:0.058013 ## [600] eval-rmse:0.119787 train-rmse:0.057969 ## [601] eval-rmse:0.119771 train-rmse:0.057929 ## [602] eval-rmse:0.119761 train-rmse:0.057859 ## [603] eval-rmse:0.119776 train-rmse:0.057803 ## [604] eval-rmse:0.119719 train-rmse:0.057770 ## [605] eval-rmse:0.119669 train-rmse:0.057736 ## [606] eval-rmse:0.119673 train-rmse:0.057671 ## [607] eval-rmse:0.119709 train-rmse:0.057600 ## [608] eval-rmse:0.119706 train-rmse:0.057541 ## [609] eval-rmse:0.119661 train-rmse:0.057512 ## [610] eval-rmse:0.119355 train-rmse:0.057484 ## [611] eval-rmse:0.119310 train-rmse:0.057422 ## [612] eval-rmse:0.119310 train-rmse:0.057360 ## [613] eval-rmse:0.119328 train-rmse:0.057331 ## [614] eval-rmse:0.119296 train-rmse:0.057272 ## [615] eval-rmse:0.119282 train-rmse:0.057250 ## [616] eval-rmse:0.119289 train-rmse:0.057170 ## [617] eval-rmse:0.119278 train-rmse:0.057117 ## [618] eval-rmse:0.119297 train-rmse:0.057054 ## [619] eval-rmse:0.119268 train-rmse:0.057005 ## [620] eval-rmse:0.119277 train-rmse:0.056955 ## [621] eval-rmse:0.119313 train-rmse:0.056888 ## [622] eval-rmse:0.119351 train-rmse:0.056846 ## [623] eval-rmse:0.119392 train-rmse:0.056811 ## [624] eval-rmse:0.119368 train-rmse:0.056731 ## [625] eval-rmse:0.119282 train-rmse:0.056671 ## [626] eval-rmse:0.119265 train-rmse:0.056625 ## [627] eval-rmse:0.119246 train-rmse:0.056559 ## [628] eval-rmse:0.119057 train-rmse:0.056540 ## [629] eval-rmse:0.119084 train-rmse:0.056507 ## [630] eval-rmse:0.119088 train-rmse:0.056461 ## [631] eval-rmse:0.119067 train-rmse:0.056389 ## [632] eval-rmse:0.119101 train-rmse:0.056362 ## [633] eval-rmse:0.119105 train-rmse:0.056298 ## [634] eval-rmse:0.119114 train-rmse:0.056242 ## [635] eval-rmse:0.119117 train-rmse:0.056226 ## [636] eval-rmse:0.119071 train-rmse:0.056196 ## [637] eval-rmse:0.119064 train-rmse:0.056177 ## [638] eval-rmse:0.119013 train-rmse:0.056135 ## [639] eval-rmse:0.118999 train-rmse:0.056096 ## [640] eval-rmse:0.119005 train-rmse:0.056061 ## [641] eval-rmse:0.118980 train-rmse:0.055993 ## [642] eval-rmse:0.118993 train-rmse:0.055958 ## [643] eval-rmse:0.119034 train-rmse:0.055909 ## [644] eval-rmse:0.119050 train-rmse:0.055856 ## [645] eval-rmse:0.118991 train-rmse:0.055804 ## [646] eval-rmse:0.118942 train-rmse:0.055738 ## [647] eval-rmse:0.118926 train-rmse:0.055677 ## [648] eval-rmse:0.118904 train-rmse:0.055655 ## [649] eval-rmse:0.118938 train-rmse:0.055647 ## [650] eval-rmse:0.118903 train-rmse:0.055596 ## [651] eval-rmse:0.118931 train-rmse:0.055538 ## [652] eval-rmse:0.118942 train-rmse:0.055477 ## [653] eval-rmse:0.119069 train-rmse:0.055414 ## [654] eval-rmse:0.119057 train-rmse:0.055361 ## [655] eval-rmse:0.119032 train-rmse:0.055350 ## [656] eval-rmse:0.118992 train-rmse:0.055290 ## [657] eval-rmse:0.118975 train-rmse:0.055249 ## [658] eval-rmse:0.119033 train-rmse:0.055174 ## [659] eval-rmse:0.118997 train-rmse:0.055151 ## [660] eval-rmse:0.119130 train-rmse:0.055118 ## [661] eval-rmse:0.119093 train-rmse:0.055067 ## [662] eval-rmse:0.119101 train-rmse:0.055009 ## [663] eval-rmse:0.119103 train-rmse:0.054939 ## [664] eval-rmse:0.119124 train-rmse:0.054913 ## [665] eval-rmse:0.119108 train-rmse:0.054898 ## [666] eval-rmse:0.119149 train-rmse:0.054847 ## [667] eval-rmse:0.119149 train-rmse:0.054795 ## [668] eval-rmse:0.119143 train-rmse:0.054769 ## [669] eval-rmse:0.119144 train-rmse:0.054717 ## [670] eval-rmse:0.119160 train-rmse:0.054666 ## [671] eval-rmse:0.119146 train-rmse:0.054633 ## [672] eval-rmse:0.119176 train-rmse:0.054580 ## [673] eval-rmse:0.119173 train-rmse:0.054525 ## [674] eval-rmse:0.119153 train-rmse:0.054490 ## [675] eval-rmse:0.119166 train-rmse:0.054460 ## [676] eval-rmse:0.119118 train-rmse:0.054425 ## [677] eval-rmse:0.119112 train-rmse:0.054401 ## [678] eval-rmse:0.118957 train-rmse:0.054388 ## [679] eval-rmse:0.118943 train-rmse:0.054340 ## [680] eval-rmse:0.118944 train-rmse:0.054324 ## [681] eval-rmse:0.118898 train-rmse:0.054271 ## [682] eval-rmse:0.118893 train-rmse:0.054224 ## [683] eval-rmse:0.118895 train-rmse:0.054164 ## [684] eval-rmse:0.118880 train-rmse:0.054135 ## [685] eval-rmse:0.118854 train-rmse:0.054082 ## [686] eval-rmse:0.118849 train-rmse:0.054066 ## [687] eval-rmse:0.118842 train-rmse:0.054029 ## [688] eval-rmse:0.118880 train-rmse:0.053957 ## [689] eval-rmse:0.118901 train-rmse:0.053879 ## [690] eval-rmse:0.118870 train-rmse:0.053837 ## [691] eval-rmse:0.118933 train-rmse:0.053799 ## [692] eval-rmse:0.118912 train-rmse:0.053779 ## [693] eval-rmse:0.118853 train-rmse:0.053728 ## [694] eval-rmse:0.118866 train-rmse:0.053708 ## [695] eval-rmse:0.118842 train-rmse:0.053641 ## [696] eval-rmse:0.118831 train-rmse:0.053607 ## [697] eval-rmse:0.118822 train-rmse:0.053564 ## [698] eval-rmse:0.118824 train-rmse:0.053536 ## [699] eval-rmse:0.118801 train-rmse:0.053472 ## [700] eval-rmse:0.118789 train-rmse:0.053404 ## [701] eval-rmse:0.118837 train-rmse:0.053357 ## [702] eval-rmse:0.118842 train-rmse:0.053320 ## [703] eval-rmse:0.118837 train-rmse:0.053278 ## [704] eval-rmse:0.118837 train-rmse:0.053225 ## [705] eval-rmse:0.118829 train-rmse:0.053199 ## [706] eval-rmse:0.118871 train-rmse:0.053156 ## [707] eval-rmse:0.118848 train-rmse:0.053105 ## [708] eval-rmse:0.118849 train-rmse:0.053068 ## [709] eval-rmse:0.118865 train-rmse:0.053049 ## [710] eval-rmse:0.118862 train-rmse:0.053022 ## [711] eval-rmse:0.118842 train-rmse:0.052971 ## [712] eval-rmse:0.118884 train-rmse:0.052939 ## [713] eval-rmse:0.118874 train-rmse:0.052873 ## [714] eval-rmse:0.118841 train-rmse:0.052848 ## [715] eval-rmse:0.118822 train-rmse:0.052810 ## [716] eval-rmse:0.118788 train-rmse:0.052746 ## [717] eval-rmse:0.118768 train-rmse:0.052699 ## [718] eval-rmse:0.118766 train-rmse:0.052686 ## [719] eval-rmse:0.118749 train-rmse:0.052635 ## [720] eval-rmse:0.118758 train-rmse:0.052589 ## [721] eval-rmse:0.118706 train-rmse:0.052545 ## [722] eval-rmse:0.118704 train-rmse:0.052503 ## [723] eval-rmse:0.118732 train-rmse:0.052468 ## [724] eval-rmse:0.118699 train-rmse:0.052430 ## [725] eval-rmse:0.118737 train-rmse:0.052372 ## [726] eval-rmse:0.118760 train-rmse:0.052344 ## [727] eval-rmse:0.118776 train-rmse:0.052299 ## [728] eval-rmse:0.118759 train-rmse:0.052263 ## [729] eval-rmse:0.118712 train-rmse:0.052226 ## [730] eval-rmse:0.118722 train-rmse:0.052196 ## [731] eval-rmse:0.118688 train-rmse:0.052153 ## [732] eval-rmse:0.118656 train-rmse:0.052109 ## [733] eval-rmse:0.118656 train-rmse:0.052076 ## [734] eval-rmse:0.118694 train-rmse:0.052030 ## [735] eval-rmse:0.118694 train-rmse:0.051994 ## [736] eval-rmse:0.118674 train-rmse:0.051969 ## [737] eval-rmse:0.118667 train-rmse:0.051921 ## [738] eval-rmse:0.118630 train-rmse:0.051878 ## [739] eval-rmse:0.118614 train-rmse:0.051831 ## [740] eval-rmse:0.118573 train-rmse:0.051813 ## [741] eval-rmse:0.118567 train-rmse:0.051759 ## [742] eval-rmse:0.118557 train-rmse:0.051707 ## [743] eval-rmse:0.118557 train-rmse:0.051655 ## [744] eval-rmse:0.118546 train-rmse:0.051598 ## [745] eval-rmse:0.118579 train-rmse:0.051581 ## [746] eval-rmse:0.118567 train-rmse:0.051540 ## [747] eval-rmse:0.118580 train-rmse:0.051485 ## [748] eval-rmse:0.118611 train-rmse:0.051448 ## [749] eval-rmse:0.118621 train-rmse:0.051402 ## [750] eval-rmse:0.118590 train-rmse:0.051359 ## [751] eval-rmse:0.118577 train-rmse:0.051342 ## [752] eval-rmse:0.118599 train-rmse:0.051315 ## [753] eval-rmse:0.118619 train-rmse:0.051274 ## [754] eval-rmse:0.118619 train-rmse:0.051244 ## [755] eval-rmse:0.118655 train-rmse:0.051213 ## [756] eval-rmse:0.118654 train-rmse:0.051161 ## [757] eval-rmse:0.118650 train-rmse:0.051123 ## [758] eval-rmse:0.118666 train-rmse:0.051092 ## [759] eval-rmse:0.118639 train-rmse:0.051049 ## [760] eval-rmse:0.118634 train-rmse:0.050959 ## [761] eval-rmse:0.118630 train-rmse:0.050884 ## [762] eval-rmse:0.118616 train-rmse:0.050846 ## [763] eval-rmse:0.118593 train-rmse:0.050793 ## [764] eval-rmse:0.118607 train-rmse:0.050774 ## [765] eval-rmse:0.118557 train-rmse:0.050740 ## [766] eval-rmse:0.118564 train-rmse:0.050718 ## [767] eval-rmse:0.118518 train-rmse:0.050676 ## [768] eval-rmse:0.118493 train-rmse:0.050658 ## [769] eval-rmse:0.118477 train-rmse:0.050622 ## [770] eval-rmse:0.118463 train-rmse:0.050575 ## [771] eval-rmse:0.118467 train-rmse:0.050546 ## [772] eval-rmse:0.118430 train-rmse:0.050517 ## [773] eval-rmse:0.118437 train-rmse:0.050503 ## [774] eval-rmse:0.118443 train-rmse:0.050440 ## [775] eval-rmse:0.118433 train-rmse:0.050400 ## [776] eval-rmse:0.118534 train-rmse:0.050355 ## [777] eval-rmse:0.118537 train-rmse:0.050329 ## [778] eval-rmse:0.118553 train-rmse:0.050299 ## [779] eval-rmse:0.118546 train-rmse:0.050254 ## [780] eval-rmse:0.118515 train-rmse:0.050202 ## [781] eval-rmse:0.118506 train-rmse:0.050175 ## [782] eval-rmse:0.118494 train-rmse:0.050154 ## [783] eval-rmse:0.118512 train-rmse:0.050108 ## [784] eval-rmse:0.118493 train-rmse:0.050086 ## [785] eval-rmse:0.118517 train-rmse:0.050018 ## [786] eval-rmse:0.118546 train-rmse:0.049973 ## [787] eval-rmse:0.118535 train-rmse:0.049937 ## [788] eval-rmse:0.118538 train-rmse:0.049874 ## [789] eval-rmse:0.118546 train-rmse:0.049860 ## [790] eval-rmse:0.118559 train-rmse:0.049811 ## [791] eval-rmse:0.118524 train-rmse:0.049758 ## [792] eval-rmse:0.118483 train-rmse:0.049717 ## [793] eval-rmse:0.118484 train-rmse:0.049690 ## [794] eval-rmse:0.118465 train-rmse:0.049649 ## [795] eval-rmse:0.118452 train-rmse:0.049618 ## [796] eval-rmse:0.118491 train-rmse:0.049576 ## [797] eval-rmse:0.118500 train-rmse:0.049560 ## [798] eval-rmse:0.118496 train-rmse:0.049514 ## [799] eval-rmse:0.118513 train-rmse:0.049470 ## [800] eval-rmse:0.118488 train-rmse:0.049427 ## [801] eval-rmse:0.118451 train-rmse:0.049381 ## [802] eval-rmse:0.118451 train-rmse:0.049354 ## [803] eval-rmse:0.118431 train-rmse:0.049336 ## [804] eval-rmse:0.118458 train-rmse:0.049301 ## [805] eval-rmse:0.118447 train-rmse:0.049290 ## [806] eval-rmse:0.118520 train-rmse:0.049221 ## [807] eval-rmse:0.118480 train-rmse:0.049169 ## [808] eval-rmse:0.118481 train-rmse:0.049104 ## [809] eval-rmse:0.118487 train-rmse:0.049039 ## [810] eval-rmse:0.118497 train-rmse:0.048990 ## [811] eval-rmse:0.118484 train-rmse:0.048954 ## [812] eval-rmse:0.118541 train-rmse:0.048918 ## [813] eval-rmse:0.118548 train-rmse:0.048881 ## [814] eval-rmse:0.118530 train-rmse:0.048813 ## [815] eval-rmse:0.118524 train-rmse:0.048770 ## [816] eval-rmse:0.118495 train-rmse:0.048756 ## [817] eval-rmse:0.118514 train-rmse:0.048744 ## [818] eval-rmse:0.118493 train-rmse:0.048693 ## [819] eval-rmse:0.118478 train-rmse:0.048660 ## [820] eval-rmse:0.118465 train-rmse:0.048645 ## [821] eval-rmse:0.118473 train-rmse:0.048595 ## [822] eval-rmse:0.118434 train-rmse:0.048547 ## [823] eval-rmse:0.118479 train-rmse:0.048525 ## [824] eval-rmse:0.118483 train-rmse:0.048515 ## [825] eval-rmse:0.118483 train-rmse:0.048457 ## [826] eval-rmse:0.118520 train-rmse:0.048442 ## [827] eval-rmse:0.118551 train-rmse:0.048397 ## [828] eval-rmse:0.118569 train-rmse:0.048356 ## [829] eval-rmse:0.118579 train-rmse:0.048336 ## [830] eval-rmse:0.118577 train-rmse:0.048309 ## [831] eval-rmse:0.118587 train-rmse:0.048269 ## [832] eval-rmse:0.118541 train-rmse:0.048253 ## [833] eval-rmse:0.118564 train-rmse:0.048204 ## [834] eval-rmse:0.118542 train-rmse:0.048180 ## [835] eval-rmse:0.118525 train-rmse:0.048117 ## [836] eval-rmse:0.118516 train-rmse:0.048083 ## [837] eval-rmse:0.118529 train-rmse:0.048042 ## [838] eval-rmse:0.118496 train-rmse:0.048007 ## [839] eval-rmse:0.118461 train-rmse:0.047939 ## [840] eval-rmse:0.118456 train-rmse:0.047898 ## [841] eval-rmse:0.118439 train-rmse:0.047849 ## [842] eval-rmse:0.118397 train-rmse:0.047819 ## [843] eval-rmse:0.118387 train-rmse:0.047784 ## [844] eval-rmse:0.118402 train-rmse:0.047769 ## [845] eval-rmse:0.118403 train-rmse:0.047750 ## [846] eval-rmse:0.118388 train-rmse:0.047709 ## [847] eval-rmse:0.118407 train-rmse:0.047673 ## [848] eval-rmse:0.118408 train-rmse:0.047638 ## [849] eval-rmse:0.118414 train-rmse:0.047606 ## [850] eval-rmse:0.118409 train-rmse:0.047595 ## [851] eval-rmse:0.118418 train-rmse:0.047538 ## [852] eval-rmse:0.118411 train-rmse:0.047504 ## [853] eval-rmse:0.118416 train-rmse:0.047479 ## [854] eval-rmse:0.118413 train-rmse:0.047431 ## [855] eval-rmse:0.118433 train-rmse:0.047399 ## [856] eval-rmse:0.118424 train-rmse:0.047361 ## [857] eval-rmse:0.118475 train-rmse:0.047316 ## [858] eval-rmse:0.118505 train-rmse:0.047286 ## [859] eval-rmse:0.118496 train-rmse:0.047262 ## [860] eval-rmse:0.118527 train-rmse:0.047212 ## [861] eval-rmse:0.118589 train-rmse:0.047178 ## [862] eval-rmse:0.118569 train-rmse:0.047126 ## [863] eval-rmse:0.118559 train-rmse:0.047086 ## [864] eval-rmse:0.118561 train-rmse:0.047047 ## [865] eval-rmse:0.118653 train-rmse:0.047021 ## [866] eval-rmse:0.118644 train-rmse:0.046993 ## [867] eval-rmse:0.118649 train-rmse:0.046942 ## [868] eval-rmse:0.118636 train-rmse:0.046893 ## [869] eval-rmse:0.118641 train-rmse:0.046841 ## [870] eval-rmse:0.118608 train-rmse:0.046784 ## [871] eval-rmse:0.118604 train-rmse:0.046766 ## [872] eval-rmse:0.118590 train-rmse:0.046711 ## [873] eval-rmse:0.118589 train-rmse:0.046671 ## [874] eval-rmse:0.118579 train-rmse:0.046638 ## [875] eval-rmse:0.118566 train-rmse:0.046631 ## [876] eval-rmse:0.118581 train-rmse:0.046588 ## [877] eval-rmse:0.118584 train-rmse:0.046542 ## [878] eval-rmse:0.118581 train-rmse:0.046502 ## [879] eval-rmse:0.118615 train-rmse:0.046456 ## [880] eval-rmse:0.118635 train-rmse:0.046426 ## [881] eval-rmse:0.118648 train-rmse:0.046383 ## [882] eval-rmse:0.118648 train-rmse:0.046337 ## [883] eval-rmse:0.118643 train-rmse:0.046310 ## [884] eval-rmse:0.118637 train-rmse:0.046303 ## [885] eval-rmse:0.118612 train-rmse:0.046272 ## [886] eval-rmse:0.118566 train-rmse:0.046221 ## [887] eval-rmse:0.118562 train-rmse:0.046175 ## [888] eval-rmse:0.118562 train-rmse:0.046137 ## [889] eval-rmse:0.118581 train-rmse:0.046125 ## [890] eval-rmse:0.118544 train-rmse:0.046076 ## [891] eval-rmse:0.118568 train-rmse:0.046018 ## [892] eval-rmse:0.118578 train-rmse:0.045990 ## [893] eval-rmse:0.118599 train-rmse:0.045942 ## [894] eval-rmse:0.118601 train-rmse:0.045917 ## [895] eval-rmse:0.118593 train-rmse:0.045877 ## [896] eval-rmse:0.118595 train-rmse:0.045818 ## [897] eval-rmse:0.118583 train-rmse:0.045798 ## [898] eval-rmse:0.118575 train-rmse:0.045758 ## [899] eval-rmse:0.118580 train-rmse:0.045715 ## [900] eval-rmse:0.118593 train-rmse:0.045685 ## [901] eval-rmse:0.118577 train-rmse:0.045658 ## [902] eval-rmse:0.118582 train-rmse:0.045605 ## [903] eval-rmse:0.118578 train-rmse:0.045557 ## [904] eval-rmse:0.118547 train-rmse:0.045524 ## [905] eval-rmse:0.118528 train-rmse:0.045507 ## [906] eval-rmse:0.118551 train-rmse:0.045488 ## [907] eval-rmse:0.118647 train-rmse:0.045463 ## [908] eval-rmse:0.118670 train-rmse:0.045405 ## [909] eval-rmse:0.118698 train-rmse:0.045347 ## [910] eval-rmse:0.118702 train-rmse:0.045322 ## [911] eval-rmse:0.118706 train-rmse:0.045281 ## [912] eval-rmse:0.118657 train-rmse:0.045255 ## [913] eval-rmse:0.118675 train-rmse:0.045232 ## [914] eval-rmse:0.118665 train-rmse:0.045206 ## [915] eval-rmse:0.118668 train-rmse:0.045162 ## [916] eval-rmse:0.118646 train-rmse:0.045132 ## [917] eval-rmse:0.118657 train-rmse:0.045105 ## [918] eval-rmse:0.118714 train-rmse:0.045090 ## [919] eval-rmse:0.118683 train-rmse:0.045051 ## [920] eval-rmse:0.118699 train-rmse:0.044985 ## [921] eval-rmse:0.118685 train-rmse:0.044948 ## [922] eval-rmse:0.118689 train-rmse:0.044897 ## [923] eval-rmse:0.118665 train-rmse:0.044860 ## [924] eval-rmse:0.118657 train-rmse:0.044835 ## [925] eval-rmse:0.118651 train-rmse:0.044792 ## [926] eval-rmse:0.118631 train-rmse:0.044741 ## [927] eval-rmse:0.118626 train-rmse:0.044696 ## [928] eval-rmse:0.118609 train-rmse:0.044677 ## [929] eval-rmse:0.118615 train-rmse:0.044658 ## [930] eval-rmse:0.118615 train-rmse:0.044611 ## [931] eval-rmse:0.118653 train-rmse:0.044581 ## [932] eval-rmse:0.118697 train-rmse:0.044541 ## [933] eval-rmse:0.118683 train-rmse:0.044519 ## [934] eval-rmse:0.118707 train-rmse:0.044480 ## [935] eval-rmse:0.118698 train-rmse:0.044436 ## [936] eval-rmse:0.118712 train-rmse:0.044393 ## [937] eval-rmse:0.118739 train-rmse:0.044341 ## [938] eval-rmse:0.118756 train-rmse:0.044301 ## [939] eval-rmse:0.118759 train-rmse:0.044279 ## [940] eval-rmse:0.118766 train-rmse:0.044230 ## [941] eval-rmse:0.118745 train-rmse:0.044206 ## [942] eval-rmse:0.118720 train-rmse:0.044169 ## [943] eval-rmse:0.118710 train-rmse:0.044127 ## [944] eval-rmse:0.118720 train-rmse:0.044123 ## [945] eval-rmse:0.118731 train-rmse:0.044106 ## [946] eval-rmse:0.118717 train-rmse:0.044073 ## [947] eval-rmse:0.118698 train-rmse:0.044011 ## [948] eval-rmse:0.118705 train-rmse:0.043969 ## [949] eval-rmse:0.118702 train-rmse:0.043914 ## [950] eval-rmse:0.118721 train-rmse:0.043892 ## [951] eval-rmse:0.118730 train-rmse:0.043861 ## [952] eval-rmse:0.118725 train-rmse:0.043838 ## [953] eval-rmse:0.118726 train-rmse:0.043829 ## [954] eval-rmse:0.118730 train-rmse:0.043805 ## [955] eval-rmse:0.118726 train-rmse:0.043758 ## [956] eval-rmse:0.118750 train-rmse:0.043717 ## [957] eval-rmse:0.118738 train-rmse:0.043693 ## [958] eval-rmse:0.118722 train-rmse:0.043660 ## [959] eval-rmse:0.118712 train-rmse:0.043634 ## [960] eval-rmse:0.118722 train-rmse:0.043616 ## [961] eval-rmse:0.118718 train-rmse:0.043582 ## [962] eval-rmse:0.118706 train-rmse:0.043562 ## [963] eval-rmse:0.118715 train-rmse:0.043552 ## [964] eval-rmse:0.118699 train-rmse:0.043512 ## [965] eval-rmse:0.118676 train-rmse:0.043493 ## [966] eval-rmse:0.118722 train-rmse:0.043471 ## [967] eval-rmse:0.118739 train-rmse:0.043439 ## [968] eval-rmse:0.118727 train-rmse:0.043427 ## [969] eval-rmse:0.118724 train-rmse:0.043397 ## [970] eval-rmse:0.118727 train-rmse:0.043360 ## [971] eval-rmse:0.118731 train-rmse:0.043333 ## [972] eval-rmse:0.118739 train-rmse:0.043304 ## [973] eval-rmse:0.118723 train-rmse:0.043294 ## [974] eval-rmse:0.118726 train-rmse:0.043260 ## [975] eval-rmse:0.118718 train-rmse:0.043212 ## [976] eval-rmse:0.118698 train-rmse:0.043185 ## [977] eval-rmse:0.118672 train-rmse:0.043151 ## [978] eval-rmse:0.118666 train-rmse:0.043127 ## [979] eval-rmse:0.118685 train-rmse:0.043107 ## [980] eval-rmse:0.118647 train-rmse:0.043073 ## [981] eval-rmse:0.118622 train-rmse:0.043029 ## [982] eval-rmse:0.118590 train-rmse:0.042989 ## [983] eval-rmse:0.118616 train-rmse:0.042973 ## [984] eval-rmse:0.118608 train-rmse:0.042952 ## [985] eval-rmse:0.118609 train-rmse:0.042934 ## [986] eval-rmse:0.118611 train-rmse:0.042919 ## [987] eval-rmse:0.118564 train-rmse:0.042890 ## [988] eval-rmse:0.118594 train-rmse:0.042867 ## [989] eval-rmse:0.118597 train-rmse:0.042856 ## [990] eval-rmse:0.118595 train-rmse:0.042835 ## [991] eval-rmse:0.118606 train-rmse:0.042778 ## [992] eval-rmse:0.118625 train-rmse:0.042735 ## [993] eval-rmse:0.118644 train-rmse:0.042725 ## [994] eval-rmse:0.118662 train-rmse:0.042676 ## [995] eval-rmse:0.118637 train-rmse:0.042642 ## [996] eval-rmse:0.118684 train-rmse:0.042610 ## [997] eval-rmse:0.118679 train-rmse:0.042580 ## [998] eval-rmse:0.118695 train-rmse:0.042545 ## [999] eval-rmse:0.118685 train-rmse:0.042508 ## [1000] eval-rmse:0.118696 train-rmse:0.042480 eval &lt;- bst$evaluation_log %&gt;% gather(tipo, rmse, -iter) ggplot(eval, aes(x=iter, y=rmse, colour=tipo, group= tipo)) + geom_line() + scale_y_log10() Tarea Revisa el script que vimos en clase de aplicación de bosques para predecir precios de casa (bosque-housing.Rmd). Argumenta por qué es mejor el segundo método para limpiar faltantes que el primero. Considera Cómo respeta cada método la división entrenamiento y validación El desempeño de cada método Considera las importancia de variables de bosque-housing.Rmd. Muestra las importancias basadas en permutaciones escaladas y no escaladas. ¿Con qué valores en el objeto randomForest se escalan las importancias? Grafica importancias de Gini (MeanDecreaseGini) y de permutaciones. ¿Los resultados son similiares? Explica qué significa MeanDecreaseGini en el contexto de un problema de regresión. Considera nuestra primera corrida de gradient boosting en las notas para el ejemplo de los precios de las casas. Corre este ejemplo usando pérdida absoluta (\\(|y-f(x)|\\)) en lugar de pérdida cuadrática (\\((y-f(x))^2\\)) Grafica las curvas de entrenamiento y validación conforme se agregan árboles Explica teóricamente cuál es la diferencia del algoritmo cuando utilizas estas dos pérdidas. Da razones por las que pérdida absoluta puede ser una mejor selección para algunos problemas de regresión. References "],
["validacion-de-modelos-problemas-comunes.html", "Clase 13 Validación de modelos: problemas comunes 13.1 Filtración de datos 13.2 Series de tiempo 13.3 Filtración en el preprocesamiento 13.4 Uso de variables fuera de rango temporal 13.5 Datos en conglomerados y muestreo complejo 13.6 Muestras de validación chicas 13.7 Otros ejemplos 13.8 Resumen Tarea", " Clase 13 Validación de modelos: problemas comunes En aprendizaje de máquina, el ajuste y afinación de parámetros es tan importante como la evaluación de desempeño o validación de los modelos resultantes. Ninguna funciona bien sin que la otra sea correctamente ejecutada. Hemos visto que ambas partes tienen dificultades algunas veces sutiles (tanto el ajuste y optimización como la evaluación de las predicciones) que pueden hacer fracasar nuestro ejercicio de modelación. En esta parte hablaremos de la evaluación de modelos. En aprendizaje máquina, considerando que utilizamos relativamente pocos supuestos teóricos, dependemos de esa evaluación para asegurarnos que estamos capturando patrones reales y útiles en los datos. Todo lo que veremos aplica tanto a separación de muestras de validación como a uso de algún tipo de validación (validación cruzada, estimación OOB en árboles, validación bootstrap, etc.) 13.1 Filtración de datos La filtración de datos ocurre cuando nuestro proceso de validación está contaminado por información que en la tarea real de predicción no tendremos disponible. En consecuencia, nuestras estimaciones de desempeño del modelo (validación) son optimistas en relación al desempeño verdadero. También podemos pensar en filtraciones tanto al conjunto de entrenamiento y validación, cuando ambos están contaminados con información que no estará disponible al momento de hacer las predicciones. Esto produce modelos que no es posible poner en producción. El primer tipo de filtraciones es más difícil de detectar antes de la puesta en producción de los modelos. El segundo tipo puede descubrirse cuando nos damos cuenta de que no es posible implementar en producción nuestro modelo porque no hay información disponible que usamos para construirlo (o peor, cuando cometemos un error en la implementación y el modelo se desempeña mal posterioremente). Veamos el primer caso: filtración de conjuntos de validación al conjunto de entrenamiento. La filtración de datos puede ocurrir de muchas maneras, muchas veces inesperadas. Quizá uno de los ejemplos más típicos es el validación de modelos de series de tiempo. 13.2 Series de tiempo Comenzamos con un ejemplo simulado. Haremos varias simulaciones para incorporar la variación producida en los modelos por la muestra de entrenamineto library(methods) library(randomForest) library(tidyverse) library(glmnet) simular_datos &lt;- function(n = 500,...){ datos &lt;- data_frame(t=1:n, x = rnorm(n,0,1)) y &lt;- numeric(n) #nivel &lt;- numeric(n) #nivel[1] &lt;- 10 y[1] &lt;- datos$x[1] #+ nivel[1] for(i in 2:n){ #nivel[i] &lt;- nivel[i-1] + rnorm(1, 0, 0.1) #y[i] &lt;- 0.01*i + datos$x[i] + nivel[i] + rnorm(1,0,0.05) y[i] &lt;- 0.01*i + datos$x[i] + 0.9*y[i-1] + rnorm(1,0,0.05) } datos$y &lt;- y datos } separar &lt;- function(df, prop){ df &lt;- df %&gt;% rowwise %&gt;% mutate(tipo = ifelse(t &gt; floor(nrow(df)*(prop[1]+prop[2])), &#39;prueba&#39;, sample(c(&#39;entrena&#39;,&#39;valida&#39;),1))) split(df, df$tipo) } ajustar_evaluar &lt;- function(df_split){ mod_1 &lt;- randomForest(y ~ x + t, data = df_split[[&#39;entrena&#39;]]) error_valida &lt;- sd(predict(mod_1, df_split[[&#39;valida&#39;]])-df_split[[&#39;valida&#39;]]$y) error_prueba &lt;- sd(predict(mod_1, df_split[[&#39;prueba&#39;]])-df_split[[&#39;prueba&#39;]]$y) c(error_valida = error_valida, error_prueba = error_prueba) } Por ejemplo: ggplot(simular_datos(), aes(x=t, y=y)) + geom_line() Separamos ingenuamente entrenamiento y prueba y ajustamos un modelo de regresión: errores &lt;- simular_datos(500) %&gt;% separar(prop= c(0.4,0.4,0.2)) %&gt;% ajustar_evaluar errores ## error_valida error_prueba ## 1.691497 3.914501 reps_1 &lt;- map(1:50, simular_datos, n = 500) %&gt;% map(separar, prop= c(0.6,0.2,0.2)) %&gt;% map(ajustar_evaluar) %&gt;% transpose %&gt;% map(unlist) %&gt;% as_data_frame gr_reps_1 &lt;- reps_1 %&gt;% mutate(rep = row_number()) %&gt;% gather(tipo, valor, -rep) ggplot(reps_1, aes(x=error_valida, y=error_prueba)) + geom_point() + geom_abline() + xlim(c(0,10)) + ylim(c(0,10)) Y vemos que los errores de validación son consistentemente menores, y por margen alto, que los errores de prueba. Podemos ver que hay un desacuerdo entre el proceso de validación y de prueba: Los valores de validación y de entrenamiento están intercalados, pues fueron seleccionados al azar. Pero el error de predicción se calcula para el futuro, y esos datos futuros no tienen traslape en tiempo con la muestra de entrenamiento. De esta manera, podríamos decir que cuando hacemos predicciones para el conjunto de validación, se nos filtran valores del futuro cercano, lo cual no tenemos disponible a la hora de probar el modelo. Podríamos cambiar nuestra manera de probar el modelo, escogendo la muestra de validación al final del periodo. separar_valid_futura &lt;- function(df, prop){ df &lt;- df %&gt;% rowwise %&gt;% mutate(tipo = ifelse(t &lt; nrow(df)*prop[1], &#39;entrena&#39;, ifelse(t&lt;nrow(df)*(prop[1]+prop[2]),&#39;valida&#39;,&#39;prueba&#39;))) split(df, df$tipo) } reps_2 &lt;- map(1:50, simular_datos, n = 500) %&gt;% map(separar_valid_futura, prop= c(0.6,0.2,0.2)) %&gt;% map(ajustar_evaluar) %&gt;% transpose %&gt;% map(unlist) %&gt;% as_data_frame gr_reps_2 &lt;- reps_2 %&gt;% mutate(rep = row_number()) %&gt;% gather(tipo, valor, -rep) ggplot(gr_reps_2, aes(x=valor, group=tipo, fill=tipo)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(reps_2, aes(x=error_valida, y=error_prueba)) + geom_point() + geom_abline() + xlim(c(0,10)) + ylim(c(0,10)) Observaciónes: Nótese que la fuente más grande de error no proviene de el hecho de que el sistema que queremos predecir es dinámico (el primer modelo, por ejemplo, usa valores más cercanos a los del futuro que queremos predecir). El problema es la filtración de datos del pasado cercano y futuro desde el conjunto de validación al de prueba. Este era parte del problema en hacer validación aleatoria simple en el concurso de las fotos. Un tratamiento de este tipo para el problema de las fotos ayudaba a obtener estimaciones más realistas del desempeño. 13.3 Filtración en el preprocesamiento Cuando preprocesamos datos para incluir en el modelo, es importante asegurarnos de no filtrar información de los datos de validación hacia los datos de enrenamiento. Nos aseguramos de esto si nuestro procesamiento, por ejemplo, es caso por caso con parámetros preestablecidos (no calculamos agregados de todos los datos, por ejemplo), o para más seguridad, haciendo por separado el preprocesamiento de entrenamiento y validación y considerando qué valores pasamos de un conjunto de datos al otro. Un ejemplo clásico es el de selección de variables, como vimos en el examen. Repetiremos varias veces para confirmar más sólidamente la idea seleccion_ajuste &lt;- function(...){ y &lt;- rbinom(50, 1, 0.5) x &lt;- matrix(rnorm(50*500,0,1), 50, 500) correlaciones &lt;- cor(x, y) # Seleccionamos las 50 variables con mayor correlación vars_selec &lt;- order(correlaciones, decreasing=TRUE)[1:50] # Hacemos la validación cruzada usual - que en este caso es errónea est_val_cruzada &lt;- sapply(1:10, function(i){ x_vc &lt;- x[-((5*i -4):(5*i)),] y_vc &lt;- y[-((5*i -4):(5*i))] mod &lt;- glmnet(y=y_vc, x= x_vc[,vars_selec], alpha=0, family=&#39;binomial&#39;, lambda = 0.5) preds_p &lt;- predict(mod, newx = x[((5*i -4):(5*i)),vars_selec])[,1] mean((preds_p &gt; 0) != y[((5*i -4):(5*i))]) }) error_validacion &lt;- mean(est_val_cruzada) modelo &lt;- glmnet(y=y, x= x[,vars_selec], alpha=0, family=&#39;binomial&#39;, lambda = 0.5) y_p &lt;- rbinom(1000, 1, 0.5) x_p &lt;- matrix(rnorm(1000*500,0,1), 1000, 500) preds_p &lt;- predict(modelo, newx = x_p[, vars_selec])[,1] error_prueba &lt;- mean((preds_p &gt; 0) != y_p) c(&#39;error_valida&#39;=error_validacion, &#39;error_prueba&#39;=error_prueba) } seleccion_ajuste() ## error_valida error_prueba ## 0.080 0.495 El resultado es catastrófico otra vez: errores_selec &lt;- map(1:30, seleccion_ajuste) %&gt;% transpose %&gt;% map(unlist) %&gt;% as.data.frame ggplot(errores_selec, aes(x=error_prueba, y=error_valida)) + geom_point() + geom_abline(colour=&#39;red&#39;) + xlim(c(0,1)) + ylim(c(0,1)) Esto lo podemos arreglar haciendo la selección de variables dentro de cada corte de validación cruzada, y así no permitimos que los datos de validación se filtren al conjunto de entrenamiento seleccion_ajuste_correcto &lt;- function(...){ y &lt;- rbinom(50, 1, 0.5) x &lt;- matrix(rnorm(50*500,0,1), 50, 500) est_val_cruzada &lt;- sapply(1:10, function(i){ x_vc &lt;- x[-((5*i -4):(5*i)),] y_vc &lt;- y[-((5*i -4):(5*i))] correlaciones_vc &lt;- cor(x_vc, y_vc) vars_selec &lt;- order(correlaciones_vc, decreasing=TRUE)[1:50] mod &lt;- glmnet(y=y_vc, x= x_vc[,vars_selec], alpha=0, family=&#39;binomial&#39;, lambda = 0.5) preds_p &lt;- predict(mod, newx = x[((5*i -4):(5*i)),vars_selec])[,1] mean((preds_p &gt; 0) != y[((5*i -4):(5*i))]) }) error_validacion &lt;- mean(est_val_cruzada) y_p &lt;- rbinom(1000, 1, 0.5) x_p &lt;- matrix(rnorm(1000*500,0,1), 1000, 500) correlaciones &lt;- cor(x, y) vars_selec &lt;- order(correlaciones, decreasing=TRUE)[1:50] modelo &lt;- glmnet(y=y, x= x[,vars_selec], alpha=0, family=&#39;binomial&#39;, lambda = 0.5) preds_p &lt;- predict(modelo, newx = x_p[, vars_selec])[,1] error_prueba &lt;- mean((preds_p &gt; 0) != y_p) c(&#39;error_valida&#39;=error_validacion, &#39;error_prueba&#39;=error_prueba) } errores_selec &lt;- map(1:30, seleccion_ajuste_correcto) %&gt;% transpose %&gt;% map(unlist) %&gt;% as.data.frame ggplot(errores_selec, aes(x=error_prueba, y=error_valida)) + geom_point() + geom_abline(colour=&#39;red&#39;) + xlim(c(0,1)) + ylim(c(0,1)) 13.4 Uso de variables fuera de rango temporal Otra razón por la que nuestro proceso de validación puede estar contaminado es porque usamos agregados que no están disponibles al momento de la predicción, y están relacionados con la variable que queremos predecir. La contaminación puede ser del conjunto de validación al de entrenamiento, o puede incluir tanto entrenamiento como validación. Imaginemos que queremos predecir los clientes que se van a quedar y los que se van a ir en función de las visitas que hacen a un sitio. Vamos a simular el tiempo que se queda cada cliente independiente de otras variables, y construimos una variable de entrada, el número de visitas, que depende del tiempo que un cliente permanece. Por simplicidad, suponemos que todos los clientes empiezan en el tiempo 0. Vamos a suponer que durante el tiempo 0.5 y 1.5, hubo una campaña de ventas para intentar recuperar a clientes abandonadores. Una fracción los clientes que abandonaron entre el tiempo 0.5 y 1.5 recibieron una llamada de servicio a cliente. Esto está registrado en la base de datos. simular_clientes &lt;- function(n,...){ tiempo_cliente &lt;- rexp(n, 0.5) llamada &lt;- ifelse(tiempo_cliente &gt; 0.5 &amp; tiempo_cliente &lt; 1.5, rbinom(1,1,0.9), 0) #cuántas visitas, dependen del tiempo (proceso de poisson) num_visitas &lt;- 1 + rpois(n, 5*tiempo_cliente) #calculamos los tiempos cuando ocurrieron esos eventos tiempos &lt;- lapply(1:n, function(i){ c(0, runif(num_visitas[i]-1, 0, tiempo_cliente[i]))}) df &lt;- data_frame(id_cliente=1:n, visitas = tiempos, tiempo_cliente = tiempo_cliente, llamada = llamada) df } set.seed(234) simular_clientes(1) %&gt;% unnest ## # A tibble: 2 x 4 ## id_cliente tiempo_cliente llamada visitas ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 0.98248 1 0.0000000 ## 2 1 0.98248 1 0.7624884 clientes_futura &lt;- simular_clientes(20000) %&gt;% unnest Ahora supongamos que hoy estamos en el tiempo t=2, así que los datos que tenemos son los siguientes (también calculamos cuántas visitas ha tendido cada cliente hoy: clientes_hoy &lt;- filter(clientes_futura, visitas &lt; 2) num_visitas_hoy &lt;- clientes_hoy %&gt;% group_by(id_cliente) %&gt;% summarise(num_visitas=n()) Queremos calificar a nuestros clientes actuales con probabilidad de que se vaya, y queremos también evaluar esta predicción. Para hacer esto, usamos los datos con tiempo &lt; 1. ¿Quienes no se han ido? Filtramos clientes activos al tiempo t=1 y vemos quiénes abandonaron al mes t=2 (próximo mes): clientes_1 &lt;- filter(clientes_hoy, tiempo_cliente &gt; 1) %&gt;% mutate(abandona = tiempo_cliente &lt; 2) Para hacer nuestro modelo, ahora usamos el número de visitas de hoy: datos_mod &lt;- clientes_1 %&gt;% left_join(num_visitas_hoy) ## Joining, by = &quot;id_cliente&quot; Y ahora dividimos entre entrenamiento y prueba: set.seed(72427) datos_mod &lt;- datos_mod %&gt;% group_by(id_cliente) %&gt;% summarise(u = runif(1,0,1), abandona = first(abandona), num_visitas=first(num_visitas), llamada = first(llamada)) entrena &lt;- filter(datos_mod, u &lt; 0.5) valida &lt;- filter(datos_mod, u &gt;= 0.5) Ajustamos nuestro modelo mod_1 &lt;- glm(abandona ~ num_visitas + llamada, entrena, family = &#39;binomial&#39;) summary(mod_1) ## ## Call: ## glm(formula = abandona ~ num_visitas + llamada, family = &quot;binomial&quot;, ## data = entrena) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.24755 -0.70896 -0.53240 0.00014 2.46293 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.32076 0.12668 2.532 0.0113 * ## num_visitas -0.15735 0.01229 -12.799 &lt;2e-16 *** ## llamada 19.44652 172.98954 0.112 0.9105 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 8186.9 on 6114 degrees of freedom ## Residual deviance: 4750.6 on 6112 degrees of freedom ## AIC: 4756.6 ## ## Number of Fisher Scoring iterations: 17 Esto parece tener sentido: cuantas más visitas, menor proabilidad de abandonar. Probamos (con devianza) preds &lt;- predict(mod_1, valida, type = &#39;response&#39;) -2*mean(valida$abandona*log(preds) + (1-valida$abandona)*log(1-preds)) ## [1] 0.7876797 Así que parece ser que nuestro modelo está haciendo una predicción razonablemente buena. Ahora calificamos a los clientes corrientes del día de hoy (t=2) prueba &lt;- clientes_hoy %&gt;% filter(tiempo_cliente&gt;=2) %&gt;% group_by(id_cliente) %&gt;% summarise(num_visitas = length(visitas), tiempo_cliente = first(tiempo_cliente), llamada = first(llamada)) prueba$abandona &lt;- prueba$tiempo_cliente &lt; 3 preds &lt;- predict(mod_1, prueba, type = &#39;response&#39;) -2*mean(prueba$abandona*log(preds) + (1-prueba$abandona)*log(1-preds)) ## [1] 1.571454 Y nuestro modelo se degrada considerablemente - no supimos predecir los abandonadores en el próximo mes. ¿Qué está mal? En primer lugar, tenemos filtración de datos porque la variable llamada contiene información futura del abandono de los clientes - aquellos clientes que abandonaron entre t=1 y t=1.5 usaron una llamada, y esto contamina nuestra muestra de entrenamiento con una variable que indica directamente abandono entre t=1 y t=2. No podemos usar esta variable, porque cuando queramos hacer predicciones no vamos a saber que ventas llamó en el futuro a una persona porque había abandonado. Ajustamos nuestro modelo sin llamada: mod_1 &lt;- glm(abandona ~ num_visitas , entrena, family = &#39;binomial&#39;) summary(mod_1) ## ## Call: ## glm(formula = abandona ~ num_visitas, family = &quot;binomial&quot;, data = entrena) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1191 -0.9487 -0.5624 1.0391 2.7870 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.43313 0.09974 24.39 &lt;2e-16 *** ## num_visitas -0.29981 0.01028 -29.17 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 8186.9 on 6114 degrees of freedom ## Residual deviance: 7089.2 on 6113 degrees of freedom ## AIC: 7093.2 ## ## Number of Fisher Scoring iterations: 4 y probamos preds &lt;- predict(mod_1, valida, type = &#39;response&#39;) -2*mean(valida$abandona*log(preds) + (1-valida$abandona)*log(1-preds)) ## [1] 1.159981 Y como esperábamos, el error es más subió. Ahora calificamos a los clientes corrientes del día de hoy (t=2) prueba &lt;- clientes_hoy %&gt;% filter(tiempo_cliente&gt;=2) %&gt;% group_by(id_cliente) %&gt;% summarise(num_visitas = length(visitas), tiempo_cliente = first(tiempo_cliente), llamada = first(llamada)) prueba$abandona &lt;- prueba$tiempo_cliente &lt; 3 preds &lt;- predict(mod_1, prueba, type = &#39;response&#39;) -2*mean(prueba$abandona*log(preds) + (1-prueba$abandona)*log(1-preds)) ## [1] 1.548026 y vemos que todavía tenemos problemas, aunque menos graves. ¿Qué está pasando? Tenemos filtración adicional de datos porque usamos las visitas totales hasta hoy. Cuando este número es grande, quiere decir que un cliente no abandona en el futuro. Así en el modelo usamos el hecho de que no había abandonado para predecir que no abandonó (!!) Podemos corregir nuestro modelo haciendo: num_visitas_1 &lt;- clientes_hoy %&gt;% filter(visitas &lt; 1) %&gt;% group_by(id_cliente) %&gt;% summarise(num_visitas=n()) datos_mod_2 &lt;- clientes_1 %&gt;% left_join(num_visitas_1) ## Joining, by = &quot;id_cliente&quot; Y ahora dividimos entre entrenamiento y prueba: set.seed(72427) datos_mod_2 &lt;- datos_mod_2 %&gt;% group_by(id_cliente) %&gt;% summarise(u = runif(1,0,1), abandona = first(abandona), num_visitas=first(num_visitas), llamada=first(llamada)) entrena_2 &lt;- filter(datos_mod_2, u &lt; 0.5) valida_2 &lt;- filter(datos_mod_2, u &gt;= 0.5) Ajustamos nuestro modelo mod_2 &lt;- glm(abandona ~num_visitas, entrena_2, family = &#39;binomial&#39;) summary(mod_2) ## ## Call: ## glm(formula = abandona ~ num_visitas, family = &quot;binomial&quot;, data = entrena_2) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.0237 -1.0022 -0.9862 1.3634 1.4301 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.35920 0.07556 -4.754 2e-06 *** ## num_visitas -0.01360 0.01179 -1.153 0.249 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 8186.9 on 6114 degrees of freedom ## Residual deviance: 8185.6 on 6113 degrees of freedom ## AIC: 8189.6 ## ## Number of Fisher Scoring iterations: 4 Nótese que el coeficiente de num_visitas es mucho más chico esta vez. Esto tiene sentido: cuantas más visitas, menor proabilidad de abandonar. Probamos (tasa de correctos) Validamos: preds &lt;- predict(mod_2, valida, type = &#39;response&#39;) -2*mean(valida$abandona*log(preds) + (1-valida$abandona)*log(1-preds)) ## [1] 1.323245 Ahora calificamos a los clientes corrientes del día de hoy (t=2) y vemos qué pasa: prueba &lt;- clientes_hoy %&gt;% filter(tiempo_cliente&gt;=2) %&gt;% group_by(id_cliente) %&gt;% summarise(num_visitas = length(visitas), tiempo_cliente = first(tiempo_cliente), llamada = first(llamada)) prueba$abandona &lt;- prueba$tiempo_cliente &lt; 3 preds &lt;- predict(mod_2, prueba, type = &#39;response&#39;) -2*mean(prueba$abandona*log(preds) + (1-prueba$abandona)*log(1-preds)) ## [1] 1.336908 Y vemos que nuestra validación y desempeño real coinciden, pues nuestro ejercicio de validación ya coincide con la tarea de predicción que nos interesa. En este caso, incluso nuestro proceso de entrenamiento está contaminado con datos que no tendremos cuando hacemos predicciones. Desgraciadamente, en este ejemplo simulado no pudimos hacer nada para predecir abandono (por construcción). Pero una validación incorrecta parecía indicar que nuestro modelo podría aportar algo. 13.5 Datos en conglomerados y muestreo complejo En muestras complejas, con el fin de reducir costos, muchas veces se muestrean casos dentro de lo que se llama comunmente unidades primarias de muestreo. Por ejemplo, las unidades primarias de muestreo pueden ser manzanas, y se muestrean varios hogares dentro de cada manzana. Es más simple técnicamente y mejor desde punto de vista del error tomar hogares al azar (no agrupados), pero los costos generalmente aumentan mucho si no usamos alguna agrupación - en este ejemplo, el encuestador tendría que transportarse continuamente para levantar encuestas que fueran seleccionadas sin agrupaciones. Como casos dentro de unidades primarias de muestreo son similares, y la mayor parte de las unidades primarias de muestreo no son muestreadas, tenemos un riesgo en nuestra validación: si hacemos conjuntos de validación al azar, podemos incluir casos de las mismas unidades primarias dentro de entremiento y validación. La homogeneidad de casos dentro de unidades primarias hace fácil predecir casos de validación, o dicho de otra manera: se nos está filtrando información desde el conjunto de validación al de entrenamiento (a través del comportamiento común dentro de unidades primarias de muestreo). En la realidad, observaremos probablemente casos para los que no tenemos ejemplos de unidades primarias. Así que tenemos que construir nuestra validación para que refleje esta tarea. set.seed(12) upms &lt;- seq(1,100,1) simular_upms &lt;- function(n){ map(seq(1, n, 1), function(upm){ num_upm &lt;- runif(1, 10, 100) a &lt;- runif(1, 0, 100) b &lt;- runif(1, 0, 1) x &lt;- rnorm(num_upm, 0, 0.2) z &lt;- rnorm(1, 0, 1) data_frame(upm = upm, x = x, z= z, y = a + b*x + rnorm(num_upm, 0, 1)) }) %&gt;% bind_rows } dat &lt;- simular_upms(n=100) prueba &lt;- simular_upms(1000) dat &lt;- dat %&gt;% mutate(u=runif(nrow(dat), 0,1)) entrena &lt;- dat %&gt;% filter(u &lt; 0.5) valida &lt;- dat %&gt;% filter(u &gt; 0.5) mod_1 &lt;- randomForest(y~x+z, data=entrena) sd(predict(mod_1, valida)-valida$y) ## [1] 13.56648 sd(predict(mod_1, prueba)- prueba$y) ## [1] 33.67454 La diferencia es considerable. Podemos arreglar haciendo la validación separando distintos upms. dat &lt;- dat %&gt;% mutate(u=runif(nrow(dat), 0,1)) entrena &lt;- dat %&gt;% filter(upm &lt; 50) valida &lt;- dat %&gt;% filter(upm &gt;= 50) mod_1 &lt;- randomForest(y~x+z, data=entrena) sd(predict(mod_1, valida)-valida$y) ## [1] 39.20522 sd(predict(mod_1, prueba)- prueba$y) ## [1] 37.16297 En encuestas reales, este efecto puede variar dependiendo de la capacidad del modelo, el diseño de la encuesta (por ejemplo, si las unidades primarias de muestreo son más homogéneas o menos homogéneas, etc), y puede ir desde un efecto prácticamente ignorable hasta uno muy grande. Ejemplo Otro ejemplo de datos en conglomerados está en nuestro ejemplo de reconocimiento de dígitos. Considera por qué es importante separar a las personas que escribieron los dígitos en entrenamiento y validación, y no los dígitos particulares. 13.5.1 Censura y evaluación incompleta Algunas veces, no todos los datos que quisiéramos tener están disponibles para construir nuestros modelos: algunos clientes o casos, por ejemplo, no están en nuestros datos (son datos censurados). Sin embargo, al poner los modelos en producción, hacemos predicciones para todos los datos, y nuestras predicciones malas para aquellos casos antes censurados pueden dañar severamente el desempeño de nuestros modelos. Este es un ejemplo de datos faltantes, pero más serio: todos las variables de algunos casos están faltantes, y algunas veces ni siquiera sabemos esto. 13.5.2 Ejemplo: tiendas cerradas Supongamos que queremos predecir las ventas de tiendas según las características del un local potencial después de un año de ser abiertas. Este modelo tiene el propósito de dedicir si abrir o uno una tienda en un local posible. Vamos a hacer este ejemplo con datos simulados. h &lt;- function(z) 1/(1+exp(-z)) simular_tiendas &lt;- function(n){ #Variables de entrada x &lt;- rnorm(n, 0, 1) a &lt;- rnorm(n, 0, 1) w &lt;- rbinom(n, 1, 0.5) # respuesta en ventas después de un año z &lt;- 2*x + a+ w + rnorm(n, 0, 0.1) ventas &lt;- exp(z)*1e5 # prob de cerrar es alta cuando las ventas son más bajas p_cerrar &lt;- h(-3 - 2*z) # Algunas tiendas quebraron (dependiendo del nivel de ventas) cerrada &lt;- rbinom(n, 1, prob = p_cerrar) data_frame(id_tienda=1:n, x=x, w=w, a=a, ventas=ventas, cerrada = cerrada) } simular_tiendas(10) ## # A tibble: 10 x 6 ## id_tienda x w a ventas cerrada ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 0.41074492 1 0.07368630 674913.217 0 ## 2 2 0.25781219 0 0.62976474 369101.632 0 ## 3 3 0.80262697 1 -0.34351863 910095.139 0 ## 4 4 -0.35724742 1 0.33934325 213056.958 0 ## 5 5 0.05827291 1 -0.60062549 173309.005 0 ## 6 6 0.82610554 0 0.42638566 790395.601 0 ## 7 7 -1.22796182 1 -1.02894403 7525.901 1 ## 8 8 1.27350037 0 0.70299632 2700229.020 0 ## 9 9 -0.66658104 0 0.02756995 27192.405 0 ## 10 10 -1.60667098 1 0.24560395 13335.742 1 set.seed(923) tiendas_entrena_valida &lt;- simular_tiendas(2000) tiendas_prueba &lt;- simular_tiendas(2000) table(tiendas_entrena_valida$cerrada) ## ## 0 1 ## 1571 429 Ahora supongamos que el sistema borró los datos históricos de las tiendas que cerraron. Nuestros datos para trabajar son entrena_valida &lt;- filter(tiendas_entrena_valida, cerrada == 0) nrow(entrena_valida) ## [1] 1571 set.seed(72427) datos &lt;- entrena_valida %&gt;% ungroup %&gt;% mutate(u = runif(nrow(entrena_valida),0,1)) entrena &lt;- filter(datos, u &lt; 0.5) %&gt;% select(-u) valida &lt;- filter(datos, u &gt;= 0.5) %&gt;% select(-u) nrow(entrena) ## [1] 805 nrow(valida) ## [1] 766 mod_log &lt;- randomForest(log(ventas)~x+w+a, data=entrena, mtry=3) #mod_log &lt;- lm(log(ventas)~x+w+a, data=entrena) preds_log &lt;- predict(mod_log, valida) valida$preds_valida &lt;- preds_log sd(preds_log-log(valida$ventas)) ## [1] 0.3000081 ggplot(valida, aes(y=log(ventas), x= preds_valida))+ geom_point() + geom_abline(colour=&#39;red&#39;) Cuando lo aplicamos a nuevas tiendas, desgraciadamente, observamos preds_log &lt;- predict(mod_log, tiendas_prueba) sd(preds_log-log(tiendas_prueba$ventas)) ## [1] 0.5730244 El error es más alto de lo que esperábamos, y nuestra predicción para las tiendas cerradas es especialmente malo: tiendas_prueba$pred_prueba &lt;- preds_log ggplot(tiendas_prueba, aes(y=log(ventas), x= pred_prueba,colour=cerrada))+ geom_point() + geom_abline(colour=&#39;red&#39;) Veamos la cadena que produjo este error: La variable cerrar está naturalmente relacionada con ventas: cuanto más bajas son las ventas al año, mayor la probabilidad de cerrar. En los datos de entrenamiento no tenemos las tiendas que cerraron (que tienen ventas más bajas) - estos datos están censurados Nuestro modelo se desempeña bien para tiendas que tienen ventas relativamente altas. Pero falla cuando intentamos predecir tiendas con ventas relativamente bajas. Soluciones para este problema son analizar cuidadosamente que datos han sido censurados de las bases de datos. En caso de que haya ocurrido, rara vez todos los datos fueron borrados: por ejemplo, quizá la variable respuesta se puede conseguir, y existen algunas de las variables explicativas - en este caso podríamos intentar imputación de datos. 13.6 Muestras de validación chicas Una muestra de validación chica es casi tan malo como una muestra de entrenamiento chica. Una muestra de entrenamiento grande nos permite intentar modelos más complejos y flexibles. Pero con una muestra de validación demasiado chica, no es posible discriminar entre los que se desempeñan bien y mal, desaprovechando las ganancias que podríamos tener por tener una buena muestra de entrenamiento. Podemos ver la situación con el ejemplo de spam spam &lt;- read_csv(&#39;datos/spam-entrena.csv&#39;) spam_prueba &lt;- read_csv(&#39;datos/spam-prueba.csv&#39;) nrow(spam) ## [1] 3067 nrow(spam_prueba) ## [1] 1534 spam &lt;- bind_cols(spam, data.frame(matrix(rnorm(nrow(spam)*100,0,1), nrow(spam), 100))) spam_prueba &lt;- bind_cols(spam_prueba, data.frame(matrix(rnorm(nrow(spam_prueba)*100,0,1), nrow(spam_prueba), 100))) Haremos cortes de distinto tamaño entrenamiento/validación y veremos qué desempeño resulta de escoger nuestro modelo final (lasso) usando una muestra de validación. library(glmnet) separar &lt;- function(datos, prop_entrena){ n &lt;- nrow(datos) datos &lt;- datos %&gt;% mutate(u = runif(n, 0, 1)) %&gt;% mutate(tipo = ifelse(u &lt; prop_entrena, &#39;entrena&#39;, &#39;validación&#39;)) %&gt;% select(-u) print(table(datos$tipo)) datos } devianza &lt;- function(z, y){ apply(-2*(y*z - log(1+exp(z))),2,mean) } ajusta_valida &lt;- function(datos, spam_prueba){ entrena &lt;- datos %&gt;% filter(tipo ==&#39;entrena&#39;) %&gt;% select(-tipo) validación &lt;- datos %&gt;% filter(tipo==&#39;validación&#39;) %&gt;% select(-tipo) x &lt;- as.matrix(entrena %&gt;% select(-spam)) y &lt;- entrena$spam mod &lt;- glmnet(x = x, y = y, alpha = 0.0, family =&#39;binomial&#39;, lambda = exp(seq(-20, -2, 0.25) )) x_val &lt;- as.matrix(validación %&gt;% select(-spam)) y_val &lt;- validación$spam x_prueba &lt;- as.matrix(spam_prueba %&gt;% select(-spam)) y_prueba &lt;- spam_prueba$spam val_error &lt;- devianza(predict(mod, x_val, type=&#39;response&#39;), y_val) prueba_error &lt;- devianza(predict(mod, x_prueba, type=&#39;response&#39;), y_prueba) #val_error &lt;- apply((predict(mod, x_val) &gt; 0) != (y_val==1), 2, mean) #prueba_error &lt;- apply((predict(mod, x_prueba) &gt; 0) != (y_prueba==1), 2, mean) data_frame(lambda = mod$lambda, val_error=val_error, prueba_error = prueba_error) } Si la muestra de validación es chica, podemos escoger un modelo subóptimo, además que la estimación del error es mala library(tidyr) set.seed(923) dat &lt;- separar(spam, 0.98) ## ## entrena validación ## 3011 56 df_1 &lt;- ajusta_valida(dat, spam_prueba) %&gt;% gather(tipo, valor, -lambda) ggplot(df_1, aes(x=lambda, y=valor, group=tipo, colour=tipo))+ geom_line() + geom_point() + scale_x_log10() En este caso escogemos un modelo bueno, pero la estimación es mala set.seed(91123) dat &lt;- separar(spam, 0.98) ## ## entrena validación ## 3004 63 df_1 &lt;- ajusta_valida(dat, spam_prueba) %&gt;% gather(tipo, valor, -lambda) ggplot(df_1, aes(x=lambda, y=valor, group=tipo, colour=tipo))+ geom_line() + geom_point()+ scale_x_log10() Por otro lado, más datos de validación nos dan una mejor estimación el error y nos permite elegir el modelo óptimo. Pero el modelo no es tan bueno porque usamos menos datos de entrenamiento. set.seed(9113) dat &lt;- separar(spam, 0.2) ## ## entrena validación ## 609 2458 df_1 &lt;- ajusta_valida(dat, spam_prueba) %&gt;% gather(tipo, valor, -lambda) ggplot(df_1, aes(x=lambda, y=valor, group=tipo, colour=tipo))+ geom_line() + geom_point()+ scale_x_log10() Cuando tenemos una muestra de validación chica, es posible obtener rangos de error para el error. El error de validación es un promedio sobre una muestra (\\(\\overline{x}\\)), así que podemos estimar su desviación estándar mediante el error estándar \\(\\frac{s}{\\sqrt{n}}\\), donde \\(s\\) es la desviación estándar de los errores individuales de la muestra de entrenamiento. 13.6.0.1 Ejemplo set.seed(91123) dat &lt;- separar(spam, 0.98) ## ## entrena validación ## 3004 63 devianza_valor &lt;- function(z, y){ -2*(y*z - log(1+exp(z))) } entrena &lt;- dat %&gt;% filter(tipo ==&#39;entrena&#39;) %&gt;% select(-tipo) validación &lt;- dat %&gt;% filter(tipo==&#39;validación&#39;) %&gt;% select(-tipo) x &lt;- as.matrix(entrena %&gt;% select(-spam)) y &lt;- entrena$spam mod &lt;- glmnet(x = x, y = y, alpha = 0.0, family =&#39;binomial&#39;, lambda = exp(-10 )) x_val &lt;- as.matrix(validación %&gt;% select(-spam)) y_val &lt;- validación$spam validacion &lt;- devianza_valor(predict(mod, x_val, type=&#39;response&#39;), y_val) Y ahora podemos calcular el estimador puntual y el error estándar: media &lt;- mean(validacion) ee &lt;- sd(validacion)/sqrt(length(validacion)) media ## [1] 1.131755 ee ## [1] 0.05313999 Un intervalo del 95% para esta estimación es entonces c(media-2*ee, media+2*ee) ## [1] 1.025475 1.238035 Si hacemos más grande la muestra de validación dat &lt;- separar(spam, 0.5) ## ## entrena validación ## 1555 1512 entrena &lt;- dat %&gt;% filter(tipo ==&#39;entrena&#39;) %&gt;% select(-tipo) validación &lt;- dat %&gt;% filter(tipo==&#39;validación&#39;) %&gt;% select(-tipo) x &lt;- as.matrix(entrena %&gt;% select(-spam)) y &lt;- entrena$spam mod &lt;- glmnet(x = x, y = y, alpha = 0.0, family =&#39;binomial&#39;, lambda = exp(-10 )) x_val &lt;- as.matrix(validación %&gt;% select(-spam)) y_val &lt;- validación$spam validacion &lt;- devianza_valor(predict(mod, x_val, type=&#39;response&#39;), y_val) media &lt;- mean(validacion) ee &lt;- sd(validacion)/sqrt(length(validacion)) media ## [1] 1.221689 ee ## [1] 0.01238195 c(media-2*ee, media+2*ee) ## [1] 1.196925 1.246453 Ejercicio Repite el ejercicio anterior para la tasa de clasificación incorrecta (ajusta un modelo y calcula el estimador de validación del error junto a su error estándar) Repite el ejercicio anterior para un problema de regresión: en este caso, considera que el error cuadrático medio es el promedio de los errores cuadráticos de cada caso de validación. ¿Cómo harías un intervalo para la raíz del error cuadrático medio? ¿Para el error absoluto promedio? 13.7 Otros ejemplos En kaggle: un concurso para detectar cáncer de próstata contenía una variable que indicaba si el paciente había tenido una operación de próstata o no. Claramente esta variable contiene información acerca de la respuesta, pero un modelo que contiene esta variable no es útil (ve al futuro para la mayoría de los pacientes). En este caso es una filtración de la respuesta a conjunto de entrenamiento y validación. E-commerce: si intentamos predecir quién va a hacer grandes compras, variables como iva (impuesto) incurrido o uso de envío gratis (que solo aplica a compras grandes) son variables que filtran información de lo que queremos predecir y no son útiles en el modelo final. Estas variables también ven al futuro. En kaggle: en el proceso de recolección de los datos, el tamaño de archivos de grabaciones que contenían llamadas de ballenas era diferente de los que no contenían llamadas. Esta es una filtración, pues en la tarea real de predicción no tendremos a alguien que prepare estos archivos de la misma manera. Recientemente se publicó un artículo donde se argumentaba que era posible distinguir (usando redes neuronales convolucionales) caras de criminales y no criminales. Las fotos se obtuvieron de fotos de la policía (criminales) y fotos de idetificaciones (no criminales). ¿Qué crees que podría fallar aquí en términos de filtración de datos? 13.8 Resumen El procesamiento de datos para modelo predictivos es difícil. Cuando hay una dimensión temporal, es bueno usarla a lo largo de todo el proceso para poner una barrera entre entrenamiento y validación. Cuando los datos están organizados en grupos dentro de los que hacemos predicciones, preguntarnos si queremos predecir para nuevos grupos o los mismo grupos existentes (ejemplo de las unidades primarias de muestreo). Investigar cuando hay casos faltantes, y evaluar qué tan peligroso es construir un modelo para hacer predicciones Muchas filtraciones son muy sutiles y dificiles de detectar. Puede tener que ver con cómo funcionan los sistemas que registran los datos, decisiones de diseños de base de datos, decisiones de limpieza de datos. Siempre es bueno proponer un piloto para verificar que nuestros modelos funcionan como se espera - y considerar que una degradación del desempeño puede deberse a una filtración. Finalmente, recordamos que la mejor división es entrenamiento-validación-prueba, con separaciones claras entre ellos. Usamos validación para ajustar hiperparámetros, y con prueba sólo evaluamos unos cuantos modelos. Tarea Ver instrucciones en el script scripts/tarea_11_boosting.Rmd "],
["reduccion-de-dimensionalidad.html", "Clase 14 Reducción de dimensionalidad 14.1 Descomposición aditiva en matrices de rango 1 14.2 Aproximación con matrices de rango 1. 14.3 Aproximación con matrices de rango bajo 14.4 Descomposición en valores singulares (SVD o DVS) 14.5 Interpretación geométrica 14.6 SVD para películas de netflix 14.7 Componentes principales 14.8 ¿Centrar o no centrar por columna? Ejemplos: donde es buena idea centrar Ejemplo: donde no centrar funciona bien 14.9 Otros métodos: t-SNE", " Clase 14 Reducción de dimensionalidad En esta parte veremos métodos no supervisados: no existe variable a predecir. En estos métodos buscamos reexpresiones útiles de nuestros datos para que sean más fáciles de entender o de explorar, para intentar comprimirlos, para poder usarlos de manera más conveniente en procesos posteriores (por ejemplo, predicción). En esta parte veremos técnicas de reducción de dimensionalidad, en particular la descomposición en valores singulares, que es una de las más útiles. La descomposición en valores singulares puede verse como un tipo de descomposición aditiva en matrices de rango uno, así que comenzaremos explicando estos conceptos. 14.1 Descomposición aditiva en matrices de rango 1 Supongamos que tenemos una matriz de datos \\(X\\) de tamaño \\(n\\times p\\) (todas son variables numéricas). En los renglones tenemos los casos (\\(n\\)) y las columnas son las variables \\(p\\). Típicamente pensamos que las columnas o variables están todas definidas en una misma escala: por ejemplo, cantidades de dinero, poblaciones, número de eventos, etc. Cuando no es así, entonces normalizamos las variables de alguna forma para no tener unidades. 14.1.1 Matrices de rango 1 Una de las estructuras de datos más simples que podemos imaginar (que sea interesante) para un tabla de este tipo es que se trata de una matriz de datos de rango 1. Es generada por un score de individuos que determina mediante un peso el valor de una variable. Es decir, el individuo \\(i\\) en la variable \\(j\\) es \\[X_{ij} = \\sigma u_i v_j\\] Donde \\(u=(u_1,u_2,\\ldots, u_n)\\) son los scores de los individuos y \\(v = (v_1, v_2, \\ldots, v_p)\\) son los pesos de las variables. Tanto \\(u\\) como \\(v\\) son vectores normalizados, es decir \\(||u||=||v||=1\\). La constante \\(\\sigma\\) nos permite pensar que los vectores \\(u\\) y \\(v\\) están normalizados. Esto se puede escribir, en notación matricial, como \\[X = \\sigma u v^t\\] donde consideramos a \\(u\\) y \\(v\\) como matrices columna. Una matriz de rango uno (o en general de rango bajo) es más simple de analizar. En rango 1, tenemos que entender la variación de \\(n+p\\) datos (componentes de \\(u\\) y \\(v\\)), mientras que en una matriz general tenemos que entender \\(n\\times p\\) datos. Cada variable \\(j\\) de las observaciones es un reescalamiento del índice o score \\(u\\) de las personas por el factor \\(\\sigma v_j\\). Igualmente, cada caso \\(i\\) de las observaciones es un reescalamiento del índice o peso \\(v\\) de las variables por el factor \\(\\sigma u_i\\). \\(u\\) y \\(v\\) representan una dimensión (dimensión latente, componente) de estos datos. Ejemplo: una matriz de rango 1 de preferencias Supongamos que las columnas de \\(X\\) son películas (\\(p\\)), los renglones (\\(n\\)) personas, y la entrada \\(X_{ij}\\) es la afinidad de la persona \\(i\\) por la película \\(j\\). Vamos a suponer que estos datos tienen una estructura ficticia de rango 1, basada en las preferencias de las personas por películas de ciencia ficción. Construimos los pesos de las películas que refleja qué tanto son de ciencia ficción o no. Podemos pensar que cada uno de estos valores el el peso de la película en la dimensión de ciencia ficción. library(tidyverse) peliculas_nom &lt;- c(&#39;Gladiator&#39;,&#39;Memento&#39;,&#39;X-Men&#39;,&#39;Scream&#39;,&#39;Amores Perros&#39;, &#39;Billy Elliot&#39;, &#39;Lord of the Rings&#39;,&#39;Mulholland drive&#39;, &#39;Amelie&#39;,&#39;Planet of the Apes&#39;) # variable latente que describe el contenido de ciencia ficción de cada v &lt;- c(-1.5, -0.5, 4, -1,-3, -3, 0, 1, -0.5, 3.5) normalizar &lt;- function(x){ norma &lt;- sqrt(sum(x^2)) if(norma &gt; 0){ x_norm &lt;- x/norma } else { x_norm &lt;- x } x_norm } v &lt;- normalizar(v) peliculas &lt;- data_frame(pelicula = peliculas_nom, v = v) %&gt;% arrange(v) peliculas ## # A tibble: 10 x 2 ## pelicula v ## &lt;chr&gt; &lt;dbl&gt; ## 1 Amores Perros -0.420084 ## 2 Billy Elliot -0.420084 ## 3 Gladiator -0.210042 ## 4 Scream -0.140028 ## 5 Memento -0.070014 ## 6 Amelie -0.070014 ## 7 Lord of the Rings 0.000000 ## 8 Mulholland drive 0.140028 ## 9 Planet of the Apes 0.490098 ## 10 X-Men 0.560112 Ahora pensamos que tenemos con individuos con scores de qué tanto les gusta la ciencia ficción set.seed(102) u &lt;- rnorm(15, 0, 1) u &lt;- normalizar(u) personas &lt;- data_frame(persona = 1:15, u = u) head(personas) ## # A tibble: 6 x 2 ## persona u ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.04215632 ## 2 2 0.18325375 ## 3 3 -0.31599557 ## 4 4 0.46314650 ## 5 5 0.28921210 ## 6 6 0.28037223 Podemos entonces construir la afinidad de cada persona por cada película (matriz \\(n\\times p\\) ) multiplicando el score de cada persona (en la dimensión ciencia ficción) por el peso de la película (en la dimensión ciencia ficción). Por ejemplo, para una persona, tenemos que su índice es personas$u[2] ## [1] 0.1832537 Esta persona tiene afinidad por la ciencia ficción, así que sus niveles de gusto por las películas son (multiplicando por \\(\\sigma = 100\\), que en este caso es una constante arbitraria seleccionada para el ejemplo): data_frame(pelicula=peliculas$pelicula, afinidad= 100*personas$u[2]*peliculas$v) %&gt;% arrange(desc(afinidad)) ## # A tibble: 10 x 2 ## pelicula afinidad ## &lt;chr&gt; &lt;dbl&gt; ## 1 X-Men 10.264263 ## 2 Planet of the Apes 8.981230 ## 3 Mulholland drive 2.566066 ## 4 Lord of the Rings 0.000000 ## 5 Memento -1.283033 ## 6 Amelie -1.283033 ## 7 Scream -2.566066 ## 8 Gladiator -3.849099 ## 9 Amores Perros -7.698197 ## 10 Billy Elliot -7.698197 Consideremos otra persona personas$u[15] ## [1] -0.05320133 Esta persona tiene disgusto ligero por la ciencia ficción, y sus scores de las películas son: data_frame(pelicula=peliculas$pelicula, afinidad= 100*personas$u[15]*peliculas$v) ## # A tibble: 10 x 2 ## pelicula afinidad ## &lt;chr&gt; &lt;dbl&gt; ## 1 Amores Perros 2.2349029 ## 2 Billy Elliot 2.2349029 ## 3 Gladiator 1.1174515 ## 4 Scream 0.7449676 ## 5 Memento 0.3724838 ## 6 Amelie 0.3724838 ## 7 Lord of the Rings 0.0000000 ## 8 Mulholland drive -0.7449676 ## 9 Planet of the Apes -2.6073868 ## 10 X-Men -2.9798706 Si fuera tan simple el gusto por las películas (simplemente depende si contienen ciencia ficción o no, y si a la persona le gusta o no), la matriz \\(X\\) de observaciones sería \\[X_1 = \\sigma uv^t\\] donde consideramos a \\(u\\) y \\(v\\) como vectores columna. El producto es de una matriz de \\(n\\times 1\\) contra una de \\(1\\times p\\), lo cual da una matriz de \\(n\\times p\\). Podemos calcular como: X = 100*tcrossprod(personas$u, peliculas$v ) # tcrossprod(x,y) da x %*% t(y) colnames(X) &lt;- peliculas$pelicula head(round(X, 1)) ## Amores Perros Billy Elliot Gladiator Scream Memento Amelie ## [1,] -1.8 -1.8 -0.9 -0.6 -0.3 -0.3 ## [2,] -7.7 -7.7 -3.8 -2.6 -1.3 -1.3 ## [3,] 13.3 13.3 6.6 4.4 2.2 2.2 ## [4,] -19.5 -19.5 -9.7 -6.5 -3.2 -3.2 ## [5,] -12.1 -12.1 -6.1 -4.0 -2.0 -2.0 ## [6,] -11.8 -11.8 -5.9 -3.9 -2.0 -2.0 ## Lord of the Rings Mulholland drive Planet of the Apes X-Men ## [1,] 0 0.6 2.1 2.4 ## [2,] 0 2.6 9.0 10.3 ## [3,] 0 -4.4 -15.5 -17.7 ## [4,] 0 6.5 22.7 25.9 ## [5,] 0 4.0 14.2 16.2 ## [6,] 0 3.9 13.7 15.7 O usando data.frames como peliculas %&gt;% crossing(personas) %&gt;% mutate(afinidad = round(100*u*v, 2)) %&gt;% select(persona, pelicula, afinidad) %&gt;% spread(pelicula, afinidad) ## # A tibble: 15 x 11 ## persona Amelie `Amores Perros` `Billy Elliot` Gladiator ## * &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -0.30 -1.77 -1.77 -0.89 ## 2 2 -1.28 -7.70 -7.70 -3.85 ## 3 3 2.21 13.27 13.27 6.64 ## 4 4 -3.24 -19.46 -19.46 -9.73 ## 5 5 -2.02 -12.15 -12.15 -6.07 ## 6 6 -1.96 -11.78 -11.78 -5.89 ## 7 7 -1.47 -8.79 -8.79 -4.40 ## 8 8 -0.41 -2.49 -2.49 -1.24 ## 9 9 -0.90 -5.39 -5.39 -2.70 ## 10 10 -3.11 -18.67 -18.67 -9.34 ## 11 11 -2.36 -14.17 -14.17 -7.09 ## 12 12 -0.19 -1.13 -1.13 -0.57 ## 13 13 1.03 6.15 6.15 3.08 ## 14 14 2.08 12.45 12.45 6.23 ## 15 15 0.37 2.23 2.23 1.12 ## # ... with 6 more variables: `Lord of the Rings` &lt;dbl&gt;, Memento &lt;dbl&gt;, ## # `Mulholland drive` &lt;dbl&gt;, `Planet of the Apes` &lt;dbl&gt;, Scream &lt;dbl&gt;, ## # `X-Men` &lt;dbl&gt; Nótese que en este ejemplo podemos simplificar mucho el análisis: en lugar de ver la tabla completa, podemos simplemente considerar los dos vectores de índices (pesos y scores), y trabajar como si fuera un problema de una sola dimensión. 14.2 Aproximación con matrices de rango 1. En general, las matrices de datos reales no son de rango 1. Más bien nos interesa saber si se puede hacer una buena aproximación de rango 1. El problema que nos interesa es el inverso: si tenemos la tabla \\(X\\), ¿cómo sabemos si se puede escribir aproximadamente en la forma simple de una matriz de rango uno? Nótese que si lo pudiéramos hacer, esto simplificaría mucho nuestro análisis de estos datos, y obtendríamos información valiosa. Medimos la diferencia entre una matriz de datos general \\(X\\) y una matriz de rango 1 \\(\\sigma uv^t\\) mediante la norma Frobenius: \\[ ||X-\\sigma uv^t||^2_F = \\sum_{i,j} (X_{i,j} - \\sigma u_iv_j)^2\\] Nos interesa resolver \\[\\min_{\\sigma, u,v} || X - \\sigma uv^t ||_F^2\\] donde \\(\\sigma\\) es un escalar, \\(u\\) es un vector columna de tamaño \\(n\\) y \\(v\\) es un vector columna de tamaño \\(p\\). Suponemos que los vectores \\(u\\) y \\(v\\) tienen norma uno. Esto no es necesario - podemos absorber constantes en \\(\\sigma\\). Ejemplo Por ejemplo, la siguiente tabla tiene gastos personales en distintos rubros en distintos años para todo Estados Unidos (en dólares nominales). library(tidyverse) X_arr &lt;- USPersonalExpenditure[, c(1,3,5)] X_arr ## 1940 1950 1960 ## Food and Tobacco 22.200 59.60 86.80 ## Household Operation 10.500 29.00 46.20 ## Medical and Health 3.530 9.71 21.10 ## Personal Care 1.040 2.45 5.40 ## Private Education 0.341 1.80 3.64 En este ejemplo podríamos tener la intuición de que la proporción de gasto se ha mantenido aproximadamente constante en cada año, y que todos los rubros han aumentado debido a la inflación. Podríamos intentar hacer varias normalizaciones para probar esta idea, pero quisiéramos idear una estrategia general. Digamos que el vector \\(u\\) denota los niveles generales de cada rubro (es un vector de longitud 5), y el vector \\(v\\) denota los niveles generales de cada año (un vector de longitud 3). Queremos ver si es razonable aproximar \\[X\\approx uv^t\\] Observación: En este caso, la ecuación de arriba \\(X_{i,j} = u_iv_j\\) expresa que hay niveles generales para cada rubro \\(i\\) a lo largo de todos los años, y para obtener una aproximación ajustamos con un factor \\(v_j\\) de inflación el año \\(j\\) La mejor manera de entender este problema es con álgebra lineal, como veremos más adelante. Por el momento intentemos aproximar directamente, intentando resolver (podemos normalizar \\(u\\) y \\(v\\) más tarde y encontrar la \\(\\sigma\\)): \\[\\min_{u,v} \\sum_{i,j} (X_{i,j} - u_iv_j)^2 = \\min_{u,v} ||X-uv^t||^2_F\\] Observación:Este problema tiene varios mínimos, pues podemos mover constantes de \\(u\\) a \\(v\\) (tiene múltiples soluciones). Hay varias maneras de lidiar con esto (por ejemplo, normalizando). Por el momento, corremos la optimización para encontrar una solución: error &lt;- function(pars){ v &lt;- pars[1:3] u &lt;- pars[4:8] mean((X_arr - tcrossprod(u, v))^2) #tcrossprod da x %*% t(y) } optim_decomp &lt;- optim(rep(0.1, 5 + 3), error, method =&#39;BFGS&#39;) v_años &lt;- optim_decomp$par[1:3] u_rubros &lt;- optim_decomp$par[4:8] La matriz \\(X_1=uv^t\\) que obtuvimos es: X_1 &lt;- tcrossprod(u_rubros, v_años) round(X_1, 1) ## [,1] [,2] [,3] ## [1,] 21.6 58.4 87.8 ## [2,] 11.1 30.1 45.3 ## [3,] 4.7 12.6 18.9 ## [4,] 1.2 3.2 4.8 ## [5,] 0.8 2.2 3.3 Podemos ver qué tan buena es la aproximación: R &lt;- X_arr - X_1 qplot(as.numeric(X_1), as.numeric(as.matrix(X_arr))) + geom_abline(colour=&#39;red&#39;) round(R,2) ## 1940 1950 1960 ## Food and Tobacco 0.60 1.25 -0.98 ## Household Operation -0.65 -1.11 0.90 ## Medical and Health -1.12 -2.87 2.18 ## Personal Care -0.15 -0.77 0.55 ## Private Education -0.46 -0.38 0.36 donde vemos que nuestra aproximación explica en buena parte la variación de los datos en la tabla \\(X\\). La descomposición que obtuvimos es de la forma \\[X = uv^t + R\\] donde \\(R\\) tiene norma Frobenius relativamente chica. Observaciones: Este método nos da un ordenamiento de rubros de gasto según su nivel general, y un ordenamiento de años según su nivel general de gasto. data_frame(rubro = rownames(X_arr), nivel = u_rubros) %&gt;% arrange(desc(nivel)) ## # A tibble: 5 x 2 ## rubro nivel ## &lt;chr&gt; &lt;dbl&gt; ## 1 Food and Tobacco 9.7404116 ## 2 Household Operation 5.0268356 ## 3 Medical and Health 2.0992569 ## 4 Personal Care 0.5380014 ## 5 Private Education 0.3634288 data_frame(año = colnames(X_arr), nivel = v_años) ## # A tibble: 3 x 2 ## año nivel ## &lt;chr&gt; &lt;dbl&gt; ## 1 1940 2.217377 ## 2 1950 5.990600 ## 3 1960 9.011784 Pudimos explicar estos datos usando esos dos índices (5+3=7 números) en lugar de toda la tabla(5(3)=15 números). Una vez explicado esto, podemos concentrarnos en los patrones que hemos aislado en la matriz \\(R\\). Podríamos repetir buscando una aproximación igual a la que acabomos de hacer para la matriz \\(X\\), o podríamos hacer distintos tipos de análisis. 14.2.1 Suma de matrices de rango 1. La matriz de datos \\(X\\) muchas veces no puede aproximarse bien con una sola matriz de rango 1. Podríamos entonces buscar descomponer los datos en más de una dimensión latente: \\[X = \\sigma_1 u_1v_1^t + \\sigma_2 u_2v_2^t+\\ldots+ \\sigma_k u_kv_k^t\\] Ejemplo: películas En nuestro ejemplo anterior, claramente debe haber otras dimensiones latentes que expliquen la afinidad por una película. Por ejemplo, quizá podríamos considerar el gusto por películas mainstream vs películas independientes. peliculas_nom &lt;- c(&#39;Gladiator&#39;,&#39;Memento&#39;,&#39;X-Men&#39;,&#39;Scream&#39;,&#39;Amores Perros&#39;, &#39;Billy Elliot&#39;, &#39;Lord of the Rings&#39;,&#39;Mulholland drive&#39;, &#39;Amelie&#39;,&#39;Planet of the Apes&#39;) # variable latente que describe el contenido de ciencia ficción de cada v_1 &lt;- c(-1.5, -0.5, 4, -1,-3, -3, 0, 1, -0.5, 3.5) v_2 &lt;- c(4.1, 0.2, 3.5, 1.5, -3.0, -2.5, 2.0, -4.5, -1.0, 2.6) #mainstream o no v_1 &lt;- normalizar(v_1) v_2 &lt;- normalizar(v_2) peliculas &lt;- data_frame(pelicula = peliculas_nom, v_1 = v_1, v_2 = v_2) %&gt;% arrange(v_2) peliculas ## # A tibble: 10 x 3 ## pelicula v_1 v_2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mulholland drive 0.140028 -0.50754390 ## 2 Amores Perros -0.420084 -0.33836260 ## 3 Billy Elliot -0.420084 -0.28196884 ## 4 Amelie -0.070014 -0.11278753 ## 5 Memento -0.070014 0.02255751 ## 6 Scream -0.140028 0.16918130 ## 7 Lord of the Rings 0.000000 0.22557507 ## 8 Planet of the Apes 0.490098 0.29324759 ## 9 X-Men 0.560112 0.39475637 ## 10 Gladiator -0.210042 0.46242889 Y las personas tienen también scores en esta nueva dimensión, que aquí simulamos al azar personas &lt;- personas %&gt;% mutate(u_1 = u, u_2 = normalizar(rnorm(15, 0, 1))) %&gt;% select(-u) head(personas) ## # A tibble: 6 x 3 ## persona u_1 u_2 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.04215632 -0.06231047 ## 2 2 0.18325375 -0.37654521 ## 3 3 -0.31599557 0.41966960 ## 4 4 0.46314650 -0.40146285 ## 5 5 0.28921210 0.12705318 ## 6 6 0.28037223 0.18002499 Por ejemplo, la segunda persona persona le gusta la ciencia ficción, y pero prefiere fuertemente películas independientes. Podemos graficar a las personas según su interés en ciencia ficción y mainstream: ggplot(personas, aes(x = u_1, y=u_2)) + geom_point() + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) + xlab(&#39;Ciencia ficción&#39;)+ ylab(&#39;Mainstream&#39;) Y también podemos graficar las películas ggplot(peliculas, aes(x = v_1, y=v_2, label = pelicula)) + geom_point() + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;)+ xlab(&#39;Ciencia ficción&#39;)+ ylab(&#39;Mainstream&#39;) + geom_text() ¿Cómo calculariamos ahora la afinidad de una persona por una película? Necesitamos calcular (dando el mismo peso a las dos dimensiones) \\[X_{i,j} = \\sigma_1 u_{1,i} v_{1,j} + \\sigma_2 u_{2,i} v_{2,j}\\] Usamos la notación \\(u_{k,i}\\) para denotar la componente \\(i\\) del vector \\(u_k\\). Antes pusimos \\(\\sigma_1=100\\). Supongamos que la siguiente componente es un poco menos importante que la primera. Podriamos escoger \\(\\sigma_2=70\\), por ejemplo. Podríamos hacer library(purrr) library(stringr) personas_larga &lt;- personas %&gt;% gather(dimension, u, u_1:u_2) %&gt;% separate(dimension, c(&#39;x&#39;,&#39;dim&#39;), &#39;_&#39;) %&gt;% select(-x) head(personas_larga) ## # A tibble: 6 x 3 ## persona dim u ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 1 0.04215632 ## 2 2 1 0.18325375 ## 3 3 1 -0.31599557 ## 4 4 1 0.46314650 ## 5 5 1 0.28921210 ## 6 6 1 0.28037223 peliculas_larga &lt;- peliculas %&gt;% gather(dimension, v, v_1:v_2) %&gt;% separate(dimension, c(&#39;x&#39;,&#39;dim&#39;), &#39;_&#39;) %&gt;% select(-x) head(peliculas_larga) ## # A tibble: 6 x 3 ## pelicula dim v ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Mulholland drive 1 0.140028 ## 2 Amores Perros 1 -0.420084 ## 3 Billy Elliot 1 -0.420084 ## 4 Amelie 1 -0.070014 ## 5 Memento 1 -0.070014 ## 6 Scream 1 -0.140028 sigma_df &lt;- data_frame(dim = c(&#39;1&#39;,&#39;2&#39;), sigma = c(100,70)) df_dim &lt;- personas_larga %&gt;% left_join(peliculas_larga) %&gt;% left_join(sigma_df) %&gt;% mutate(afinidad = sigma*u*v) ## Joining, by = &quot;dim&quot; ## Joining, by = &quot;dim&quot; df_agg &lt;- df_dim %&gt;% group_by(persona, pelicula) %&gt;% summarise(afinidad = round(sum(afinidad),2)) df_agg %&gt;% spread(pelicula, afinidad) ## # A tibble: 15 x 11 ## # Groups: persona [15] ## persona Amelie `Amores Perros` `Billy Elliot` Gladiator ## * &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.20 -0.30 -0.54 -2.90 ## 2 2 1.69 1.22 -0.27 -16.04 ## 3 3 -1.10 3.33 4.99 20.22 ## 4 4 -0.07 -9.95 -11.53 -22.72 ## 5 5 -3.03 -15.16 -14.66 -1.96 ## 6 6 -3.38 -16.04 -15.33 -0.06 ## 7 7 -1.07 -7.60 -7.80 -6.02 ## 8 8 2.08 4.99 3.74 -11.46 ## 9 9 0.95 0.17 -0.76 -10.29 ## 10 10 -2.44 -16.65 -16.99 -12.10 ## 11 11 -2.01 -13.12 -13.30 -8.52 ## 12 12 -0.34 -1.60 -1.52 0.07 ## 13 13 0.70 5.17 5.33 4.42 ## 14 14 0.01 6.27 7.30 14.68 ## 15 15 -3.43 -9.17 -7.27 16.70 ## # ... with 6 more variables: `Lord of the Rings` &lt;dbl&gt;, Memento &lt;dbl&gt;, ## # `Mulholland drive` &lt;dbl&gt;, `Planet of the Apes` &lt;dbl&gt;, Scream &lt;dbl&gt;, ## # `X-Men` &lt;dbl&gt; Observación: Piensa qué harías si vieras esta tabla directamente, e imagina cómo simplificaría la comprensión y análisis si conocieras las matrices de rango 1 con las que se construyó este ejemplo. Consideremos la persona 2: filter(personas, persona==2) ## # A tibble: 1 x 3 ## persona u_1 u_2 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 0.1832537 -0.3765452 Que tiene gusto por la ciencia ficción y le gustan películas independientes. Sus afinidades son: filter(df_agg, persona==2) %&gt;% arrange(desc(afinidad)) ## # A tibble: 10 x 3 ## # Groups: persona [1] ## persona pelicula afinidad ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 Mulholland drive 15.94 ## 2 2 Amelie 1.69 ## 3 2 Planet of the Apes 1.25 ## 4 2 Amores Perros 1.22 ## 5 2 X-Men -0.14 ## 6 2 Billy Elliot -0.27 ## 7 2 Memento -1.88 ## 8 2 Lord of the Rings -5.95 ## 9 2 Scream -7.03 ## 10 2 Gladiator -16.04 Explicaríamos así esta descomposición: Cada persona \\(i\\) tiene un nivel de gusto por ciencia ficción (\\(u_{1,i}\\)) y otro nivel de gusto por películas independientes (\\(u_{2,i}\\)). Cada película \\(j\\) tiene una calificación o peso en la dimensión de ciencia ficción (\\(v_{1,i}\\)) y un peso en la dimensión de independiente (\\(v_{2,i}\\)) La afinidad de una persona \\(i\\) por una película \\(j\\) se calcula como \\[ \\sigma_1 u_{1,i}v_{1,j} + \\sigma_2 u_{2,i}v_{2,j}\\] Una matriz de rango 2 es una suma (o suma ponderada) de matrices de rango 1 Las explicaciones de matrices de rango aplican para cada sumando (ver arriba) En este caso, hay dos dimensiones latentes que explican los datos: preferencia por independientes y preferencia por ciencia ficción. En este ejemplo ficticio estas componentes explica del todo a los datos. 14.3 Aproximación con matrices de rango bajo Nuestro problema generalmente es el inverso: si tenemos la matriz de datos \\(X\\), ¿podemos encontrar un número bajo \\(k\\) de dimensiones de forma que \\(X\\) se escribe (o aproxima) como suma de matrices de \\(k\\) matrices rango 1? Lograr esto sería muy bueno, pues otra vez simplificamos el análisis a solo un número de dimensiones \\(k\\) (muy) menor a \\(p\\), el número de variables, sin perder mucha información (con buen grado de aproximación). Adicionalmente, las dimensiones encontradas pueden mostrar patrones interesantes que iluminan los datos, esqpecialmente en términos de aquellas dimensiones que aportan mucho a la aproximación. En general, buscamos encontrar una aproximación de la matriz \\(X\\) mediante una suma de matrices de rango 1 \\[X \\approx \\sigma_1 u_1v_1^t + \\sigma_2 v_2v_2^t+\\ldots+ \\sigma_k u_kv_k^t.\\] A esta aproximación le llamamos una aproximación de rango \\(k\\). Hay muchas maneras de hacer esto, y probablemente la mayoría de ellas no son muy interesantes. Podemos más concretamente preguntar, ¿cuál es la mejor aproximación de rango \\(k\\) que hay? \\[\\min_{X_k} || X - X_k ||_F^2\\] donde consideramos la distancia entre \\(X\\) y \\(X_k\\) con la norma de Frobenius, que está definida por: \\[|| A ||_F^2 = \\sum_{i,j} a_{i,j}^2\\] y es una medida de qué tan cerca están las dos matrices \\(A\\) y \\(B\\), componente a componente. 14.3.1 Discusión: aproximación de rango 1. Empecemos resolviendo el problema más simple, que es \\[\\min_{\\sigma,u,v} || X - \\sigma uv^t ||_F^2\\] donde \\(\\sigma\\) es un escalar, \\(u\\) es un vector columna de tamaño \\(n\\) y \\(v\\) es un vector columna de tamaño \\(p\\). Suponemos que los vectores \\(u\\) y \\(v\\) tienen norma uno. El objetivo que queremos minimizar es \\[\\sum_{i,j} (X_{i,j} - \\sigma u_iv_j)^2\\] Derivando con respecto a \\(u_i\\) y \\(v_j\\), e igualando a cero, obtenemos (la sigma podemos quitarla en la derivada, pues multiplica todo el lado derecho): \\[\\frac{\\partial}{\\partial u_i} = -2\\sigma\\sum_{j} (X_{i,j} - \\sigma u_iv_j)v_j = 0\\] \\[\\frac{\\partial}{\\partial v_j} = -2\\sigma\\sum_{i} (X_{i,j} - \\sigma u_iv_j)u_i = 0\\] Que simplificando (y usando que la norma de \\(u\\) y \\(v\\) es igual a 1: \\(\\sum_iu_i^2 = \\sum_j v_j^2=1\\)) quedan: \\[\\sum_j X_{i,j}v_j = \\sigma u_i,\\] \\[\\sum_i X_{i,j}u_i =\\sigma v_j,\\] O en forma matricial \\[\\begin{equation} Xv = \\sigma u \\tag{14.1} \\end{equation}\\] \\[\\begin{equation} u^t X= \\sigma v^t. \\tag{14.2} \\end{equation}\\] Podemos resolver este par de ecuaciones para encontrar la solución al problema de optimización de arriba. Este problema tiene varias soluciones (con distintas \\(\\sigma\\)), pero veremos cómo podemos escoger la que de mejor la aproximación (adelanto: escoger las solución con \\(\\sigma^2\\) más grande). A un par de vectores \\((u,v)\\) que cumplen esta propiedad les llamamos vector propio izquierdo (\\(u\\)) y vector propio derecho (\\(v\\)), con valor singular asociado \\(\\sigma\\). Por convención, tomamos \\(\\sigma \\geq 0\\) (si no, podemos multiplicar a \\(u\\) por menos, por ejemplo). Y tenemos un resultado importante que nos será útil, y que explica el nombre de estos vectores: Si \\((u,v)\\) son vectores propios de \\(X\\) asociados a \\(\\sigma\\), entonces \\(v\\) es un vector propio de la matriz cuadrada \\(X^tX\\) (\\(p\\times p\\)) con valor propio \\(\\sigma^2\\). \\(u\\) es un vector propio de la matrix cuadrada \\(XX^t\\) (\\(n\\times n\\)) con valor propio \\(\\sigma^2\\). Observaciones: La demostración es fácil pues aplicando \\(X^t\\) a ambos lados de (14.1), obtenemos \\(X^t X v= \\sigma X^t u\\), que implica \\((X^t X) v= \\sigma (u^tX)^t = \\sigma^2 v\\). Podemos hacer lo mismo para (14.2). Nótese que \\(X^tX\\) es una matriz simétrica. Por el teorema espectral, existe una base ortogonal de vectores propios (usual) \\(v_1, v_2, \\ldots, v_p\\) con valores propios reales. Adicionalmente, como \\(X^tX\\) es positivo-definida, entonces todos estos vectores propios tienen valor propio no negativos. Ejemplo Verifiquemos en el ejemplo del gasto en rubros. Si comparamos \\(Xv\\) con \\(u\\), vemos que son colineales (es decir, \\(Xv=\\sigma u\\)): # qplot(Xv, u), si Xv=sigma*u entonces Xv y u deben ser proporcionales qplot(as.matrix(X_arr) %*% v_años, u_rubros) + geom_smooth(method=&#39;lm&#39;) Y también # qplot(u^tX, v^t), si u^tXv=sigma*v entonces Xv y u deben ser proporcionales qplot(t(as.matrix(X_arr)) %*% u_rubros, (v_años) ) + geom_smooth(method=&#39;lm&#39;) Ahora normalizamos \\(u\\) y \\(v\\) para encontrar \\(\\sigma\\): u_rubros_norm &lt;- normalizar(u_rubros) v_años_norm &lt;- normalizar(v_años) (as.matrix(X_arr) %*% v_años_norm)/u_rubros_norm ## [,1] ## Food and Tobacco 123.4858 ## Household Operation 123.4855 ## Medical and Health 123.4864 ## Personal Care 123.4891 ## Private Education 123.4799 Y efectivamente vemos que \\((u,v)\\) (normalizados) forman satisfacen las ecuaciones mostradas arriba, con \\(\\sigma\\) igual a: first((as.matrix(X_arr) %*% v_años_norm)/u_rubros_norm) ## [1] 123.4858 Si hay varias soluciones, ¿cuál \\(\\sigma\\) escogemos? Supongamos que encontramos dos vectores propios \\((u,v)\\) (izquierdo y derecho) con valor propio asociado \\(\\sigma\\). Podemos evaluar la calidad de la aproximación usando la igualdad \\[\\||A||_F^2 = traza (AA^t)\\] que es fácil de demostrar, pues la componente \\((i,i)\\) de \\(AA^t\\) está dada por el producto punto del renglon \\(i\\) de A por el renglón \\(i\\) de \\(A\\), que es \\(\\sum_{i,j}a_{i,j}^2.\\) Entonces tenemos que \\[||X-\\sigma uv^t||_F^2 = \\mathrm{Tr} ((X-\\sigma uv^t)(X-\\sigma uv^t)^t)\\] que es igual a \\[ \\mathrm{Tr} (XX^t) - 2\\sigma \\mathrm{Tr} ( X(vu^t)) + \\sigma^2\\mathrm{Tr}(uv^tvu^t)\\] Como \\(u\\) y \\(v\\) tienen norma 1, tenemos que \\(v^tv=1\\), y \\(\\textrm{Tr(uu^t)} = \\sum_i u_i^2 = 1\\). Adicionalmente, usando el hecho de que \\(Xv=\\sigma u\\) obtenemos \\[ ||X-\\sigma uv^t||_F^2 = \\mathrm{Tr} (XX^t) - \\sigma^2\\] que es una igualdad interesante: quiere decir que la mejor aproximación se encuentra encontrando el par de valores propios tal que el valor propio asociado \\(\\sigma\\) tiene el valor \\(\\sigma^2\\) más grande posible. La cantidad a la cantidad \\(\\mathrm{Tr} (XX^t)\\) está dada por \\[\\mathrm{Tr} (XX^t) = ||X||_F^2 = \\sum_{i,j} X_{i,j}^2,\\] que es una medida del “tamaño” de la matriz \\(X\\). 14.3.2 Discusión: aproximaciones de rango más alto Vamos a repetir el análisis para dimensión 2, repitiendo el proceso que hicimos arriba. Denotamos como \\(u_1\\) y \\(v_1\\) los vectores \\(u\\) y \\(v\\) que encontramos en el paso anterior. Ahora buscamos minimizar \\[\\min_{u_2,v_2} || X - \\sigma_1 u_1 v_1^t - \\sigma_2 u_2 v_2^{t} ||_F^2\\] Repetimos el argumento de arriba y derivando respecto a las componentes de \\(u_2,v_2\\), y usando el hecho de que \\((u_1, v_1)\\) son vectores propios derecho e izquierdo asociados a \\(\\sigma_1\\), obtenemos: \\(v_2\\) es ortogonal a \\(v_1\\). \\(u_2\\) es ortogonal a \\(u_1\\). \\((u_2, v_2)\\) tienen que ser vectores propios derecho e izquierdo asociados a \\(\\sigma_2\\geq 0\\). Usando el hecho de que \\(v_1\\) y \\(v_2\\) son ortogonales, podemos podemos demostrar igual que arriba que \\[|| X - \\sigma_1 u_1 v_1^t - \\sigma_2 u_2 v_2^{t} ||_F^2 = \\textrm{Tr} (XX^t) - (\\sigma_1^2 + \\sigma_2^2)\\] De modo que obtenemos la mejor aproximación escogiendo los dos valores de \\(\\sigma_1^2\\) y \\(\\sigma_2^2\\) más grandes para los que hay solución de (14.1) y (14.2) y Observaciones: Aunque aquí usamos un argumento incremental o greedy (comenzando con la mejor aproximación de rango 1), es posible demostrar que la mejor aproximación de rango 2 se puede construir de este modo. Ver por ejemplo estas notas. En el caso de dimensión 2, vemos que la solución es incremental: \\(\\sigma_1, u_1, v_1\\) son los mismos que para la solución de dimensión 1. En dimensión 2, tenemos que buscar el siguiente valor singular más grande después de \\(\\sigma_1\\), de forma que tenemos \\(\\sigma_1^2 \\geq \\sigma_2^2\\). La solución entonces es agregar \\(\\sigma_2 u_2 v_2^t\\), donde \\((u_2,v_2)\\) es el par de vectores propios izquierdo y derecho. Ahora podemos enunciar nuestro teorema: Aproximación de matrices mediante valores singulares Sea \\(X\\) una matriz \\(n\\times p\\), y supongamos que \\(p\\leq n\\). Entonces, para cada \\(k \\leq p\\), la mejor aproximación de rango \\(k\\) a la matriz \\(X\\) se puede escribir como una suma \\(X_k\\) de \\(k\\) matrices de rango 1: \\[X_k = \\sigma_1 u_1v_1^t + \\sigma_2 u_2v_2^t + \\ldots \\sigma_k u_kv_k^t,\\] donde La calidad de la aproximación está dada por \\[||X-X_k||^2_F = ||X||^2_F - (\\sigma_1^2+ \\sigma_2^2 + \\cdots + \\sigma_k^2),\\] de forma que cada aproximación es sucesivamente mejor. \\(\\sigma_1^2 \\geq \\sigma_2^2 \\geq \\cdots \\geq \\sigma_k^2\\geq 0\\) Los vectores \\((u_i,v_i)\\) son un par de vectores propios izquierdo y derechos para \\(X\\) con valor singular \\(\\sigma_i\\). \\(v_1,\\ldots, v_k\\) son vectores ortogonales de norma 1 \\(u_1,\\ldots, u_k\\) son vectores ortogonales de norma 1 Observaciones: Normalmente no optimizamos como hicimos en el ejemplo de la matriz de gastos para encontrar las aproximación de rango bajo, sino que se usan algoritmos para encontrar vectores propios de \\(X^tX\\) (que son las \\(v\\)’s), o más generalmente algoritmos basados en álgebra lineal que intentan encontrar directamente los pares de vectores (u_i, v_i), y otros algoritmos numéricos (por ejemplo, basados en iteraciones). Un resultado interesante (que faltaría por demostrar) es que si tomamos la aproximación de rango \\(p\\) (cuando \\(p\\leq n\\)), obtenemos que \\[X= \\sigma_1 u_1v_1^t + \\sigma_2 u_2v_2^t + \\ldots \\sigma_p u_pv_p^t\\] es decir, la aproximación es exacta. Esto es un fraseo del teorema de descomposición en valores singulares, que normalmente se expresa de otra forma (ver más adelante). Ejemplo Consideremos el ejemplo de los gastos. Podemos usar la función svd de R svd_gasto &lt;- svd(X_arr) El objeto de salida contiene los valores singulares (en d). Nótese que ya habíamos calculado por fuerza bruta el primer valor singular: sigma &lt;- svd_gasto$d sigma ## [1] 123.4857584 4.5673718 0.3762533 Los vectores \\(v_1,v_2,v_3\\) (pesos de las variables) en nuestras tres nuevas dimensiones, que son las columnas de v &lt;- svd_gasto$v rownames(v) &lt;- colnames(X_arr) v ## [,1] [,2] [,3] ## 1940 -0.2007388 -0.3220495 -0.92519623 ## 1950 -0.5423269 -0.7499672 0.37872247 ## 1960 -0.8158342 0.5777831 -0.02410854 y los vectores \\(u_1,u_2,u_3\\), que son los scores de los rubros en cada dimensión dim(svd_gasto$u) ## [1] 5 3 u &lt;- (svd_gasto$u) rownames(u) &lt;- rownames(X_arr) u ## [,1] [,2] [,3] ## Food and Tobacco -0.87130286 -0.3713244 -0.1597823 ## Household Operation -0.44966139 0.3422116 0.4108311 ## Medical and Health -0.18778444 0.8259030 -0.2584369 ## Personal Care -0.04812680 0.2074885 -0.4372590 ## Private Education -0.03250802 0.1408623 0.7400691 Podemos considerar ahora la segunda dimensión que encontramos. En los scores: \\(u_2\\) tiene valores altos en el rubro 3 (salud), y valores negativos en rubro 1. Es un patrón de gasto más alto en todo menos en comida (que es el rubro 1), especialmente en salud. Ahora vemos \\(v_2\\): tiene un valor alto en el año 60 (3a entrada), y valores más negativos para los dos primeros años (40 y 50) Así que decimos que en los 60, el ingreso se desplazó hacia salud (y otros rubros en general), reduciéndose el de comida. Si multiplicamos podemos ver la contribución de esta matriz de rango 1 (en billones (US) de dólares): d &lt;- svd_gasto$d (d[2]*tcrossprod(svd_gasto$u[,2], svd_gasto$v[,2])) %&gt;% round(1) ## [,1] [,2] [,3] ## [1,] 0.5 1.3 -1.0 ## [2,] -0.5 -1.2 0.9 ## [3,] -1.2 -2.8 2.2 ## [4,] -0.3 -0.7 0.5 ## [5,] -0.2 -0.5 0.4 Este es un efecto relativamente chico (comparado con el patrón estable de la primera dimensión), pero ilumina todavía un aspecto adicional de esta tablita. La norma de la diferencia entre la matriz \\(X\\) y la aproximación de rango 2 podemos calcularla de dos maneras: sum(X_arr^2) - sum(d[1:2]^2) ## [1] 0.1415665 O calculando la aproximación y la diferencia directamente. Podemos hacerlo de la siguiente forma X_arr_2 &lt;- d[1]*tcrossprod(u[,1], v[,1]) + d[2]*tcrossprod(u[,2], v[,2]) sum((X_arr - X_arr_2)^2) ## [1] 0.1415665 Pero podemos calcular la aproximación \\(X_2\\) en forma matricial, haciendo X_arr_2 &lt;- u[,1:2] %*% diag(d[1:2]) %*% t(v[,1:2]) sum((X_arr - X_arr_2)^2) ## [1] 0.1415665 14.4 Descomposición en valores singulares (SVD o DVS) Aunque ya hemos enunciado los resultados, podemos enunciar el teorema de descomposición en valores singulares en términos matriciales. Supongamos entonces que tenemos una aproximación de rango \\(k\\) \\[X_k = \\sigma_1 u_1v_1^t + \\sigma_2 u_2v_2^t + \\ldots \\sigma_k u_kv_k^t\\] Se puede ver que esta aproximación se escribe como (considera todos los vectores como vectores columna) \\[ X_k = (u_1,u_2, \\ldots, u_k) \\left( {\\begin{array}{ccccc} \\sigma_1 &amp; 0 &amp; \\cdots &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma_2 &amp; 0 &amp;\\cdots &amp; 0 \\\\ \\vdots &amp; &amp; &amp; \\vdots\\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; \\sigma_k \\\\ \\end{array} } \\right) \\left ( \\begin{array}{c} v_1^t \\\\ v_2^t \\\\ \\vdots \\\\ v_k^t \\end{array} \\right) \\] o más simplemente, como \\[X_k = U_k \\Sigma_k V_k^t\\] donde \\(U_k\\) (\\(n\\times k\\)) contiene los vectores \\(u_i\\) en sus columnas, \\(V_k\\) (\\(k\\times p\\)) contiene los vectores \\(v_j\\) en sus columnas, y la matriz \\(\\Sigma_k\\) es la matriz diagonal con los primeros \\(\\sigma_1\\geq \\sigma_2\\geq\\cdots \\sigma_k\\) valores singulares. Ver el ejemplo anterior para ver cómo los cálculos son iguales. Descomposición en valores singulares Sea \\(X\\) una matriz de \\(n\\times p\\) con \\(p\\leq n\\). Entonces existe una factorización \\[X=U\\Sigma V^t,\\] \\(\\Sigma\\) es una matriz diagonal con valores no-negativos (valores singulares). Los valores singulares de \\(\\Sigma\\) estan ordenados en orden decreciente. Las columnas de U y V son vectores ortogonales unitarios. La i-ésima columna \\(u_i\\) de \\(V\\) y la í-esima columna \\(v_i\\) de \\(V\\) son pares de vectores propios \\((u_i, v_i)\\) izquierdo y derecho de \\(X\\) con valor singular \\(\\sigma_i = \\Sigma_{i,i}\\) Una vez que tenemos esta descomposición, podemos extraer la aproximación que nos sea útil: una aproximación \\(X_k\\) de orden \\(k\\) se escribe como \\[X_k = U_k\\Sigma_k V_k^t\\] donde \\(U_k\\) contiene las primeras \\(k\\) columnas de \\(U\\), \\(V_k\\) las primeras \\(k\\) columnas de \\(V\\), y \\(\\Sigma_k\\) es la submatriz cuadrada \\(k\\times k\\) de los primeros \\(k\\) renglones y columnas de \\(\\Sigma\\) : knitr::include_graphics(&quot;imagenes/svd.png&quot;) Frecuenta el teorema de aproximación óptima (teorema de Ekhart-Young) se deriva de la descomposición en valores singulares, que se demuestra antes usando técnicas de álgebra lineal. 14.5 Interpretación geométrica La descomposición en valores singulares también se puede interpretar geométricamente. Para ver cómo funciona, primero observamos que: Proyecciones Los vectores \\(v_1,v_2, \\ldots, v_p\\) están en el espacio de variables o columnas (son de dimensión \\(p\\)). La componente de la proyección (ver proyección de vectores ) de la matriz de datos sobre una de estas dimensiones está dada por \\[Xv_j,\\] que son iguales a los scores de los casos escalados por \\(\\sigma\\): \\[\\sigma_j u_j\\]. Las proyecciones \\(d_j = \\sigma_j u_j\\) son las variables que normalmente se usan para hacer análisis posterior, aunque cuando la escala de las proyecciones no es importante, también se pueden usar simplemente las \\(u_j\\). Por ejemplo, la projeccion del rengón \\(x_i\\) de la matriz \\(X\\) es \\((x_i^tv_j) v_j\\) (nótese que \\(x_i^tv_j\\) es un escalar, la componente de la proyección). Consideremos unos datos simulados set.seed(3221) x_1 &lt;- rnorm(200,2, 1) x_2 &lt;- rnorm(200,0,1) + x_1 datos &lt;- data_frame(x_1, x_2) ggplot(datos, aes(x=x_1, y=x_2)) + geom_point() + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) Hacemos descomposición en valores singulares y graficamos svd_x &lt;- svd(datos) v &lt;- svd_x$v %&gt;% t %&gt;% as.data.frame() u &lt;- svd_x$u %&gt;% data.frame colnames(v) &lt;- c(&#39;x_1&#39;,&#39;x_2&#39;) colnames(u) &lt;- c(&#39;x_1&#39;,&#39;x_2&#39;) d &lt;- svd_x$d #Nota: podemos mover signos para hacer las gráficas y la interpetación # más simples v[,1] &lt;- - v[,1] u[,1] &lt;- - u[,1] v ## x_1 x_2 ## 1 0.6726219 -0.7399864 ## 2 0.7399864 0.6726219 Graficamos ahora los dos vectores \\(v_1\\) y \\(v_2\\), escalándolos para ver mejor cómo quedan en relación a los datos (esto no es necesario hacerlo): ggplot(datos) + geom_point(aes(x=x_1, y=x_2)) + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) + geom_segment(data = v, aes(xend= 4*x_1, yend=4*x_2, x=0, y=0), col=&#39;red&#39;, size=1.1, arrow = arrow(length = unit(0.3,&quot;cm&quot;))) + coord_equal() El primer vector es el “que pasa más cercano a los puntos”, en el sentido de que la distancia entre los datos proyectados al vector y los datos es lo más chica posible (mejor aproximación). La proyección de los datos sobre \\(v\\) es igual a \\(Xv_1=\\sigma_1 u_1\\), es decir, está dada por \\(\\sigma u_1\\) Las proyecciones de los datos sobre el segundo vector \\(v_2\\) están dadas igualmente por \\(\\sigma_2 u_2\\). Sumamos esta proyección a la de la primera dimensión para obtener una mejor aproximación a los datos (en este caso, exacta). Por ejemplo, seleccionemos el primer punto y obtengamos sus proyecciones: proy_1 &lt;- (d[1])*u[1,1]*v[,1] #v_1 por el score en la dimensión 1 u[1,1] proy_2 &lt;- (d[2])*u[1,2]*v[,2] #v_2 por el score en la dimensión 1 u[1,1] proy_2 + proy_1 ## [1] 3.030313 1.883698 datos[1,] ## # A tibble: 1 x 2 ## x_1 x_2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3.030313 1.883698 Podemos graficar la aproximación sucesiva: datos$selec &lt;- c(&#39;seleccionado&#39;, rep(&#39;no_seleccionado&#39;, nrow(datos)-1)) ggplot(datos) + geom_point(aes(x=x_1, y=x_2, colour=selec, size=selec)) + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) + geom_segment(aes(xend= proy_1[1], yend=proy_1[2], x=0, y=0), col=&#39;red&#39;, size=1.1, arrow = arrow(length = unit(0.3,&quot;cm&quot;))) + geom_segment(aes(xend= proy_2[1] + proy_1[1], yend=proy_2[2] + proy_1[2], x=proy_1[1], y=proy_1[2]), col=&#39;red&#39;, size=1.1, arrow = arrow(length = unit(0.2,&quot;cm&quot;))) + coord_equal() Las aproximaciones de la descomposión en valores singulares mediante matrices de rango 1 puede entenderse como la búsqueda sucesiva de subespacios de dimensión baja, donde al proyectar los datos perdemos poca información. Las proyecciones sucesivas se hacen sobre vectores ortogonales, y en este sentido la DVS separa la información en partes que no tienen contenido común (desde el punto de vista lineal). Finalmente, muchas veces graficamos las proyecciones en el nuevo espacio creado por las dimensiones de la DVS (nótese la escala distinta de los ejes). proyecciones &lt;- data_frame(dim_1 = d[1]*u[,1], dim_2 = d[2]*u[,2], selec = datos$selec) ggplot(proyecciones, aes(x = dim_1, y = dim_2, size=selec, colour=selec)) + geom_point() ## Warning: Using size for a discrete variable is not advised. 14.6 SVD para películas de netflix Vamos a intentar encontrar dimensiones latentes para los datos del concurso de predicción de Netflix (una de las componentes de las soluciones ganadoras fue descomposición en valores singulares). #no correr en notas - son unas 50 millones de evaluaciones # puedes bajar los datos y reproducir desde datos originales bajando el archivo # https://s3.amazonaws.com/netflix-am2017/muestra_calificaciones_1.csv if(FALSE){ evals &lt;- read_csv(&#39;datos/netflix/muestra_calificaciones_1.csv&#39;) evals } peliculas_nombres &lt;- read_csv(&#39;datos/netflix/peliculas_1.csv&#39;) ## Parsed with column specification: ## cols( ## pelicula_id = col_integer(), ## year = col_integer(), ## name = col_character() ## ) peliculas_nombres ## # A tibble: 646 x 3 ## pelicula_id year name ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 2003 Something&#39;s Gotta Give ## 2 2 1992 Reservoir Dogs ## 3 3 2003 X2: X-Men United ## 4 4 2004 Taking Lives ## 5 5 1959 North by Northwest ## 6 6 2004 Harold and Kumar Go to White Castle ## 7 7 2001 Bridget Jones&#39;s Diary ## 8 8 2000 High Fidelity ## 9 9 2000 Pay It Forward ## 10 10 1999 Dogma ## # ... with 636 more rows Hay muchas peliculas que no son evaluadas por ningún usuario. Aquí tenemos que decidir cómo tratar estos datos: si los rellenamos con 0, la implicación es que un usuario tiene bajo interés en una película que no ha visto. Hay otras opciones (y quizá un método que trate apropiadamente los datos faltantes es mejor). #no correr en notas library(Matrix) library(methods) library(irlba) if(FALSE){ evals &lt;- evals %&gt;% group_by(usuario_id) %&gt;% mutate(calif_centrada = calif - mean(calif)) #Usamos matriz rala - de otra manera la matriz es demasiado grande evals_mat &lt;- sparseMatrix(i = evals$usuario_id, j=evals$pelicula_id, x = evals$calif) svd_parcial &lt;- irlba(evals_mat, 4) saveRDS(svd_parcial, file =&#39;cache_obj/svd_netflix.rds&#39;) } svd_parcial &lt;- readRDS(&#39;cache_obj/svd_netflix.rds&#39;) svd_parcial$d ## [1] 16853.865 5346.353 4170.122 3970.022 #no correr en notas if(FALSE){ V_peliculas &lt;- data_frame(v_1 = svd_parcial$v[,1], v_2 = svd_parcial$v[,2], v_3 = svd_parcial$v[,3], v_4 = svd_parcial$v[,4], pelicula_id=1:ncol(evals_mat)) %&gt;% left_join(peliculas_nombres) U_usuarios &lt;- data_frame(u_1 = svd_parcial$u[,1], v_2=svd_parcial$u[,2], u_3 = svd_parcial$u[,3], u_4 = svd_parcial$u[,4], usuario_id = 1:nrow(evals_mat)) saveRDS(V_peliculas, file = &#39;cache_obj/v_peliculas.rds&#39;) saveRDS(U_usuarios, file = &#39;cache_obj/u_usuarios.rds&#39;) } V_peliculas &lt;- readRDS(&#39;cache_obj/v_peliculas.rds&#39;) U_usuarios &lt;- readRDS(&#39;cache_obj/u_usuarios.rds&#39;) Veamos primero las componentes 2, 3 y 4. qplot(svd_parcial$u[,2]) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. qplot(svd_parcial$v[,2])# ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. library(ggrepel) pel_graf &lt;- V_peliculas %&gt;% mutate(dist_0 = sqrt(v_2^2 + v_3^2)) muestra &lt;- pel_graf %&gt;% mutate(etiqueta = ifelse(dist_0 &gt; 0.08, name, &#39;&#39;)) ggplot(muestra, aes(x=v_2, y=v_3, label=etiqueta)) + geom_point(alpha=0.2) + geom_text_repel(size=2.5) + xlab(&#39;Mainstream vs Independiente&#39;) + ylab(&#39;Violenta/Acción vs Drama&#39;) pel_graf &lt;- V_peliculas %&gt;% mutate(dist_0 = sqrt(v_3^2 + v_4^2)) muestra &lt;- pel_graf %&gt;% mutate(etiqueta = ifelse(dist_0 &gt; 0.08, name, &#39;&#39;)) ggplot(muestra, aes(x=v_3, y=v_4, label=etiqueta)) + geom_point(alpha=0.2) + geom_text_repel(size=2.5) + xlab(&#39;Violenta/Acción vs Drama&#39;) + ylab(&#39;Fantasía/Ciencia Ficción&#39;) Dejamos la primer componente porque es más bien consecuencia de cómo construimos la matriz que buscamos descomponer: qplot(svd_parcial$u[,1]) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. qplot(svd_parcial$v[,1]) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Esta componente está asociada con el número de evaluaciones que tiene cada usuario y que tiene cada persona if(FALSE){ evals_num_u &lt;- evals %&gt;% group_by(usuario_id) %&gt;% summarise(num_evals = n()) saveRDS(evals_num_u, &#39;cache_obj/evals_num_u.rds&#39;) } evals_num_u &lt;- readRDS(&#39;cache_obj/evals_num_u.rds&#39;) qplot(evals_num_u$num_evals, svd_parcial$u[,1]) if(FALSE){ evals_num_p &lt;- evals %&gt;% group_by(pelicula_id) %&gt;% summarise(num_evals = n(), calif_prom=mean(calif)) saveRDS(evals_num_p, &#39;cache_obj/evals_num_p.rds&#39;) } evals_num_p &lt;- readRDS(&#39;cache_obj/evals_num_p.rds&#39;) qplot(evals_num_p$num_evals, svd_parcial$v[,1]) Esta dimensión aparece pues la primera aproximación de rango 1 intenta replicar los valores “bajos” de pocas evaluaciones tanto en usuarios como en películas. En realidad es una distorsión producida por cómo hemos tratado los datos (“imputando” cero cuando no existe una evaluación). 14.6.1 Calidad de representación de SVD. Podemos hacer varios cálculos para entender qué tan buena es nuestra aproximación de rango bajo \\(X_k\\). Por ejemplo, podríamos calcular las diferencias de \\(X-X_k\\) y presentarlas de distinta forma. Ejemplo En el ejemplo de rubros de gasto, podríamos mostrar las diferencias en billones (us) de dólares, donde vemos que la aproximación es bastante buena qplot(as.numeric(X_arr-X_arr_2)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Que podríamos resumir, por ejemplo, con la media de errores absolutos: mean(abs(as.numeric(X_arr-X_arr_2))) ## [1] 0.06683576 Otra opción es usar la norma Frobenius, calculando para la apoximación de rango 1 1 - (sum(X_arr^2) - sum(svd_gasto$d[1]^2))/sum(X_arr^2) ## [1] 0.9986246 Lo que indica que capturamos 99.8% de la información, y para la de rango 2: d 1-(sum(X_arr^2) - sum(svd_gasto$d[1:2]^2))/sum(X_arr^2) ## [1] 0.9999907 Lo que indica que estos datos (en 3 variables), podemos entenderlos mediante un análisis de dos dimensiones Podemos medir la calidad de la representación de \\(X\\) (\\(n\\times p\\) con \\(p &lt; n\\)) de una aproximación \\(X_k\\) de SVD mediante \\[1-\\frac{||X-X_k||_F^2}{||X||_F^2} = \\frac{\\sigma_1^2 + \\sigma_2^2 + \\cdots \\sigma_k^2}{\\sigma_1^2 + \\sigma_2^2 + \\cdots \\sigma_p^2},\\] que es un valor entre 0 y 1. Cuanto más cercana a 1 está, mejor es la representación. Observaciones: Dependiendo de nuestro objetivo, nos interesa alcanzar distintos niveles de calidad de representación. Por ejemplo, algunas reglas de dedo: Si queremos usar los datos para un proceso posterior, o dar una descripción casi completa de los datos, quizá buscamos calidad \\(&gt;0.9\\) o mayor. Si nos interesa extraer los patrones más importantes, podemos considerar valores de calidad mucho más chicos, entendiendo que hay una buena parte de la información que no se explica por nuestra aproximación. Ejemplo Para el problema de Netflix podemos calcular la calidad de representación dada por las primeras 4 dimensiones #norma_X &lt;- sum(evals$calif^2) norma_X &lt;- 738958222 sum(svd_parcial$d^2)/norma_X ## [1] 0.4679388 Lo que indica que todavía hay mucha información por explorar en los datos de netflix. La contribución a este porcentaje de cada dimensión (svd_parcial$d[1]^2)/norma_X ## [1] 0.3843962 (svd_parcial$d[2]^2)/norma_X ## [1] 0.0386808 (svd_parcial$d[3]^2)/norma_X ## [1] 0.02353302 (svd_parcial$d[3]^2)/norma_X ## [1] 0.02353302 14.7 Componentes principales Componentes principales es la descomposición en valores singulares aplicada a una matriz de datos centrada por columna. Esta operación convierte el problema de aproximación de matrices de rango bajo en uno de aproximaciones que buscan explicar la mayoría de la varianza (incluyendo covarianza) de las variables de la matriz de datos \\(X\\). Consideremos entonces una matriz de datos \\(X\\) de tamaño \\(n\\times p\\). Definimos la matrix centrada por columna \\(\\tilde{X}\\) , que se calcula como \\[\\tilde{X}_{i,j} = X_{i,j} - \\mu_j\\] donde \\(\\mu_j = \\frac{1}{n} \\sum_j X_{i,j}\\). La diferencia en construcción entre svd y svd con columnas centradas (componentes principales) es que en svd las proyecciones se hacen pasando por el origen, pero en componentes principales se hacen a partir del centroide de los datos Ejemplo Veamos primero el último ejemplo simulado que hicimos anterioremnte. Primero centramos los datos por columna: datos_c &lt;- scale(datos %&gt;% select(-selec), scale = FALSE) %&gt;% as.data.frame ggplot(datos_c, aes(x=x_1, y=x_2)) + geom_point() + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) Y ahora calculamos la descomposición en valores singulares svd_x &lt;- svd(datos_c) v &lt;- svd_x$v %&gt;% t %&gt;% as.data.frame() u &lt;- svd_x$u %&gt;% data.frame colnames(v) &lt;- c(&#39;x_1&#39;,&#39;x_2&#39;) colnames(u) &lt;- c(&#39;x_1&#39;,&#39;x_2&#39;) d &lt;- svd_x$d v ## x_1 x_2 ## 1 0.507328 0.861753 ## 2 0.861753 -0.507328 Notemos que los resultados son similares, pero no son los mismos. Graficamos ahora los dos vectores \\(v_1\\) y \\(v_2\\), que en este contexto se llaman direcciones principales ggplot(datos_c) + geom_point(aes(x=x_1, y=x_2)) + geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) + geom_segment(data = v, aes(xend= 5*x_1, yend=5*x_2, x=0, y=0), col=&#39;red&#39;, size=1.1, arrow = arrow(length = unit(0.3,&quot;cm&quot;))) + coord_equal() Las componentes de las proyecciones de los datos sobre las direcciones principales dan las componentes principales (nótese que multiplicamos por los valores singulares): head(svd_x$u %*% diag(svd_x$d)) ## [,1] [,2] ## [1,] 0.3230641 0.9257829 ## [2,] 0.4070429 1.5360770 ## [3,] -1.2788977 -0.2762829 ## [4,] 0.8910247 0.4071926 ## [5,] -4.4466993 -0.5743111 ## [6,] -1.2267878 0.3470759 Que podemos graficar comps &lt;- svd_x$u %*% diag(svd_x$d) %&gt;% data.frame ggplot(comps, aes(x=X1, y=X2)) + geom_point()+ geom_vline(xintercept = 0, colour=&#39;red&#39;) + geom_hline(yintercept = 0, colour=&#39;red&#39;) Este resultado lo podemos obtener directamente usando la función princomp comp_principales &lt;- princomp(datos %&gt;% select(-selec)) scores &lt;- comp_principales$scores head(scores) ## Comp.1 Comp.2 ## [1,] 0.3230641 -0.9257829 ## [2,] 0.4070429 -1.5360770 ## [3,] -1.2788977 0.2762829 ## [4,] 0.8910247 -0.4071926 ## [5,] -4.4466993 0.5743111 ## [6,] -1.2267878 -0.3470759 Y verificamos que los resultados son los mismos: qplot(scores[,1], comps[,1]) qplot(scores[,2], -comps[,2]) 14.7.1 Varianza en componentes principales. Cuando centramos por columna, la svd es un tipo de análisis de la matriz de varianzas y covarianzas de la matriz \\(X\\), dada por \\[C = \\frac{1}{n} \\tilde{X}^t \\tilde{X}\\] (Nota: asegúrate de que entiendes por qué esta es la matriz de varianzas y covarianzas de \\(X\\)). Nótese que las proyecciones (que se llaman componentes principales) \\(\\tilde{X}v_j = \\sigma_j u_j = d_j\\) satisfacen que La media de las proyecciones \\(d_j\\) es igual a cero Pues \\[\\sigma_j \\sum_k {u_{j,k}} = \\sum_k \\sum_i (\\tilde{X}_{k,i})v_{j,i} = \\sum_i v_{j,i}\\sum_k (\\tilde{X}_{k,i}) = 0,\\] pues las columnas de \\(\\tilde{X}\\) tienen media cero. 2- \\(\\sigma_j^2\\) es la varianza de la proyección \\(d_j\\), pues \\[Var(d_j) = \\sigma_j^2 \\sum_k (u_{j,k} - 0)^2 = \\sigma_j^2,\\] y el vector \\(u_j\\) tienen norma 1. La ortogonalidad de los vectores \\(u_j\\) se interpreta ahora en términos de covarianza: \\[Cov(d_i,d_j) = \\frac{1}{n}\\sum_{k=1}^n (d_{i,k}-0)(d_{j,k}-0) = \\frac{1}{n}\\sum_{k=1}^n \\sigma_j\\sigma_i u_{i,k}u_{j,k} = 0\\] Así que Buscamos sucesivamente direcciones para proyectar que tienen varianza máxima (ver ejemplo anterior), y que sean no correlacionadas de forma que no compartan información lineal entre ellas. Adicionalmente, vimos que podíamos escribir \\[||\\tilde{X}||^2_F = \\sum_{j=1}^p \\sigma_{j}^2\\] Y el lado izquierdo es en este caso una suma de varianzas: \\[\\sum_{j=1}^p Var(X_j) = \\sum_{j=1}^p \\sigma_{j}^2.\\] El lado izquierdo se llama Varianza total de la matriz \\(X\\). Componentes principales particiona la varianza total de la matriz \\(X\\) en componentes . 14.8 ¿Centrar o no centrar por columna? Típicamente, antes de aplicar SVD hacemos algunos pasos de procesamiento de las variables. En componentes principales, este paso de procesamiento es centrar la tabla por columnas. Conviene hacer esto cuando: Centramos si las medias de las columnas no tienen información importante o interesante para nuestros propósitos - es mejor eliminar esta parte de variación desde el principio para no lidiar con esta información en las dimensiones que obtengamos. En otro caso, quizá es mejor no centrar. Centramos si nos interesa más tener una interpretación en términos de varianzas y covarianzas que hacer una aproximación de los datos originales. Sin embargo, también es importante notar que muchas veces los resultados de ambos análisis son similares en cuanto a interpretación y en cuanto a usos posteriores de las dimensiones obtenidas. Pueden ver análisis detallado en este artículo, que hace comparaciones a lo largo de varios conjuntos de datos. Ejemplo: resultados similares En el ejemplo de gasto en rubros que vimos arriba, los pesos \\(v_j\\) son muy similares: comps_1 &lt;- princomp(USPersonalExpenditure[,c(1,3,5)]) svd_1 &lt;- svd(USPersonalExpenditure[,c(1,3,5)]) comps_1$loadings[,] ## Comp.1 Comp.2 Comp.3 ## 1940 -0.2099702 -0.2938755 0.93249650 ## 1950 -0.5623341 -0.7439168 -0.36106546 ## 1960 -0.7998081 0.6001875 0.00905589 svd_1$v ## [,1] [,2] [,3] ## [1,] -0.2007388 -0.3220495 -0.92519623 ## [2,] -0.5423269 -0.7499672 0.37872247 ## [3,] -0.8158342 0.5777831 -0.02410854 comps_1$scores ## Comp.1 Comp.2 Comp.3 ## Food and Tobacco -68.38962 -0.8783065 0.06424607 ## Household Operation -16.25334 0.9562769 -0.16502902 ## Medical and Health 16.13276 2.2900370 0.07312028 ## Personal Care 33.29512 -1.0003211 0.23036177 ## Private Education 35.21507 -1.3676863 -0.20269910 svd_1$u %*% diag(svd_1$d) ## [,1] [,2] [,3] ## [1,] -107.593494 -1.6959766 -0.06011861 ## [2,] -55.526778 1.5630078 0.15457655 ## [3,] -23.188704 3.7722060 -0.09723774 ## [4,] -5.942974 0.9476773 -0.16452015 ## [5,] -4.014277 0.6433704 0.27845344 Llegaríamos a conclusiones similares si interpretamos cualquiera de los dos análisis (verifica por ejemplo el ordenamiento de rubros y años en cada dimensión). Ejemplos: donde es buena idea centrar Por ejemplo, si hacemos componentes principales con los siguientes datos: whisky &lt;- read_csv(&#39;./datos/whiskies.csv&#39;) head(whisky) ## # A tibble: 6 x 17 ## RowID Distillery Body Sweetness Smoky Medicinal Tobacco Honey Spicy ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 01 Aberfeldy 2 2 2 0 0 2 1 ## 2 02 Aberlour 3 3 1 0 0 4 3 ## 3 03 AnCnoc 1 3 2 0 0 2 0 ## 4 04 Ardbeg 4 1 4 4 0 0 2 ## 5 05 Ardmore 2 2 2 0 0 1 1 ## 6 06 ArranIsleOf 2 3 1 1 0 1 1 ## # ... with 8 more variables: Winey &lt;int&gt;, Nutty &lt;int&gt;, Malty &lt;int&gt;, ## # Fruity &lt;int&gt;, Floral &lt;int&gt;, Postcode &lt;chr&gt;, Latitude &lt;int&gt;, ## # Longitude &lt;int&gt; whisky_sabor &lt;- whisky %&gt;% select(Body:Floral) comp_w &lt;- princomp(whisky_sabor) Veamos los pesos de las primeras cuatro dimensiones round(comp_w$loadings[,1:4],2) ## Comp.1 Comp.2 Comp.3 Comp.4 ## Body 0.36 -0.49 -0.03 0.07 ## Sweetness -0.20 -0.05 0.26 0.37 ## Smoky 0.48 -0.07 -0.22 -0.09 ## Medicinal 0.58 0.16 -0.04 -0.08 ## Tobacco 0.09 0.02 0.00 0.03 ## Honey -0.22 -0.42 -0.11 -0.03 ## Spicy 0.06 -0.18 -0.70 0.17 ## Winey -0.04 -0.64 0.23 0.23 ## Nutty -0.05 -0.26 0.18 -0.85 ## Malty -0.13 -0.10 -0.11 -0.07 ## Fruity -0.20 -0.12 -0.40 -0.09 ## Floral -0.38 0.13 -0.34 -0.15 La primera componente separa whisky afrutado/floral/dulce de los whishies ahumados con sabor medicinal. La segunda componente separa whiskies con más cuerpo, características de vino y miel de otros más ligeros. Las siguientes componentes parece oponer Spicy contra Fruity y Floral, y la tercera principalmente contiene la medición de Nutty. Según vimos arriba, podemos ver que porcentaje de la varianza explica cada componente summary(comp_w) ## Importance of components: ## Comp.1 Comp.2 Comp.3 Comp.4 ## Standard deviation 1.5268531 1.2197972 0.86033607 0.79922719 ## Proportion of Variance 0.3011098 0.1921789 0.09560193 0.08250322 ## Cumulative Proportion 0.3011098 0.4932887 0.58889059 0.67139381 ## Comp.5 Comp.6 Comp.7 Comp.8 ## Standard deviation 0.74822104 0.6811330 0.62887454 0.59593956 ## Proportion of Variance 0.07230864 0.0599231 0.05108089 0.04587064 ## Cumulative Proportion 0.74370245 0.8036256 0.85470644 0.90057708 ## Comp.9 Comp.10 Comp.11 Comp.12 ## Standard deviation 0.52041611 0.49757158 0.42174644 0.271073661 ## Proportion of Variance 0.03498097 0.03197728 0.02297382 0.009490848 ## Cumulative Proportion 0.93555805 0.96753533 0.99050915 1.000000000 Y vemos que las primeras dos componentes explican casi el 50% de la varianza. Las siguientes componentes aportan relativamente pooca varianza comparada con la primera Podemos graficar los whiskies en estas dos dimensiones: library(ggrepel) scores_w &lt;- comp_w$scores %&gt;% data.frame scores_w$Distillery &lt;- whisky$Distillery ggplot(scores_w, aes(x=Comp.1, y= -Comp.2, label=Distillery)) + geom_vline(xintercept=0, colour = &#39;red&#39;) + geom_hline(yintercept=0, colour = &#39;red&#39;) + geom_point()+ geom_text_repel(size=2.5, segment.alpha = 0.3, force = 0.1, seed=202) + xlab(&#39;Fruity/Floral vs. Smoky/Medicional&#39;) + ylab(&#39;Winey/Body and Honey&#39;) ¿Que pasa si usamos svd sin centrar? Vemos que la primera componente simplemente captura los distintos niveles promedio de las variables. Esta componente no es muy interesante, pues por las características del whisky es normal que Medicinal o Tabaco tengo una media baja, comparado con dulzor, Smoky, etc. Adicionalmente, el vector \\(u\\) asociado a esta dimensión tiene poca variación: svd_w &lt;- svd(whisky_sabor) svd_w$v[,1:2] ## [,1] [,2] ## [1,] -0.39539241 -0.38286900 ## [2,] -0.42475240 0.19108176 ## [3,] -0.28564145 -0.48775482 ## [4,] -0.09556061 -0.57453247 ## [5,] -0.02078706 -0.09187451 ## [6,] -0.24191199 0.20518808 ## [7,] -0.26485793 -0.07103866 ## [8,] -0.19488910 0.01930294 ## [9,] -0.27858538 0.03430020 ## [10,] -0.33669488 0.11644937 ## [11,] -0.34001725 0.18933847 ## [12,] -0.31441595 0.37730436 plot(svd_w$v[,1], apply(whisky_sabor, 2, mean)) mean(svd_w$u[,1]) ## [1] -0.1063501 sd(svd_w$u[,1]) ## [1] 0.01792508 Observación La primera componente de svd está haciendo el trabajo de ajustar la media. Como no nos interesa este hecho, podemos mejor centrar desde el principio y trabajar con las componentes principales. ¿Cómo se ven las siguientes dos dimensiones del análisis no centrado? Ejemplo: donde no centrar funciona bien Considera el ejemplo de la tarea con la tabla de gastos en distintas categorías de alimentos según el decil de ingreso del hogar. ¿Por qué en este ejemplo centrar por columna no es tan buena idea? Si hacemos el centrado, quitamos información importante de la tabla, que es que los distintos deciles tienen distintos niveles de gasto. Veamos como lucen los dos análisis. Para componentes principales: deciles &lt;- read_csv(&#39;./datos/enigh_deciles.csv&#39;) %&gt;% as.data.frame ## Warning: Missing column names filled in: &#39;X1&#39; [1] deciles %&gt;% arrange(desc(d1)) ## X1 d1 d2 d3 d4 ## 1 CEREALES 1330728 1869247 2254304 2331371 ## 2 CARNES 1072718 1754012 2131706 2514365 ## 3 VERDURAS, LEGUMBRES, LEGUMINOSAS 973984 1279986 1478179 1590063 ## 4 LECHE Y SUS DERIVADOS 585910 895216 1242102 1395102 ## 5 OTROS ALIMENTOS DIVERSOS 290038 448629 689605 781629 ## 6 HUEVO 255321 360471 421613 442603 ## 7 FRUTAS 192462 283549 337608 468187 ## 8 AZUCAR Y MIELES 167042 212941 200200 191048 ## 9 ACEITES Y GRASAS 135823 190052 179945 183546 ## 10 PESCADOS Y MARISCOS 110398 187546 213830 236001 ## 11 TUBERCULOS 107231 158078 190705 201664 ## 12 CAFE, TE Y CHOCOLATE 71945 120338 108609 97139 ## 13 ESPECIAS Y ADEREZOS 57580 80636 91758 108561 ## d5 d6 d7 dd8 d9 d10 ## 1 2576134 2593607 2839141 2770198 2740160 2710885 ## 2 2965671 3228132 3708675 3943535 4183472 4724145 ## 3 1668224 1725576 1783611 1808792 1827177 1982693 ## 4 1582291 1783207 1966252 2123150 2360369 3091577 ## 5 1031991 1115892 1451119 1540150 2282137 2713540 ## 6 405520 404737 451280 418855 398713 365472 ## 7 517938 571262 704867 765013 882037 1384251 ## 8 202397 190093 157009 173545 164273 163299 ## 9 193544 197424 188956 180809 182252 208958 ## 10 286507 297299 333812 437266 496656 865432 ## 11 229090 214818 214251 224368 221747 228002 ## 12 124502 128589 109801 126464 143134 225452 ## 13 116499 134123 155394 152145 167650 182256 deciles &lt;- deciles %&gt;% column_to_rownames(var = &#39;X1&#39;) comp_enigh &lt;- princomp(deciles) Veamos las primeras dos componente, cuyas direcciones principales son: comp_enigh$loadings[,1:2] ## Comp.1 Comp.2 ## d1 -0.1224572 -0.29709420 ## d2 -0.1858230 -0.35463967 ## d3 -0.2324626 -0.34856083 ## d4 -0.2610938 -0.29531199 ## d5 -0.3010861 -0.23322242 ## d6 -0.3221099 -0.16749474 ## d7 -0.3650886 -0.07154327 ## dd8 -0.3783732 0.02514659 ## d9 -0.4019500 0.30553359 ## d10 -0.4425357 0.62905737 Y los scores son: comp_enigh$scores[,1:2] ## Comp.1 Comp.2 ## CEREALES -4548094.6 -1191716.24 ## CARNES -7068531.1 392100.57 ## PESCADOS Y MARISCOS 1880143.9 290076.15 ## LECHE Y SUS DERIVADOS -2688171.0 541441.01 ## HUEVO 1882277.7 -346789.97 ## ACEITES Y GRASAS 2525107.6 -157761.37 ## TUBERCULOS 2460994.3 -134899.61 ## VERDURAS, LEGUMBRES, LEGUMINOSAS -2029622.4 -715856.37 ## FRUTAS 960953.6 445884.20 ## AZUCAR Y MIELES 2551904.3 -217378.43 ## CAFE, TE Y CHOCOLATE 2685873.3 -33326.41 ## ESPECIAS Y ADEREZOS 2679471.1 -33837.02 ## OTROS ALIMENTOS DIVERSOS -1292306.9 1162063.49 Y la tabla de rango 1 es tab_1 &lt;- tcrossprod(comp_enigh$scores[,1], comp_enigh$loadings[,1]) colnames(tab_1) &lt;- colnames(deciles) tab_1 &lt;- tab_1 %&gt;% data.frame %&gt;% mutate(categoria = rownames(deciles)) %&gt;% gather(decil, gasto, d1:d10) tab_1$categoria &lt;- reorder(tab_1$categoria, tab_1$gasto, mean) ggplot(tab_1, aes(x=categoria, y=gasto, colour=decil, group=decil)) + geom_line() + coord_flip() Que podemos comparar con el análisis no centrado: svd_enigh &lt;- svd(deciles) tab_1 &lt;- tcrossprod(svd_enigh$u[,1], svd_enigh$v[,1]) colnames(tab_1) &lt;- colnames(deciles) tab_1 &lt;- tab_1 %&gt;% data.frame %&gt;% mutate(categoria = rownames(deciles)) %&gt;% gather(decil, gasto, d1:d10) tab_1$categoria &lt;- reorder(tab_1$categoria, tab_1$gasto, mean) ggplot(tab_1, aes(x=categoria, y=gasto, colour=decil, group=decil)) + geom_line() + coord_flip() Y aunque los resultados son similares, puede ser más simple entender la primera dimensión del svd no centrado que guarda los efectos de los distintos niveles de gasto de los deciles. En el caso del análisis centrado, tenemos una primera componente que sólo se entiende bien sabiendo los niveles promedio de gasto a lo largo de las categorías. Observación: Quizá una solución más natural es hacer el análisis de componentes principales usando la transpuesta de esta matriz (usa la función prcomp), donde tiene más sentido centrar por categoría de alimento, y pensar que las observaciones son los distintos deciles (que en realidad son agrupaciones de observaciones). 14.8.1 Otros tipos de centrado Es posible hacer doble centrado, por ejemplo (por renglón y por columna). Discute por qué el doble centrado puede ser una buena idea para los datos del tipo de Netflix. 14.8.2 Reescalando variables Cuando las columnas tienen distintas unidades (especialmente si las escalas son muy diferentes), conviene reescalar la matriz antes de hacer el análisis centrado o no centrado. De otra forma, parte del análisis intenta absorber la diferencia en unidades, lo cual generalmente no es de interés. En componentes principales, podemos estandarizar las columnas. En el análisis no centrado, podemos poner las variables en escala 0-1, por ejemplo, o dividir entre la media (si son variables positivas). Ejemplo comp &lt;- princomp(attenu %&gt;% select(-station, -event)) comp$loadings[,1] ## mag dist accel ## -0.005746131 -0.999982853 0.001129688 Y vemos que la dirección de la primera componente es justamente en la dirección de la variable dist (es decir, la primera componente es dist). Esto es porque la escala de dist es más amplia: apply(attenu %&gt;% select(-station), 2, mean) ## event mag dist accel ## 14.7417582 6.0840659 45.6032967 0.1542198 Esto lo corregimos estandarizando las columnas, o equivalentemente, usando cor = TRUE como opción en princomp comp &lt;- princomp(attenu %&gt;% select(-station, -event), cor = TRUE) comp$loadings[,1] ## mag dist accel ## -0.5071375 -0.7156080 0.4803298 14.9 Otros métodos: t-SNE Existen otros métodos para reducir dimensionalidad, como MDS (multidimensional scaling, que se concentra en preservar distancias entre casos), o métodos de embeddings basados en redes neuronales (usar datos en capas ocultas con un número relativamente chico de unidades, cuando queremos que nuestra representación esté asociada a una variable respuesta). Aquí mostramos una de estas técnicas: t-SNE, t-Stochastic Neighbor Embedding. Como motivación a esta técnica, observemos primero que los métodos como componentes principales buscan obtener una representación que mantenga lejos casos que son muy diferentes (pues buscamos aproximar los datos, o encontrar direcciones de máxima varianza). Sin embargo, en algunos casos los que más nos puede interesar es una representación que mantenga casos similares cercanos, aún cuando perdamos información acerca de distancia de casos muy distintos. Ejemplo Consideremos los siguientes datos: x_1 &lt;- c(rnorm(100, 0, 0.5), -20, -25, 20, 22) x_2 &lt;- c(rnorm(50, 1, 0.2), rnorm(50, -1, 0.1), 5, -10, -1, 1) color &lt;- as.character(c(rep(1, 50), rep(2, 50), rep(3, 4))) dat_ej &lt;- data_frame(x_1, x_2, color) ggplot(dat_ej, aes(x = x_1, y = x_2, colour = color)) + geom_point() Notamos que la primera dimensión de svd va en dirección del eje 1, donde hay más dispersión en los datos alrededor del origen. svd(dat_ej[,1:2])$v ## [,1] [,2] ## [1,] -0.99640133 0.08476078 ## [2,] -0.08476078 -0.99640133 Los datos proyectados, sin embargo, oculta la estructura de grupos que hay en los puntos cercanos al origen, pues la proyección es qplot(svd(dat_ej[,1:2])$u[,1], fill = dat_ej$color) + xlab(&#39;u&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Aunque es posible considerar las siguientes aproximaciones, en un problema de dimensión alta esto puede querer decir que nos costará más trabajo encontrar la estructura de casos similares usando estas técnicas lineales. En t-SNE construimos una medida de similitud entre puntos que busca concentrarse en puntos similares. Por ejemplo, la estructura de clusters de estos datos podemos recuperarla en una dimensión: library(tsne) sne_ejemplo &lt;- tsne(as.matrix(dat_ej[,1:2]), k = 1, perplexity=100) qplot(sne_ejemplo, fill = dat_ej$color) + xlab(&#39;u&#39;) 14.9.1 SNE t-SNE es una adaptación de SNE (Stochastic Neighbor Embedding). Veamos primero las ideas básicas de esta técnica: Para controlar las distancias que nos interesa preservar, primero introducimos una medida de similitud entre dos casos (renglones) \\(x^{(i)}, x^{(j)}\\): \\[\\begin{equation} p_{j|i} = \\frac{1}{P_i}\\exp\\left(- ||x_j - x_i ||^2 / 2\\sigma_i^2 \\right) \\tag{14.3} \\end{equation}\\] donde \\(P_i\\) es una constante de normalización tal que \\(\\sum_j {p_{j|i}} = 1\\). Por el momento pensemos que la \\(\\sigma_i\\) está fija en algún valor arbitrario. Notamos que \\(p_{j|i}\\) toma valores cercanos a 1 cuando \\(x_i\\) y \\(x_j\\) están cercanos (su distancia es chica), y rápidamente decae a 0 cuando \\(x_i\\) y \\(x_j\\) se empiezan a alejar. Cuando la \\(\\sigma\\) es chica, si \\(x_i\\) y \\(x_j\\) están aunque sea un poco separados, \\(p_{j|i}\\approx 0\\). Si \\(\\sigma\\) es grande, entonces esta cantidad decae más lentamente cuando los puntos se alejan Podemos pensar que tenemos una campana gaussiana centrada en \\(x_i\\), y la \\(p_{j|i}\\) está dada por la densidad mostrada arriba evaluada en \\(x_j\\) Ahora pensemos que buscamos convertir estos dos puntos a un nuevo espacio de dimensión menor. Denotamos por \\(y_j\\) y \\(y_i\\) a los puntos correpondientes en el nuevo espacio. Definimos análogamente : \\[q_{j|i} = \\frac{1}{Q_i}\\exp\\left(- ||y_j - y_i ||^2 \\right)\\] ¿Cómo encontramos los valores \\(y_j\\)? La idea es intentar aproximar las similitudes derivadas: \\[ p_{j|i} \\approx q_{j|i},\\] Pensemos que \\(i\\) está fija (que corresponden a los puntos \\(x^{(i)}\\) y \\(y^{(i)}\\)). Lo que queremos hacer es aproximar la estructura local alrededor de \\(x^{(i)}\\) mediante los puntos \\(y\\). Y esto queremos hacerlo para cada caso \\(i\\) en los datos. Nótese que estas similitudes son sensibles cuando los puntos están relativamente cerca, pero se hacen rápidamente cero cuando las distancias son más grandes. Esto permite que al intentar hacer la aproximación realmente nos concentremos en la estructura local alrededor de cada punto \\(x^{(i)}\\) Nótese que usamos \\(\\sigma=1\\) en la definición de las \\(q_{j|i}\\), pues esto depende de la escala de la nueva representación (que vamos a encontrar). Dependiendo de la \\(\\sigma_i\\), podemos afinar el método para definir qué tan local es la estructura que queremos aproximar. 14.9.2 Minimización para SNE En SNE, el objetivo a minimizar es la divergencia de Kullback-Liebler, que es una especie de distancia entre distribuciones de probabilidad. Buscamos resolver \\[\\min_{y^{1}, y^{2}, \\ldots, y^{(n)}} \\sum_{i,j} p_{j|i} \\log \\frac{p_{j|i}}{q_{j|i}}\\] Existen muchas maneras de entender esta cantidad. Por lo pronto, notemos que Si \\(p_{j|i} = q_{1|j}\\), entonces esta cantidad es 0 (pues el logaritmo de 1 es cero). Para cada \\(i\\), \\(\\sum_{j} p_{j|i} \\log \\frac{p_{j|i}}{q_{j|i}}\\geq 0\\). Demuestra usando cálculo. Y más importante: \\(p_{j|i} \\log \\frac{p_{j|i}}{q_{j|i}}\\) es muy grande cuando obtenemos una \\(q_{j|i}\\) chica para una \\(p_{j|i}\\) grande (representar lejos puntos cercanos), pero no aumenta tanto si obtenemos una \\(q_{j|i}\\) grande para una \\(p_{j|i}\\) chica. Puedes usar cálculo para convencerte de esto también, por ejemplo, derivando con respecto a las \\(x\\) la cantidad \\(\\sum_i p_i \\log p_i/x_i -\\sum_i x_i\\) (donde \\(q_i = x_i/\\sum_i x_i\\), \\(\\sum_i x_i\\) es la constante de normalización). Finalmente, la divergencia se minimiza usando descenso en gradiente. t-SNE se basa en las mismas ideas, con algunas mejoras (ver paper): Utiliza una versión simétrica de la distancia de Kullback-Liebler (que simplifica el cálculo del gradiente para obtener un algoritmo más rápido), y utiliza una distribución t en el espacio de baja dimensión en lugar de una guassiana. Esto último mejora el desempeño del algoritmo en dimensiones altas. Ejemplo Uno de los ejemplos clásicos es con imágenes de dígitos set.seed(288022) zip_train &lt;- read_csv(&#39;datos/zip-train.csv&#39;) muestra_dig &lt;- zip_train %&gt;% sample_n(500) tsne_digitos &lt;- tsne(muestra_dig[,2:257], max_iter=500) dat_tsne &lt;- data.frame(tsne_digitos) dat_tsne$digito &lt;- as.character(muestra_dig$X1) ggplot(dat_tsne, aes(x=X1, y=X2, colour=digito, label=digito)) + geom_text() Comparemos con componentes principales, que no logra separar muy bien los distintos dígitos: comps &lt;- princomp(muestra_dig[,-1]) dat_comps_tsne &lt;- data.frame(comps$scores[,1:2]) dat_comps_tsne$digito &lt;- as.character(muestra_dig$X1) ggplot(dat_comps_tsne, aes(x=Comp.1, y=Comp.2, colour=digito, label=digito)) + geom_text() y vemos, por ejemplo, que la primera componente está separando principalmente ceros de unos. La primera dirección principal es: image(matrix(comps$loadings[,1], 16,16)) 14.9.3 Perplexity El único parámetro que resta discutir es \\(\\sigma_i\\) en (14.3). La primera idea es que distintas regiones (vecindades de \\(x^{(i)}\\)) pueden requerir distintas \\(\\sigma_i\\). Este valor se escoge de forma que todos los puntos tengan aproximadamente un número fijo (aproximado) de vecinos, a través de un valor que se llama perplexity. De esta forma, \\(\\sigma_i\\) en regiones donde hay densidad alta de datos es más chica, y \\(sigma_i\\) en regiones donde hay menos densidad es más grande (para mantener el número de vecinos aproximadamente similar). Antes de ver más detalles, veamos el ejemplo de los dígitos cambiando el valor de perplejidad: tsne_digitos &lt;- tsne(muestra_dig[,2:257], perplexity = 5, max_iter=500) dat_tsne &lt;- data.frame(tsne_digitos) dat_tsne$digito &lt;- as.character(muestra_dig$X1) ggplot(dat_tsne, aes(x=X1, y=X2, colour=digito, label=digito)) + geom_text() tsne_digitos &lt;- tsne(muestra_dig[,2:257], perplexity = 100, max_iter=500) dat_tsne &lt;- data.frame(tsne_digitos) dat_tsne$digito &lt;- as.character(muestra_dig$X1) ggplot(dat_tsne, aes(x=X1, y=X2, colour=digito, label=digito)) + geom_text() Notas (opcional): si \\(P\\) es el valor de perplejidad escogido, el valor de \\(\\sigma_i\\) escoge de manera que las \\(p_{j|i}\\) obtenidas satisfagan \\(2^{-\\sum_{j} p_{j|i}\\log_2 p_{j|i}}\\approx P\\). El valor de la izquierda en esta ecuación se puede interpretar como una medida suave del número de puntos donde se concentra la masa de probabilidad. Por ejemplo: perp &lt;- function(x){2^sum(-x*log2(x))} p &lt;- c(0.01, 0.01, 0.01, 0.02, 0.95) sum(p) ## [1] 1 perp(p) ## [1] 1.303593 p &lt;- c(0.01, 0.18, 0.01, 0.4, 0.4 ) perp(p) ## [1] 3.107441 p &lt;- c(rep(1,20), rep(20,20)) perp(p/sum(p)) ## [1] 24.21994 Tarea (para 27 de Noviembre) Validación: lee este blogpost, acerca de la construcción apropiada de conjuntos de validación y prueba. Reducción de dimensionalidad: ve script/tarea_12_dimensionalidad.Rmd 14.9.4 Tarea (para 4 de diciembre) Prepara con tu equipo una descripción corta (1-2 párrafos describiendo objetivo, datos y métodos) de lo que piensan hacer como trabajo final. Las condiciones para el examen final son: La presentación final será de unos 7 minutos máximo (se penalizará pasarse del tiempo). Pueden usar un documento de html, pdf o diapositivas para presentar. En caso de ser necesario, les pediré a los equipos el documento presentado con posibles preguntas adicionales. La calificación se hará en tres dimensiones: complejidad y tratamiento de datos (datos complejos y bien procesados dan más puntos), ejecución de los modelos (correcta selección de parámetros, validación), y presentación (explicaciones claras de puntos importantes). Sugerencias: escoger un método o variación de métodos que no hayamos visto en clase, y aplicarlo a unos cuantos ejemplos de datos. También es posible concentrarse en un conjunto de datos y aplicar algunos métodos para obtener las mejores predicciones posibles. Recuerden que es mejor hacer un proyecto relativamente limitado con explicaciones y resultados claros que un proyecto demasiado complejo que no puedan explicar razonablemente bien en el tiempo que tienen. "],
["analisis-de-conglomerados-clustering.html", "Clase 15 Análisis de conglomerados (clustering) 15.1 Introducción 15.2 Enfoques: combinatorio y basado en modelos. 15.3 K-medias 15.4 Selección de número de clusters. 15.5 Dificultades en segmentación/clustering.", " Clase 15 Análisis de conglomerados (clustering) 15.1 Introducción Mediante clustering (o análisis de conglomerados) buscamos aprender agrupaciones de casos, de manera que casos dentro de un grupo (cluster, o conglomerado) estén cercanos entre ellos, mientras que puntos en distintos clusters están a distancia más grande - todo esto de acuerdo a alguna medida de distancia. Entonces podemos: Descubrir estructuras interesantes de agrupamiento en los datos que nos ayuden a entenderlos. Resumir grupos (que pueden ser grandes), por casos representativos y/o promedio. Así la tarea de entender los casos (que pueden ser miles, por ejemplo) se reduce a entender los grupos (que pueden ser decenas o cientos, por ejemplo). Usar grupos como insumo para otras tareas (por ejemplo, distintos modelos predictivos según cluster, distintas estrategias de tratamiento según cluster, etc.) Generalmente el proceso de clustering involucra tres pasos: Selección de métrica de distancia entre puntos. Aplicación de un algoritmo de clustering. Selección de número de clusters. Para variables numéricas, una elección usual es la distancia euclideana (cuando las variables tienen las mismas unidades, o escala similar), o la distancia euclideana para las variables estandarizadas. Ejemplo Consideramos las ocurrencias de temblores cerca de Fiji (The data set give the locations of 1000 seismic events of MB &gt; 4.0. The events occurred in a cube near Fiji since 1964.) quakes_1 &lt;- quakes[, c(&#39;lat&#39;,&#39;long&#39;)] quakes_1$id &lt;- 1:nrow(quakes_1) ggplot(quakes_1, aes(x=long, y=lat)) + geom_point() ¿Podemos separar en grupos estos datos? Podemos pensar en varias maneras de hacer esto. Un enfoque simple que nos puede dar una agrupación interesante es pensar en agrupar de manera que obtengamos grupos relativamente compactos o concentrados alrededor de un valor (en estos datos, ¿qué otras cosas se te ocurren? ¿qué otros patrones interesantes sería interesante agrupar?). 15.2 Enfoques: combinatorio y basado en modelos. Los enfoques basados en modelos (por ejemplo, modelos de mezclas) se basan en la introducción de variables latentes que explican diferencias en las distribuciones de las variables observadas. En estas notas veremos métodos combinatorios, que trabajan directamente sobre los datos (sin modelos), intentanto segmentar en grupos a través de los cuales minimizamos alguna medida objetivo. En este contexto, el problema de segmentación/clustering se plantea como sigue: Suponemos que buscamos \\(K\\) grupos. Una asignación \\(C\\) es una función que asigna a cada observación \\(x_i\\) un grupo \\(C(i)\\in \\{1,\\ldots, k\\}\\). Tenemos una distancia o disimilitud \\(d(x,y)\\) entre puntos, y una función \\(W(C_k)\\) que mide qué tan disperso es el grupo \\(C_k\\) en términos de la distancia \\(d\\). Buscamos entonces encontrar una solución \\(C^*(i)\\) que minimice \\[W(C) = \\sum_{k=1}^K W(C_k),\\] es decir, buscamos que las distancias dentro de cada grupo (within groups) \\(k\\) sean lo más chicas posibles. . Resolver este problema enumerando todas las posibles asignaciones \\(C\\) no es factible, pues el número de posibles asignaciones es típicamente gigantesco aún para un conjunto de datos muy chico. La idea entonces es buscar heurísticas que den soluciones razonables a este problema de minimización. 15.3 K-medias Este es posiblemente el algoritmo (o familia de algoritmos) más popular de segmentación, y se escala razonablemente bien a problemas relativamente grandes. Supongamos que tenemos datos numéricos con escala o unidades similares (si no, podemos estandarizar). En k-medias, un buen agrupamiento es uno en el que la variación dentro de los grupos es chica. En primer lugar fijamos el número \\(K\\) de grupos que buscamos. Supongamos entonces que \\(C_1\\cup\\cdots\\cup C_K\\) es una partición de los datos, y sea \\(W(C_k)\\) nuestra medida de variación dentro de los clusters. Entonces buscaremos resolver \\[\\min_{C_1,\\ldots, C_K} \\sum_{k=1}^K W(C_k)\\] Una medida usual es la siguiente: \\[W(C_k)=\\sum_{i\\in C_k} ||x_i-\\bar{x}_k||^2,\\] donde \\(\\bar{x}_k=\\frac{1}{|C_k|}\\sum_{i\\in C_k} x_i\\) es el centroide del grupo \\(C_k\\). Esto mide qué tan compacto es un grupo considerando la suma de distancias a su centroide. El problema que queremos resolver es entonces \\[\\begin{equation} \\min_{C_1,\\ldots, C_K} \\sum_{k=1}^K \\sum_{i\\in C_k} ||x_i-\\bar{x}_k||^2 \\tag{15.1} \\end{equation}\\] Nótese que cada caso \\(x_k\\) contribuye un sumando a la función objetivo. Este problema es demasiado grande para resolver por fuerza bruta (por ejemplo, enlistando todas las posibles agrupaciones). Podemos desarrollar una heurística considerando primero del problema ampliado \\[\\begin{equation} \\min_{C_1,\\ldots, C_K, m_1,\\ldots, m_K} \\sum_{k=1}^K \\sum_{i\\in C_k}||x_i-m_k||^2 \\tag{15.2} \\end{equation}\\] cuya solución es la misma que el problema (15.1). La razón es que la función objetivo del (15.2) alcanza un mínimo menor o igual al de (15.1), pero dada cualquier solución \\(C_1^*,C_2^*,\\ldots,C_K^*\\) de (15.2), tenemos que para toda \\(k\\), las distancias al cuadrado son mínimas al centroide de los datos: \\[\\sum_{i \\in C_k} ||x_i - \\bar{x}_k||^2 \\leq \\sum_{i \\in C_k} ||x_i - m_k||^2 \\] por lo que \\(C_1^*,C_2^*,\\ldots,C_K^*\\) también es solución de (15.1). Ahora desarrollamos la heurística de k-medias. Comenzando con el problema ampliado, notamos que si fijamos puntos centrales iniciales \\[m_1,\\ldots, m_K\\] podemos minimizar (con centros fijos) \\[\\min_{C_1,\\ldots, C_K} \\sum_{k=1}^K \\sum_{i\\in C_k}||x_i-m_k||^2 \\] asignando cada \\(x_i\\) al cluster \\(C_k\\) con la \\(m_k\\) asociada más cercana a \\(x_i\\). Una vez que tenemos clusters dados, podemos ahora minimizar (con clusters fijos) \\[\\min_{m_1,\\ldots, m_K} \\sum_{k=1}^K \\sum_{i\\in C_k}||x_i-m_k||^2 \\] cuya solución es \\[m_k = \\bar{x}_k = \\frac{1}{n_k}\\sum_{i\\in C_k} x_i,\\] Ahora podemos iterar: agrupar en clusters, encontrar centroide, etc. Esto sugiere el siguiente algoritmo: Algoritmo de k-medias K-means Sea \\(k\\) el número de grupos que buscamos. Escogemos \\(k\\) puntos en los datos de entrenamiento al azar. En el paso \\(s=1,2,\\ldots\\): Dadas los centros \\(m_k\\) (que pensamos fijas), encontramos una nueva asignación \\(C_k\\) a clusters que minimice \\[ 2\\sum_{k=1}^K \\sum_{i\\in C_k} ||x_i - m_k||^2,\\] y esto se hace asignando cada observación al centro \\(m_k\\) que esté más cercano. (cálculo de centroides) Dada una asignación a clusters, encontramos nuevos centros promediando en cada cluster : \\[m_k = \\frac{1}{|C_k|}\\sum_{i\\in C_k} x_i.\\] 3.Repetimos. Nos detenemos cuando los centroides se quedan casi fijos de una iteración a la siguiente (y en consecuencia la cantidad a minimizar no decrece más). Observaciones: Este algoritmo converge (cada paso reduce la función de costo), pero no tiene garantía de obtener un mínimo global. Conviene correr varias veces, para distintos arranques aleatorios, y escoger la solución con función objetivo más chica. Cuando no es posible correrlo múltiples veces, puede ser que la solución obtenida esté muy lejos de una óptima. Hay distintas maneras de implementar este algoritmo, algunas más eficientes que otras. Ejemplo Describiremos iteraciones para \\(k=5\\) para el conjunto de datos, con una implementación simple: quakes_1 &lt;- quakes[, c(&#39;lat&#39;,&#39;long&#39;)] quakes_1$id &lt;- 1:nrow(quakes_1) ggplot(quakes_1, aes(x=long, y=lat)) + geom_point() Seleccionamos muestra de datos al azar (centroides) set.seed(2512) K &lt;- 5 centros &lt;- sample_n(quakes_1, K) %&gt;% mutate(k = 1:K) %&gt;% select(-id) centros ## lat long k ## 1 -18 181 1 ## 2 -20 181 2 ## 3 -24 180 3 ## 4 -23 185 4 ## 5 -14 172 5 ggplot(quakes_1, aes(x=long, y=lat)) + geom_point() + geom_point(data = centros, aes(x=long, y=lat), size=7, colour=&#39;red&#39;) Agrupamos: agrupar &lt;- function(datos, centros){ datos_larga &lt;- datos %&gt;% gather(variable, valor, -id) centros_larga &lt;- centros %&gt;% gather(variable, valor_m, -k) dat &lt;- full_join(datos_larga, centros_larga) %&gt;% mutate(dif_cuad = (valor-valor_m)^2) %&gt;% group_by(id, k) %&gt;% summarise(dist_cuad = sum(dif_cuad)) %&gt;% group_by(id) %&gt;% arrange(id, k) %&gt;% summarise(k = which.min(dist_cuad)) dat &lt;- dat %&gt;% left_join(datos) dat } agrup &lt;- agrupar(quakes_1, centros) ## Joining, by = &quot;variable&quot; ## Joining, by = &quot;id&quot; ggplot(agrup, aes(x=long, y=lat, colour=factor(k))) + geom_point() Recalculamos centros: recalcular_centros &lt;- function(datos_agrup){ datos_agrup %&gt;% gather(variable, valor, -id, -k) %&gt;% group_by(k, variable) %&gt;% summarise(valor = mean(valor)) %&gt;% spread(variable, valor) } centros &lt;- recalcular_centros(agrup) ggplot(quakes_1, aes(x=long, y=lat)) + geom_point() + geom_point(data = centros, aes(x=long, y=lat), size=7, colour=&#39;red&#39;) Y ahora calculamos distancia dentro de clusters: wss &lt;- function(agrupacion, centros){ wss &lt;- agrupacion %&gt;% group_by(k) %&gt;% gather(variable, valor,-id, -k) %&gt;% left_join(centros %&gt;% gather(variable, centro, -k)) %&gt;% group_by(k) %&gt;% summarise(dist_2 = sum((valor - centro)^2)) sum(wss$dist_2) } wss(agrup, centros) ## Joining, by = c(&quot;k&quot;, &quot;variable&quot;) ## [1] 11022 Agrupamos: agrup &lt;- agrupar(quakes_1, centros) ## Joining, by = &quot;variable&quot; ## Joining, by = &quot;id&quot; ggplot(agrup, aes(x=long, y=lat, colour=factor(k))) + geom_point() Recalculamos centros: centros &lt;- recalcular_centros(agrup) wss(agrup, centros) ## Joining, by = c(&quot;k&quot;, &quot;variable&quot;) ## [1] 9657 ggplot(quakes_1, aes(x=long, y=lat)) + geom_point() + geom_point(data = centros, aes(x=long, y=lat), size=7, colour=&#39;red&#39;) Una iteración más da agrup &lt;- agrupar(quakes_1, centros) ## Joining, by = &quot;variable&quot; ## Joining, by = &quot;id&quot; centros &lt;- recalcular_centros(agrup) wss(agrup, centros) ## Joining, by = c(&quot;k&quot;, &quot;variable&quot;) ## [1] 9115 ggplot(agrup, aes(x=long, y=lat, colour=factor(k))) + geom_point() Ejercicio Corre varias veces este ejemplo con distinta semilla. ¿Obtienes mejores o peores soluciones que la de arriba? ¿Qué tan “naturales” crees que son los grupos que obtuvimos? ¿Qué defectos potenciales le ves a este agrupamiento? Usando la funcion k-means La función \\(k-means\\) de R automáticamente produce varias corridas con distintos inicios aleatorios y selecciona la que produzca el mínimo más bajo. set.seed(2800) k_medias &lt;- kmeans(quakes_1[, c(&#39;long&#39;,&#39;lat&#39;)], centers = 5, nstart=30) # escoger varios comienzos aleatorios str(k_medias) ## List of 9 ## $ cluster : int [1:1000] 1 1 1 2 1 2 4 5 5 2 ... ## $ centers : num [1:5, 1:2] 181 184 170 167 181 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:5] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. ..$ : chr [1:2] &quot;long&quot; &quot;lat&quot; ## $ totss : num 62065 ## $ withinss : num [1:5] 1868 2465 314 682 1406 ## $ tot.withinss: num 6736 ## $ betweenss : num 55330 ## $ size : int [1:5] 326 353 69 136 116 ## $ iter : int 3 ## $ ifault : int 0 ## - attr(*, &quot;class&quot;)= chr &quot;kmeans&quot; grupo &lt;- k_medias$cluster quakes_1$grupo &lt;- grupo ggplot(quakes_1, aes(x=long, y=lat, colour=factor(grupo))) + geom_point() Observaciones: En la salida la cantidad k_medias$tot.withinss ## [1] 6736 nos da el valor de (15.1) para la solución obtenida. Observación: El algoritmo usado por defecto en R es el de Hartigan-Wong, que es más eficiente que la implementación simple que vimos arriba. 15.4 Selección de número de clusters. Variación dentro de clusters para distintas soluciones Podemos medir la calidad de la segmentación según el mínimo alcanzado de la función objetivo: la suma de cuadrados dentro de los clusters (withinss), que nos dice qué tan compactos son. Primero vemos un ejemplo simple con tres clusters claros: set.seed(2800) df &lt;- data.frame(x=c(rnorm(100,-50,10), rnorm(100,0,10), rnorm(70,30,2) )) qplot(df$x) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ajustes_km &lt;- lapply(1:20, function(k){ kmedias &lt;- kmeans(df, centers = k, nstart = 20) kmedias }) tot_within &lt;- sapply(ajustes_km, function(aj){ aj$tot.withinss}) datos_codo &lt;- data_frame(no_clusters = 1:length(tot_within), tot_within = tot_within) %&gt;% mutate(sol_3 = no_clusters==3) ggplot(datos_codo, aes(x = no_clusters, y = tot_within)) + geom_line() + geom_point(aes(colour = sol_3), size=3) Agregar un cluster adicional hace más complejo nuestro resumen, así que incrementamos el número de clusters sólo cuando tenemos una mejora considerable en la solución. En este caso particular, un buen lugar para cortar es el codo (en 3 clusters), pues añadir un cluster más no mejora la solución considerablemente. A veces el punto de corte no es tan claro, aunque vemos que en nuestro ejemplo de terremotos la estructura más clara es la de 2 grupos: set.seed(2800) df &lt;- quakes_1[, c(&#39;lat&#39;,&#39;long&#39;)] ajustes_km &lt;- lapply(1:20, function(k){ kmedias &lt;- kmeans(df, centers = k, nstart = 20) kmedias }) tot_within &lt;- sapply(ajustes_km, function(aj){ aj$tot.withinss}) qplot(1:length(tot_within), tot_within, geom=&#39;line&#39;) + geom_point(size=3) Las soluciones son grupos_df &lt;- lapply(ajustes_km[1:8], function(aj){ data_frame(num = max(aj$cluster), cluster = aj$cluster, id= 1:length(aj$cluster))}) %&gt;% bind_rows() grupo_df_2 &lt;- left_join(grupos_df, quakes_1) ## Joining, by = &quot;id&quot; ggplot(grupo_df_2, aes(x=lat, y=long, colour=factor(cluster))) + facet_wrap(~num) + geom_point() La gráfica de codo con la función objetivo es una manera de guiar la selección de número de clusters, aún cuando no siempre da respuestas muy claras. En cualquier caso, rara vez seleccionamos el número de clusters usando solamente el criterio de la gráfica de la función objetivo. 15.4.1 Criterios específicos Más frecuentemente, la selección de número de grupos se hace tomando en cuenta el uso posterior que se va a hacer de los clusters. Por ejemplo: En segmentación de clientes/usuarios, casi siempre queremos un grupo no muy grande de grupos (2-10, máximo decenas), pues puede ser difícil diseñar productos o estrategias particulares si tenemos muchos segmentos.Lo mismo sucede en aplicaciones científicas, por ejemplo clasificar objetos como estrellas, galaxias, etc. Existen algunas aplicaciones de clustering donde buscamos muchos grupos, por ejemplo, para agrupar objetos (noticias, tweets, por ejemplo) en clusters muy compactos (de alta similitud), y así detectar duplicados o relaciones entre fuentes de noticias, usuarios de twitter, etc. Hay más opciones además de algoritmos clásicos como k-means para este tipo de problemas La estrategia típica es entonces producir varias agrupaciones, y compararlas según sus virtudes para uso posterior. Ejemplo Consideremos segmentar personas según sus actitudes hacia las responsabilidades que tiene o no el gobierno en cuanto al bienestar de las personas. European Social Survey (ESS) data from the 2008 (fourth) round in the United Kingdom. The data are from a questionnaire on “what the responsibilities of governments should or should not be”. gvjbevn Job for everyone, governments’ responsibility (0-10). gvhlthc Health care for the sick, governments’ responsibility (0-10). gvslvol Standard of living for the old, governments’ responsibility (0-10). gvslvue Standard of living for the unemployed, governments’ responsibility (0-10). gvcldcr Child care services for working parents, governments’ responsibility (0-10). gvpdlwk Paid leave from work to care for sick family, governments’ responsibility (0-10). sbprvpv Social benefits/services prevent widespread poverty (1-5). sbeqsoc Social benefits/services lead to a more equal society (1-5). sbcwkfm Social benefits/services make it easier to combine work and family (1-5). load(&#39;datos/ess4_gb.Rdata&#39;) dat &lt;- ess4.gb %&gt;% select(idno, gvjbevn:sbcwkfm) nombres &lt;- data_frame(var = c(&quot;gvjbevn&quot;, &quot;gvhlthc&quot;, &quot;gvslvol&quot;, &quot;gvslvue&quot;, &quot;gvcldcr&quot;, &quot;gvpdlwk&quot;, &quot;sbprvpv&quot;, &quot;sbeqsoc&quot;, &quot;sbcwkfm&quot;), nombre = c(&#39;trabajo_a_todos&#39;,&#39;cuidados_salud_enfermos&#39;,&#39;garantizar_nivel_mayores&#39;,&#39;garantizar_nivel_desempleados&#39;,&#39;ayuda_padres_trabajadores&#39;,&#39;ausencia_cuidar_enfermos&#39;,&#39;beneficios_pobreza&#39;,&#39;beneficios_igualdad&#39;,&#39;beneficios_fam_trabajo&#39;)) head(dat) ## idno gvjbevn gvhlthc gvslvol gvslvue gvcldcr gvpdlwk sbprvpv sbeqsoc ## 1 110701 0 10 8 5 5 4 2 4 ## 2 110702 5 10 10 10 5 10 4 4 ## 3 110703 8 9 10 0 8 10 4 4 ## 4 110704 8 10 10 3 8 6 3 2 ## 5 110705 7 10 8 8 9 8 4 2 ## 6 110708 0 10 10 5 7 7 2 2 ## sbcwkfm ## 1 2 ## 2 4 ## 3 3 ## 4 2 ## 5 2 ## 6 2 En este caso particular tenemos unas variables que están en escala 1-5 y otras 1-10. Esta variabilidad distinta sólo es escala de las respuestas, así que normalizamos dividiendo cada pregunta por su máximo (dividir entre 10 las preguntas en escala de 1 a 10 y entre 5 las de 1 a 5. Podríamos también estandarizar): dat_2 &lt;- dat %&gt;% gather(var, valor, gvjbevn:sbcwkfm) %&gt;% left_join(nombres) %&gt;% select(-var) %&gt;% group_by(nombre) %&gt;% mutate(valor_escalado = valor/max(valor, na.rm=T)) %&gt;% select(-valor) %&gt;% spread(nombre, valor_escalado) ## Joining, by = &quot;var&quot; dat_3 &lt;- filter(dat_2, apply(dat_2, 1, function(x){!any(is.na(x))})) dat_3 ## # A tibble: 2,108 x 10 ## idno ausencia_cuidar_enfermos ayuda_padres_trabajadores ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 110701 0.4 0.5 ## 2 110702 1.0 0.5 ## 3 110703 1.0 0.8 ## 4 110704 0.6 0.8 ## 5 110705 0.8 0.9 ## 6 110708 0.7 0.7 ## 7 110710 0.5 0.3 ## 8 110711 0.7 0.5 ## 9 110712 0.9 0.3 ## 10 110713 0.7 0.8 ## # ... with 2,098 more rows, and 7 more variables: ## # beneficios_fam_trabajo &lt;dbl&gt;, beneficios_igualdad &lt;dbl&gt;, ## # beneficios_pobreza &lt;dbl&gt;, cuidados_salud_enfermos &lt;dbl&gt;, ## # garantizar_nivel_desempleados &lt;dbl&gt;, garantizar_nivel_mayores &lt;dbl&gt;, ## # trabajo_a_todos &lt;dbl&gt; ajustes_km &lt;- lapply(1:10, function(k){ kmedias &lt;- kmeans(dat_3[,-1], centers = k, nstart = 20, iter.max=40) kmedias }) tot_within &lt;- sapply(ajustes_km, function(aj){ aj$tot.withinss}) qplot(1:length(tot_within), tot_within, geom=&#39;line&#39;) + geom_point() En esta gráfica no vemos un codo claro. Veamos primero la solución de dos grupos: sol_cl &lt;- ajustes_km[[2]] table(sol_cl$cluster) ## ## 1 2 ## 1116 992 Ahora veamos cómo resumir grupos para entender qué tipo de casos están en cada uno de ellos. Consideramos las variables originales escaladas: cluster_df &lt;- data.frame(idno = dat_3$idno, cluster = sol_cl$cluster) dat_4 &lt;- dat_3 %&gt;% gather(variable, valor, ausencia_cuidar_enfermos:trabajo_a_todos) %&gt;% left_join(cluster_df) ## Joining, by = &quot;idno&quot; Y resumimos dentro de cada grupo cada una de las variables. Elecciones populares son la media y el error estándar de la media (desviación estándar dividida entre la raíz del número de casos): resumen_1 &lt;- dat_4 %&gt;% group_by(cluster, variable) %&gt;% summarise(media = mean(valor), ee = sd(valor)/sqrt(length(valor))) resumen_1 ## # A tibble: 18 x 4 ## # Groups: cluster [?] ## cluster variable media ee ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 ausencia_cuidar_enfermos 0.82 0.0048 ## 2 1 ayuda_padres_trabajadores 0.80 0.0048 ## 3 1 beneficios_fam_trabajo 0.49 0.0047 ## 4 1 beneficios_igualdad 0.55 0.0056 ## 5 1 beneficios_pobreza 0.51 0.0053 ## 6 1 cuidados_salud_enfermos 0.93 0.0032 ## 7 1 garantizar_nivel_desempleados 0.71 0.0057 ## 8 1 garantizar_nivel_mayores 0.92 0.0031 ## 9 1 trabajo_a_todos 0.74 0.0056 ## 10 2 ausencia_cuidar_enfermos 0.60 0.0063 ## 11 2 ayuda_padres_trabajadores 0.57 0.0060 ## 12 2 beneficios_fam_trabajo 0.53 0.0051 ## 13 2 beneficios_igualdad 0.60 0.0057 ## 14 2 beneficios_pobreza 0.53 0.0055 ## 15 2 cuidados_salud_enfermos 0.82 0.0050 ## 16 2 garantizar_nivel_desempleados 0.47 0.0055 ## 17 2 garantizar_nivel_mayores 0.79 0.0047 ## 18 2 trabajo_a_todos 0.42 0.0067 ## adicionalmente, invertimos las 3 preguntas en escala de 1 a 5, pues 1 representa mayor acuerdo. filtro &lt;- resumen_1$variable %in% c(&#39;beneficios_fam_trabajo&#39;,&#39;beneficios_igualdad&#39;,&#39;beneficios_pobreza&#39;) resumen_1$media[filtro] &lt;- 1-resumen_1$media[filtro] resumen_1$variable &lt;- reorder(resumen_1$variable, resumen_1$media, mean) ggplot(resumen_1, aes(x=variable, y=media, ymin=media-ee, ymax=media+ee, colour=factor(cluster), group=cluster)) + geom_point() + coord_flip() + geom_line() + geom_linerange() Y notamos dos grupos claros que esperaríamos ver. ¿Cómo la describirías? Esta puede ser una solución aceptable (por ejemplo, si vamos a usar los grupos en otro modelo, como parte de diseños de estrategias de comunicación, etc.) Intentemos ahora con 5 grupos: sol_cl &lt;- ajustes_km[[5]] table(sol_cl$cluster) ## ## 1 2 3 4 5 ## 450 417 573 306 362 Todos los grupos tienen tamaño razonable. No queremos tener grupos muy chicos, pues entonces es difícil caracterizarlos o entenderlos: si hay 15 personas en un grupo, cualquier resumen de este grupo estaría sujeto a variación muestral alta. Consideramos las variables originales: cluster_df &lt;- data_frame(idno = dat_3$idno, cluster = sol_cl$cluster) dat_4 &lt;- dat_3 %&gt;% gather(variable, valor, ausencia_cuidar_enfermos:trabajo_a_todos) %&gt;% left_join(cluster_df) ## Joining, by = &quot;idno&quot; resumen_5 &lt;- dat_4 %&gt;% group_by(cluster, variable) %&gt;% summarise(media = mean(valor), ee = sd(valor)/sqrt(length(valor))) ## adicionalmente, invertimos las 3 preguntas en escala de 1 a 5, pues 1 representa mayor acuerdo. filtro &lt;- resumen_5$variable %in% c(&#39;beneficios_fam_trabajo&#39;,&#39;beneficios_igualdad&#39;,&#39;beneficios_pobreza&#39;) resumen_5$media[filtro] &lt;- 1-resumen_5$media[filtro] resumen_5$variable &lt;- reorder(resumen_5$variable, resumen_5$media, mean) ggplot(resumen_5, aes(x=variable, y=media, ymin=media-ee, ymax=media+ee, colour=factor(cluster), group=cluster)) + geom_point() + coord_flip() + geom_line() + geom_linerange() Y en estos casos es especialmente útil perfilar los grupos, es decir, mostrar las diferencias en las medias de cada grupo con respecto a la media general: resumen_perfil_5 &lt;- resumen_5 %&gt;% group_by(variable) %&gt;% mutate(perfil = media - mean(media)) ggplot(resumen_perfil_5, aes(x = variable, y = perfil, colour = factor(cluster), group = cluster)) + geom_point() + coord_flip() + geom_line() + facet_wrap(~cluster) + geom_hline(yintercept=0, colour=&#39;gray&#39;) ¿Cómo les llamarías a cada uno de estos grupos? Observación: heterogeneidad en uso de escalas Los datos en escalas de acuerdo, que son usados frecuentemente en encuestas sociales y de negocios, tienen dificultades adicionales: Desde el punto de vista estadístico, usamos una medición ordinal como si fuera numérica. Esto sugiere utilizar técnicas de clustering más compicadas adaptadas a datos ordinales. Pero en realidad este es un aspecto menor en el análisis de este tipo de datos. La dificultad grande en el análisis de este tipo de datos es la heterogeneidad en el uso de la escala. Esto quiere decir que no todas las personas usan escalas 1-10 (o 1-5, o 1-100, o Totalmente de acuerdo-Totalmente en desacuerdo) de la misma manera. Hay algunos que usan todo el rango de la escala, otros que se concentran en la mitad, etc. y muchas veces eso no tienen que ver sólo con el verdadero nivel de acuerdo o desacuerdo de la persona, sino también con cómo usa el lenguaje cada persona. El verdadero problema es entonces en la medición, no que tratemos como numérica a una variable que no lo es. Los datos de una persona no son realmente directamente comparables con los de otra persona. Hay maneras de lidiar con esto: por ejemplo, centrar los niveles de respuesta de cada persona, usar modelos que intentan medir la heterogeneidad de uso en la escala y separar de niveles de acuerdo, pero lo mejor en estos casos siempre es (si es posible) mejorar la medición. 15.5 Dificultades en segmentación/clustering. Aunque la idea conceptual de clustering es más o menos clara, en la práctica es una tarea difícil. Vamos a empezar apuntando dificultades que se comunmente se encuentran: La estructura de grupos naturales que buscamos no es necesariamente de clusters compactos. No existen grupos naturales En dimensiones altas (digamos &gt; 30) muchas veces todos los puntos están a distancias similares entre ellos, especialmente cuando hay variables que aportan la separación entre grupos. Dificultades en la selección de medida de distancia (o disimilitud). 15.5.1 Estructuras no compactas En algunos casos se dice que k-medias no tiene supuestos, otros dicen que tienen supuestos de clusters esféricos, etc. k-medias es un algoritmo, no es un modelo. Así que en realidad no tiene supuestos en el sentido típico. Lo importante es entender la cantidad que estamos minimizando. Si lo que realmente queremos hacer es minimizar esta cantidad, entonces k-medias nos devuelve una solución razonable. Pero hay veces que no queremos minimizar esta cantidad. Un ejemplo clásico es el siguiente theta &lt;- runif(200,0,2*pi) r &lt;- c(runif(100,0,0.3), runif(100,0.8,1)) df &lt;- data.frame(x=r*cos(theta), y=r*sin(theta)) df$grupo &lt;- kmeans(df, centers=2, nstart=20)$cluster ggplot(df, aes(x=x, y=y, colour=factor(grupo))) + geom_point() ¿Por qué falla k-medias? Porque la estructura de grupos que estábamos buscando no es una donde los clusters están definidos por distancia baja a su centro. Aquí realmente queremos otra cosa más complicada, como clusters definidos por cantidades invariantes (en este caso, distancia al origen) Sin embargo, si lo que nos interesa es distancia baja a un centroide, entonces está solución es razonable para k=2 theta &lt;- runif(200,0,2*pi) r &lt;- c(runif(100,0,0.3), runif(100,0.8,1)) df &lt;- data.frame(x=r*cos(theta), y=r*sin(theta)) df$grupo &lt;- kmeans(df, centers=5, nstart=20)$cluster ggplot(df, aes(x=x, y=y, colour=factor(grupo))) + geom_point() Como ejemplo, pensemos que el espacio es un espacio de “gustos por películas”. Aún cuando podría ser muy interesante descubrir esta estructura concéntrica, el grupo exterior contiene personas con gustos diametralmente opuestos! El problema de hacer dos clusters, uno central y otro concéntrico puede ser un poco artificial. 15.5.2 Existencia o no de grupos “naturales” Otro punto que se discute usualmente es si hay o no grupos naturales, que se refiere a grupos bien compactos y diferenciados entre sí, como en el ejemplo inicial ggplot(filter(iris, Species %in% c(&#39;setosa&#39;,&#39;versicolor&#39;)), aes(x=Sepal.Length, y=Petal.Width)) + geom_point() Pero es común, por ejemplo, encontrar cosas como siguen: set.seed(90902) df &lt;- data.frame(x = rnorm(500,0,1), y = rnorm(500,0,1)) df$grupo &lt;- kmeans(df, centers=5, nstart=20)$cluster ggplot(df, aes(x=x, y=y, colour=factor(grupo))) + geom_point() Nótese que k-means logró encontrar una buena solución, y esta solución puede ser muy útil para nuestros fines (agrupa puntos “similares”). Sin embargo, en esta situación debemos reconocer que los tamaños, las posiciones, y el número de grupos es fundamentalmente arbitrario, y una “buena” solución depende de nuestros fines. Si corremos otra vez el algoritmo, vemos que los grupos encontrados son similares: df$grupo &lt;- kmeans(df, centers=5, nstart=20)$cluster ggplot(df, aes(x=x, y=y, colour=factor(grupo))) + geom_point() Sin embargo, si tomamos otra muestra distinta set.seed(909021) df &lt;- data.frame(x = rnorm(500,0,1), y = rnorm(500,0,1)) df$grupo &lt;- kmeans(df, centers=5, nstart=20)$cluster ggplot(df, aes(x=x, y=y, colour=factor(grupo))) + geom_point() La solución es bastante diferente. Esta diferencia no se debe al comienzo aleatorio del algoritmo. Se debe más bien a que los grupos se están definiendo por variación muestral, y pequeñas diferencias en las muestras. **En esta situación, debemos entender lo arbitrario de la solución, y considerar si una solución así es útil para nuestros fines. Esto no le quita necesariamente utilidad a la segmentación resultante, pero hay que recordar que los grupos que encontramos son en ciertos aspectos arbitrarios. Ejemplo También en esta situación puede ser que el criterio para segmentar no es uno apropiado para un algoritmo de segmentación. Por ejemplo, supongamos que un fabricante de zapatos nos pide segmentar a sus clientes en términos de su estatura y su tamaño de pie. Observamos: set.seed(909021) x &lt;- rnorm(200, 160, 15 ) df &lt;- data.frame(estatura=x, pie= 6 + 2*(x-160)/30 + rnorm(200,0,0.5)) ggplot(df, aes(x=estatura, y=pie)) + geom_point() ¿Dónde cortamos los grupos? Aunque cualquier algoritmo no supervisado nos va a dar una respuesta, muy posiblemente sería buena idea encontrar puntos de cortes definidos de otra manera (por ejemplo algo tan simple como cuantiles!). Algo similar ocurre en la segmentación por actitudes/ideología: no hay “buenos” y “malos” o “saludables” y “descuidados”, sino continuos a lo largo de estas actitudes. Grupos en dimensión alta En dimensión más alta (50 variables, 10 casos) observamos cosas como la siguiente: mat_1 &lt;- rbind(matrix(rnorm(10*50), ncol=50)) mat_1[1:10,1] &lt;- mat_1[1:10,1] + 10 dist(mat_1, method = &#39;euclidean&#39;) ## 1 2 3 4 5 6 7 8 9 ## 2 9.9 ## 3 9.3 9.2 ## 4 9.9 10.0 9.4 ## 5 9.3 10.6 10.8 9.1 ## 6 10.7 11.7 10.5 9.9 10.4 ## 7 8.2 11.8 10.1 11.2 11.1 12.4 ## 8 9.6 10.0 10.0 9.8 9.6 10.0 11.3 ## 9 9.9 9.5 9.3 9.9 9.4 10.6 11.1 9.4 ## 10 8.2 10.1 9.9 9.9 10.0 9.9 10.3 10.8 9.4 donde todos los puntos están a más o menos la misma distancia, aún cuando existe una estructura de grupos natural. En dimensión baja, la situación se ve muy diferente: mat_1 &lt;- rbind(matrix(rnorm(10*2), ncol=2)) mat_1[1:5,1] &lt;- mat_1[1:5,1] + 10 dist(mat_1, method = &#39;euclidean&#39;) ## 1 2 3 4 5 6 7 8 9 ## 2 1.43 ## 3 2.42 3.76 ## 4 1.05 1.66 2.31 ## 5 2.19 1.36 4.59 2.86 ## 6 9.00 9.12 10.28 10.04 7.88 ## 7 11.85 11.95 13.06 12.89 10.69 2.85 ## 8 8.23 8.26 9.63 9.26 6.99 0.95 3.69 ## 9 10.72 10.72 12.09 11.75 9.44 1.83 1.38 2.50 ## 10 10.86 10.65 12.49 11.85 9.30 2.82 2.70 3.00 1.63 ggplot(data.frame(mat_1), aes(x=X1, y=X2)) + geom_point() Y cualquier técnica razonable que usemos lograría encontrar estos grupos. Muchas veces pueden encontrarse mejores soluciones aplicando alguna técnica de reducción de dimensionalidad antes de hacer clustering. Dificultades en la selección de métrica Muchas veces no hay una métrica natural para el problema. En este caso, muchas veces escogemos distancia euclideana (con variables estandarizadas o no, dependiendo de sus escalas). La inclusión de variables categóricas plantea distintas alternativas que dan resultados distintos. En dimensión alta, incluso con variables numéricas, no siempre es claro que peso debería tener cada variable. Los métodos básicos de clustering generalmente producen una solución bien definida en problemas dimensión baja con clusters razonablemente bien definidos, donde hay una métrica de distancia natural. En otros casos, como: Dimensión alta con muchas variables ruidosas o que no aportan en la definición de los clusters. Estructuras relativamente dispersas que quisiéramos agrupar Clusters no bien definidos. Dificultad en escoger una métrica única apropiada para el problema, el resultado puede depender mucho del algoritmo, el criterio del analista, y la muestra de entrenamiento. Eso no quiere decir que la segmentación de casos que produce el algoritmo no sea útil, más bien que es difìcil obtener grupos naturales "]
]
